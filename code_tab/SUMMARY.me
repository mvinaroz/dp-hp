condMMD_binary_priv.py - a unified file for non-private setting for binary categories (isolet, cervical, adult, census, epileptic, credit datasets available), contains options both for multicategorical and binary datasets. Uses one generatorfor all categories.
condMMD_binary_priv_base.py - as above, but onlcudes only the numerical data option (no mixed categories option)

in FRR_Gauss we have the following parameters:
n_features - number of features, e.g. 10000
X - batch size * real input features, e.g. [91362, 29]
W_freq - n_features/2, number of real input features, e.g. [5000,29]

XWT - We'll multiply X by W_freq and so we can substitute the real input number of features withe proposed number of features, e.g. [91362, 5000]
then we take the cos and sin of this XWT

and we concatenate them, so Z=torch.cat((Z1,Z2),1), e.g. [91362,10000]

---
In MMD we transpose x to another feature space h (which is a reproducing hilbert space),
then we draw sampes from this new distribution h, and we take a mean

We compare the mean of the original data transformed to h and then we take mean of all the origninal samples transformed to h


------------------------

To run the tabular experiments it is enough to run the script single_generator_priv_all.py with some of the arguments.

The arguments are following:

dataset - type of dataset, choices: epileptic, credit, census, cervical, adult, isolet, default =credit
private - if running the generator with privacy guarantees, choices: 0, (non-private), 1 (private), by default non-private training is run
epochs - number of training epochs for generator
batch - batch size
num_features - number of features
undersample - the undersampling to the most prelevant class label
repeat - number of repetitions of the entire experiment
classifiers - a list of methods to test, by default all methods are run.

0 - LogisticRegression
1 - GaussianNB
2 - BernoulliNB
3 - LinearSVC
4 - DecisionTreeClassifier
5 - LinearDiscriminantAnalysis
6 - AdaBoostClassifier
7 - BaggingClassifier
8 - RandomForestClassifier
9 - GradientBoostingClassifier
10 - MLP
11 - XGBoost



Example:
python single_generator_priv_all.py --dataset credit --private 0 --epochs 2000 --batch 0.3 --num_features 1000 --undersample 0.5 --repeat 2 --classifiers 1 2 5


First we test a dataset on a number of classifiers on a real dataset (in a private or non-private setting). Subsequently, we train a generator with the DP-MERF method and then we test the classifiers on the generated data (again in a private or non-private setting).