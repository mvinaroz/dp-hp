


{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 50, 'order_hermite': 100, 'subsampled_rate': 0.25} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.87498040e-01 4.86150907e-01 2.04208185e-02 5.70927597e-03
 2.20958246e-04] 

Train Epoch: 0 	 Loss: 18.608139
Train Epoch: 1 	 Loss: 17.610224
Train Epoch: 2 	 Loss: 16.380009
Train Epoch: 3 	 Loss: 14.930698
Train Epoch: 4 	 Loss: 14.145355
Train Epoch: 5 	 Loss: 13.896638
Train Epoch: 6 	 Loss: 13.654846
Train Epoch: 7 	 Loss: 13.984179
Train Epoch: 8 	 Loss: 13.192487
Train Epoch: 9 	 Loss: 13.299866
Train Epoch: 10 	 Loss: 13.260178
Train Epoch: 11 	 Loss: 13.348336
Train Epoch: 12 	 Loss: 13.828921
Train Epoch: 13 	 Loss: 12.795244
Train Epoch: 14 	 Loss: 12.869157
Train Epoch: 15 	 Loss: 12.883806
Train Epoch: 16 	 Loss: 12.741853
Train Epoch: 17 	 Loss: 12.566770
Train Epoch: 18 	 Loss: 12.567135
Train Epoch: 19 	 Loss: 13.783806
Train Epoch: 20 	 Loss: 13.836796
Train Epoch: 21 	 Loss: 13.648571
Train Epoch: 22 	 Loss: 13.318447
Train Epoch: 23 	 Loss: 13.155077
Train Epoch: 24 	 Loss: 13.188467
Train Epoch: 25 	 Loss: 12.775610
Train Epoch: 26 	 Loss: 12.775908
Train Epoch: 27 	 Loss: 13.082092
Train Epoch: 28 	 Loss: 13.172705
Train Epoch: 29 	 Loss: 12.872208
Train Epoch: 30 	 Loss: 12.596434
Train Epoch: 31 	 Loss: 12.788650
Train Epoch: 32 	 Loss: 12.839144
Train Epoch: 33 	 Loss: 12.655478
Train Epoch: 34 	 Loss: 12.587660
Train Epoch: 35 	 Loss: 12.256773
Train Epoch: 36 	 Loss: 13.012104
Train Epoch: 37 	 Loss: 12.690390
Train Epoch: 38 	 Loss: 13.061336
Train Epoch: 39 	 Loss: 12.431218
Train Epoch: 40 	 Loss: 12.347219
Train Epoch: 41 	 Loss: 12.354948
Train Epoch: 42 	 Loss: 12.715569
Train Epoch: 43 	 Loss: 12.496141
Train Epoch: 44 	 Loss: 12.888729
Train Epoch: 45 	 Loss: 12.911161
Train Epoch: 46 	 Loss: 12.669933
Train Epoch: 47 	 Loss: 12.224312
Train Epoch: 48 	 Loss: 12.651506
Train Epoch: 49 	 Loss: 12.552331

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.771

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.341

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.717

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.757

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.926

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.008

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.875

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.885

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.874

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.861

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.747

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.905
------
f1 mean across methods is 0.722


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.88460277e-01 4.84981967e-01 2.07059260e-02 5.57384995e-03
 2.77979729e-04] 

Train Epoch: 0 	 Loss: 19.938898
Train Epoch: 1 	 Loss: 18.563831
Train Epoch: 2 	 Loss: 17.369396
Train Epoch: 3 	 Loss: 17.163723
Train Epoch: 4 	 Loss: 14.713643
Train Epoch: 5 	 Loss: 14.755545
Train Epoch: 6 	 Loss: 15.558549
Train Epoch: 7 	 Loss: 15.198738
Train Epoch: 8 	 Loss: 14.856195
Train Epoch: 9 	 Loss: 14.449430
Train Epoch: 10 	 Loss: 14.982068
Train Epoch: 11 	 Loss: 15.037243
Train Epoch: 12 	 Loss: 14.040409
Train Epoch: 13 	 Loss: 13.840210
Train Epoch: 14 	 Loss: 14.658837
Train Epoch: 15 	 Loss: 15.002532
Train Epoch: 16 	 Loss: 14.549722
Train Epoch: 17 	 Loss: 13.886679
Train Epoch: 18 	 Loss: 13.856892
Train Epoch: 19 	 Loss: 14.099790
Train Epoch: 20 	 Loss: 13.930811
Train Epoch: 21 	 Loss: 14.560579
Train Epoch: 22 	 Loss: 13.994389
Train Epoch: 23 	 Loss: 13.917116
Train Epoch: 24 	 Loss: 14.583546
Train Epoch: 25 	 Loss: 13.430558
Train Epoch: 26 	 Loss: 14.258772
Train Epoch: 27 	 Loss: 14.140308
Train Epoch: 28 	 Loss: 14.325379
Train Epoch: 29 	 Loss: 14.325007
Train Epoch: 30 	 Loss: 14.128747
Train Epoch: 31 	 Loss: 14.311300
Train Epoch: 32 	 Loss: 14.218463
Train Epoch: 33 	 Loss: 14.000078
Train Epoch: 34 	 Loss: 14.187870
Train Epoch: 35 	 Loss: 13.764731
Train Epoch: 36 	 Loss: 14.401427
Train Epoch: 37 	 Loss: 13.826331
Train Epoch: 38 	 Loss: 13.977216
Train Epoch: 39 	 Loss: 13.917517
Train Epoch: 40 	 Loss: 14.346530
Train Epoch: 41 	 Loss: 13.745852
Train Epoch: 42 	 Loss: 13.804378
Train Epoch: 43 	 Loss: 13.648338
Train Epoch: 44 	 Loss: 13.658865
Train Epoch: 45 	 Loss: 13.990527
Train Epoch: 46 	 Loss: 13.473676
Train Epoch: 47 	 Loss: 13.967354
Train Epoch: 48 	 Loss: 13.688957
Train Epoch: 49 	 Loss: 13.668996

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.861

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.480

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.884

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.859

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.890

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.928

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.935

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.879

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.927

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.885

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.861

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.942
------
f1 mean across methods is 0.861


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.87391125e-01 4.86136652e-01 2.05491169e-02 5.67363754e-03
 2.49468987e-04] 

Train Epoch: 0 	 Loss: 17.523617
Train Epoch: 1 	 Loss: 18.616074
Train Epoch: 2 	 Loss: 15.286688
Train Epoch: 3 	 Loss: 14.405681
Train Epoch: 4 	 Loss: 13.387394
Train Epoch: 5 	 Loss: 13.378944
Train Epoch: 6 	 Loss: 13.689040
Train Epoch: 7 	 Loss: 14.272029
Train Epoch: 8 	 Loss: 12.584774
Train Epoch: 9 	 Loss: 13.550368
Train Epoch: 10 	 Loss: 13.282547
Train Epoch: 11 	 Loss: 13.208920
Train Epoch: 12 	 Loss: 12.986089
Train Epoch: 13 	 Loss: 13.258760
Train Epoch: 14 	 Loss: 13.400915
Train Epoch: 15 	 Loss: 13.303963
Train Epoch: 16 	 Loss: 12.953765
Train Epoch: 17 	 Loss: 13.018031
Train Epoch: 18 	 Loss: 12.636539
Train Epoch: 19 	 Loss: 13.261861
Train Epoch: 20 	 Loss: 13.014697
Train Epoch: 21 	 Loss: 13.082033
Train Epoch: 22 	 Loss: 12.908669
Train Epoch: 23 	 Loss: 13.222889
Train Epoch: 24 	 Loss: 13.076159
Train Epoch: 25 	 Loss: 12.815586
Train Epoch: 26 	 Loss: 12.752777
Train Epoch: 27 	 Loss: 13.310694
Train Epoch: 28 	 Loss: 13.058525
Train Epoch: 29 	 Loss: 13.421917
Train Epoch: 30 	 Loss: 13.158068
Train Epoch: 31 	 Loss: 13.389281
Train Epoch: 32 	 Loss: 12.545528
Train Epoch: 33 	 Loss: 13.019201
Train Epoch: 34 	 Loss: 12.845018
Train Epoch: 35 	 Loss: 12.683278
Train Epoch: 36 	 Loss: 12.996557
Train Epoch: 37 	 Loss: 13.061982
Train Epoch: 38 	 Loss: 12.314455
Train Epoch: 39 	 Loss: 12.163542
Train Epoch: 40 	 Loss: 12.237535
Train Epoch: 41 	 Loss: 12.835413
Train Epoch: 42 	 Loss: 13.069841
Train Epoch: 43 	 Loss: 12.106764
Train Epoch: 44 	 Loss: 12.740089
Train Epoch: 45 	 Loss: 12.736933
Train Epoch: 46 	 Loss: 12.755228
Train Epoch: 47 	 Loss: 12.817305
Train Epoch: 48 	 Loss: 12.212269
Train Epoch: 49 	 Loss: 12.817316

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.907

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.495

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.889

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.855

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.919

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.915

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.953

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.924

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.923

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.925

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.840

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.934
------
f1 mean across methods is 0.873


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.89322727e-01 4.84254943e-01 2.03637971e-02 5.80193588e-03
 2.56596673e-04] 

Train Epoch: 0 	 Loss: 17.480251
Train Epoch: 1 	 Loss: 15.800221
Train Epoch: 2 	 Loss: 13.298601
Train Epoch: 3 	 Loss: 13.208525
Train Epoch: 4 	 Loss: 12.632915
Train Epoch: 5 	 Loss: 12.415091
Train Epoch: 6 	 Loss: 12.287081
Train Epoch: 7 	 Loss: 12.398705
Train Epoch: 8 	 Loss: 12.124007
Train Epoch: 9 	 Loss: 12.531486
Train Epoch: 10 	 Loss: 11.957960
Train Epoch: 11 	 Loss: 12.346359
Train Epoch: 12 	 Loss: 12.009024
Train Epoch: 13 	 Loss: 11.800795
Train Epoch: 14 	 Loss: 12.580113
Train Epoch: 15 	 Loss: 11.952422
Train Epoch: 16 	 Loss: 12.412043
Train Epoch: 17 	 Loss: 11.844263
Train Epoch: 18 	 Loss: 11.841450
Train Epoch: 19 	 Loss: 11.890295
Train Epoch: 20 	 Loss: 11.946710
Train Epoch: 21 	 Loss: 12.199476
Train Epoch: 22 	 Loss: 12.015331
Train Epoch: 23 	 Loss: 12.369001
Train Epoch: 24 	 Loss: 12.078944
Train Epoch: 25 	 Loss: 11.899940
Train Epoch: 26 	 Loss: 12.150698
Train Epoch: 27 	 Loss: 12.104269
Train Epoch: 28 	 Loss: 12.034049
Train Epoch: 29 	 Loss: 11.862022
Train Epoch: 30 	 Loss: 11.823980
Train Epoch: 31 	 Loss: 11.904633
Train Epoch: 32 	 Loss: 12.315805
Train Epoch: 33 	 Loss: 12.186895
Train Epoch: 34 	 Loss: 11.742772
Train Epoch: 35 	 Loss: 11.796972
Train Epoch: 36 	 Loss: 11.701180
Train Epoch: 37 	 Loss: 12.020826
Train Epoch: 38 	 Loss: 11.744068
Train Epoch: 39 	 Loss: 11.922791
Train Epoch: 40 	 Loss: 11.819821
Train Epoch: 41 	 Loss: 11.914423
Train Epoch: 42 	 Loss: 12.085330
Train Epoch: 43 	 Loss: 12.199389
Train Epoch: 44 	 Loss: 11.996471
Train Epoch: 45 	 Loss: 11.575554
Train Epoch: 46 	 Loss: 12.450480
Train Epoch: 47 	 Loss: 11.886293
Train Epoch: 48 	 Loss: 11.987858
Train Epoch: 49 	 Loss: 12.141500

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.327

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.851

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.868

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.319

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.928

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.947

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.937

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.934

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.924

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.943

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.604

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.945
------
f1 mean across methods is 0.794


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.88766768e-01 4.84896435e-01 2.05562446e-02 5.52395615e-03
 2.56596673e-04] 

Train Epoch: 0 	 Loss: 18.576366
Train Epoch: 1 	 Loss: 18.746271
Train Epoch: 2 	 Loss: 15.514018
Train Epoch: 3 	 Loss: 13.515724
Train Epoch: 4 	 Loss: 13.512774
Train Epoch: 5 	 Loss: 13.258718
Train Epoch: 6 	 Loss: 13.183817
Train Epoch: 7 	 Loss: 13.577918
Train Epoch: 8 	 Loss: 12.909355
Train Epoch: 9 	 Loss: 13.478785
Train Epoch: 10 	 Loss: 13.497811
Train Epoch: 11 	 Loss: 13.564203
Train Epoch: 12 	 Loss: 13.086160
Train Epoch: 13 	 Loss: 13.041483
Train Epoch: 14 	 Loss: 13.491599
Train Epoch: 15 	 Loss: 12.977756
Train Epoch: 16 	 Loss: 13.023580
Train Epoch: 17 	 Loss: 12.951258
Train Epoch: 18 	 Loss: 13.562708
Train Epoch: 19 	 Loss: 12.932456
Train Epoch: 20 	 Loss: 13.345874
Train Epoch: 21 	 Loss: 12.906345
Train Epoch: 22 	 Loss: 13.025644
Train Epoch: 23 	 Loss: 12.861077
Train Epoch: 24 	 Loss: 13.138315
Train Epoch: 25 	 Loss: 12.678154
Train Epoch: 26 	 Loss: 12.730337
Train Epoch: 27 	 Loss: 12.507265
Train Epoch: 28 	 Loss: 12.918745
Train Epoch: 29 	 Loss: 12.638182
Train Epoch: 30 	 Loss: 12.474337
Train Epoch: 31 	 Loss: 12.925564
Train Epoch: 32 	 Loss: 12.370440
Train Epoch: 33 	 Loss: 12.858624
Train Epoch: 34 	 Loss: 12.473314
Train Epoch: 35 	 Loss: 12.309103
Train Epoch: 36 	 Loss: 12.768633
Train Epoch: 37 	 Loss: 12.371372
Train Epoch: 38 	 Loss: 12.297535
Train Epoch: 39 	 Loss: 13.155065
Train Epoch: 40 	 Loss: 13.568374
Train Epoch: 41 	 Loss: 12.462868
Train Epoch: 42 	 Loss: 12.405807
Train Epoch: 43 	 Loss: 12.250818
Train Epoch: 44 	 Loss: 12.241438
Train Epoch: 45 	 Loss: 12.394608
Train Epoch: 46 	 Loss: 12.281651
Train Epoch: 47 	 Loss: 13.028755
Train Epoch: 48 	 Loss: 12.366520
Train Epoch: 49 	 Loss: 12.430193

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.324

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.689

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.761

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.320

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.871

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.883

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.906

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.380

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.452

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.356

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.324

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.428
------
f1 mean across methods is 0.558


 ---------------------------------------- 


For each of the methods
Average F1:  [0.63793281 0.57133566 0.82363842 0.62209318 0.90696912 0.73647892
 0.92112688 0.80042875 0.82019245 0.7940413  0.67508578 0.83078277]
Std F1:  [0.25883228 0.17840271 0.07095208 0.24960078 0.02242304 0.36488428
 0.02742596 0.21137062 0.18491378 0.22096102 0.19772482 0.20194586]
Average over repetitions across all methods
Average f1 score:  0.7616755046850836
Std F1:  0.11528687245278034

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 50, 'order_hermite': 100, 'subsampled_rate': 0.3} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33142857e-01 4.43051948e-01 1.84025974e-02 5.18831169e-03
 2.14285714e-04] 

Train Epoch: 0 	 Loss: 19.178656
Train Epoch: 1 	 Loss: 20.097893
Train Epoch: 2 	 Loss: 17.519861
Train Epoch: 3 	 Loss: 16.093027
Train Epoch: 4 	 Loss: 15.838033
Train Epoch: 5 	 Loss: 15.408820
Train Epoch: 6 	 Loss: 15.241924
Train Epoch: 7 	 Loss: 15.450925
Train Epoch: 8 	 Loss: 15.266874
Train Epoch: 9 	 Loss: 15.306885
Train Epoch: 10 	 Loss: 14.251292
Train Epoch: 11 	 Loss: 14.831600
Train Epoch: 12 	 Loss: 14.337791
Train Epoch: 13 	 Loss: 14.553049
Train Epoch: 14 	 Loss: 14.509169
Train Epoch: 15 	 Loss: 14.221069
Train Epoch: 16 	 Loss: 14.590050
Train Epoch: 17 	 Loss: 14.303403
Train Epoch: 18 	 Loss: 13.765041
Train Epoch: 19 	 Loss: 14.299097
Train Epoch: 20 	 Loss: 15.101190
Train Epoch: 21 	 Loss: 13.978783
Train Epoch: 22 	 Loss: 14.215172
Train Epoch: 23 	 Loss: 13.955818
Train Epoch: 24 	 Loss: 13.828695
Train Epoch: 25 	 Loss: 14.108732
Train Epoch: 26 	 Loss: 13.858227
Train Epoch: 27 	 Loss: 14.041435
Train Epoch: 28 	 Loss: 14.231703
Train Epoch: 29 	 Loss: 14.295256
Train Epoch: 30 	 Loss: 13.746537
Train Epoch: 31 	 Loss: 13.983774
Train Epoch: 32 	 Loss: 13.587356
Train Epoch: 33 	 Loss: 13.802843
Train Epoch: 34 	 Loss: 13.894193
Train Epoch: 35 	 Loss: 13.893316
Train Epoch: 36 	 Loss: 13.812925
Train Epoch: 37 	 Loss: 13.791790
Train Epoch: 38 	 Loss: 14.171282
Train Epoch: 39 	 Loss: 14.377968
Train Epoch: 40 	 Loss: 13.696890
Train Epoch: 41 	 Loss: 13.936056
Train Epoch: 42 	 Loss: 14.601414
Train Epoch: 43 	 Loss: 14.713002
Train Epoch: 44 	 Loss: 14.663120
Train Epoch: 45 	 Loss: 14.331786
Train Epoch: 46 	 Loss: 14.078278
Train Epoch: 47 	 Loss: 14.623844
Train Epoch: 48 	 Loss: 14.442823
Train Epoch: 49 	 Loss: 13.837152

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.882

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.187

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.790

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.868

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.920

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.953

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.925

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.528

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.946

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.902

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.875

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.909
------
f1 mean across methods is 0.807


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.34577922e-01 4.41084416e-01 1.90324675e-02 5.05844156e-03
 2.46753247e-04] 

Train Epoch: 0 	 Loss: 18.368677
Train Epoch: 1 	 Loss: 18.364408
Train Epoch: 2 	 Loss: 17.034962
Train Epoch: 3 	 Loss: 15.685579
Train Epoch: 4 	 Loss: 14.190779
Train Epoch: 5 	 Loss: 13.875492
Train Epoch: 6 	 Loss: 14.040212
Train Epoch: 7 	 Loss: 14.137678
Train Epoch: 8 	 Loss: 13.513786
Train Epoch: 9 	 Loss: 13.730572
Train Epoch: 10 	 Loss: 13.632284
Train Epoch: 11 	 Loss: 13.266623
Train Epoch: 12 	 Loss: 13.522065
Train Epoch: 13 	 Loss: 14.993105
Train Epoch: 14 	 Loss: 13.343323
Train Epoch: 15 	 Loss: 13.320621
Train Epoch: 16 	 Loss: 13.245798
Train Epoch: 17 	 Loss: 13.530001
Train Epoch: 18 	 Loss: 13.114885
Train Epoch: 19 	 Loss: 13.017167
Train Epoch: 20 	 Loss: 13.422426
Train Epoch: 21 	 Loss: 13.086987
Train Epoch: 22 	 Loss: 13.462572
Train Epoch: 23 	 Loss: 12.775423
Train Epoch: 24 	 Loss: 13.912777
Train Epoch: 25 	 Loss: 12.738810
Train Epoch: 26 	 Loss: 13.062922
Train Epoch: 27 	 Loss: 13.418694
Train Epoch: 28 	 Loss: 13.137930
Train Epoch: 29 	 Loss: 13.120809
Train Epoch: 30 	 Loss: 13.226753
Train Epoch: 31 	 Loss: 13.275063
Train Epoch: 32 	 Loss: 13.432816
Train Epoch: 33 	 Loss: 13.201323
Train Epoch: 34 	 Loss: 13.052528
Train Epoch: 35 	 Loss: 13.262449
Train Epoch: 36 	 Loss: 13.008684
Train Epoch: 37 	 Loss: 12.988598
Train Epoch: 38 	 Loss: 12.920666
Train Epoch: 39 	 Loss: 13.079317
Train Epoch: 40 	 Loss: 13.172747
Train Epoch: 41 	 Loss: 13.449686
Train Epoch: 42 	 Loss: 13.428445
Train Epoch: 43 	 Loss: 12.968960
Train Epoch: 44 	 Loss: 12.952043
Train Epoch: 45 	 Loss: 13.226689
Train Epoch: 46 	 Loss: 13.272209
Train Epoch: 47 	 Loss: 12.997160
Train Epoch: 48 	 Loss: 13.390967
Train Epoch: 49 	 Loss: 12.972349

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.828

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.504

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.827

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.810

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.890

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.909

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.921

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.447

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.884

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.870

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.850

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.863
------
f1 mean across methods is 0.800


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.32896104e-01 4.43051948e-01 1.85844156e-02 5.24675325e-03
 2.20779221e-04] 

Train Epoch: 0 	 Loss: 20.734747
Train Epoch: 1 	 Loss: 18.831432
Train Epoch: 2 	 Loss: 17.832638
Train Epoch: 3 	 Loss: 15.427769
Train Epoch: 4 	 Loss: 14.915549
Train Epoch: 5 	 Loss: 16.079617
Train Epoch: 6 	 Loss: 14.703081
Train Epoch: 7 	 Loss: 14.496038
Train Epoch: 8 	 Loss: 13.466105
Train Epoch: 9 	 Loss: 14.475674
Train Epoch: 10 	 Loss: 14.558809
Train Epoch: 11 	 Loss: 15.161526
Train Epoch: 12 	 Loss: 14.756281
Train Epoch: 13 	 Loss: 14.921041
Train Epoch: 14 	 Loss: 14.106039
Train Epoch: 15 	 Loss: 13.839487
Train Epoch: 16 	 Loss: 13.995942
Train Epoch: 17 	 Loss: 13.813818
Train Epoch: 18 	 Loss: 14.380110
Train Epoch: 19 	 Loss: 14.753672
Train Epoch: 20 	 Loss: 13.912642
Train Epoch: 21 	 Loss: 13.873419
Train Epoch: 22 	 Loss: 13.903572
Train Epoch: 23 	 Loss: 13.423119
Train Epoch: 24 	 Loss: 13.901976
Train Epoch: 25 	 Loss: 14.229124
Train Epoch: 26 	 Loss: 13.509600
Train Epoch: 27 	 Loss: 13.500885
Train Epoch: 28 	 Loss: 13.403247
Train Epoch: 29 	 Loss: 14.401742
Train Epoch: 30 	 Loss: 14.666062
Train Epoch: 31 	 Loss: 14.335408
Train Epoch: 32 	 Loss: 14.219432
Train Epoch: 33 	 Loss: 13.451313
Train Epoch: 34 	 Loss: 14.280789
Train Epoch: 35 	 Loss: 13.817185
Train Epoch: 36 	 Loss: 13.667391
Train Epoch: 37 	 Loss: 13.074166
Train Epoch: 38 	 Loss: 13.741619
Train Epoch: 39 	 Loss: 13.129282
Train Epoch: 40 	 Loss: 13.301453
Train Epoch: 41 	 Loss: 13.395493
Train Epoch: 42 	 Loss: 13.862753
Train Epoch: 43 	 Loss: 13.183111
Train Epoch: 44 	 Loss: 13.191319
Train Epoch: 45 	 Loss: 13.188138
Train Epoch: 46 	 Loss: 13.269009
Train Epoch: 47 	 Loss: 13.574615
Train Epoch: 48 	 Loss: 13.855446
Train Epoch: 49 	 Loss: 13.175745

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.794

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.512

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.894

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.776

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.930

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.917

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.919

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.885

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.915

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.905

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.798

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.907
------
f1 mean across methods is 0.846


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33707792e-01 4.42259740e-01 1.85194805e-02 5.28571429e-03
 2.27272727e-04] 

Train Epoch: 0 	 Loss: 19.909718
Train Epoch: 1 	 Loss: 17.222391
Train Epoch: 2 	 Loss: 16.340118
Train Epoch: 3 	 Loss: 14.349707
Train Epoch: 4 	 Loss: 14.077855
Train Epoch: 5 	 Loss: 13.502653
Train Epoch: 6 	 Loss: 13.878702
Train Epoch: 7 	 Loss: 13.485046
Train Epoch: 8 	 Loss: 13.889116
Train Epoch: 9 	 Loss: 13.343995
Train Epoch: 10 	 Loss: 13.664769
Train Epoch: 11 	 Loss: 13.207126
Train Epoch: 12 	 Loss: 13.303118
Train Epoch: 13 	 Loss: 13.598927
Train Epoch: 14 	 Loss: 13.301214
Train Epoch: 15 	 Loss: 13.490809
Train Epoch: 16 	 Loss: 13.432763
Train Epoch: 17 	 Loss: 13.192183
Train Epoch: 18 	 Loss: 13.603441
Train Epoch: 19 	 Loss: 12.689495
Train Epoch: 20 	 Loss: 12.850678
Train Epoch: 21 	 Loss: 12.881078
Train Epoch: 22 	 Loss: 13.732194
Train Epoch: 23 	 Loss: 13.053629
Train Epoch: 24 	 Loss: 12.912899
Train Epoch: 25 	 Loss: 13.643305
Train Epoch: 26 	 Loss: 13.080154
Train Epoch: 27 	 Loss: 13.592398
Train Epoch: 28 	 Loss: 12.837966
Train Epoch: 29 	 Loss: 12.674834
Train Epoch: 30 	 Loss: 12.807174
Train Epoch: 31 	 Loss: 12.810047
Train Epoch: 32 	 Loss: 12.673756
Train Epoch: 33 	 Loss: 13.345110
Train Epoch: 34 	 Loss: 12.625070
Train Epoch: 35 	 Loss: 13.552944
Train Epoch: 36 	 Loss: 12.941569
Train Epoch: 37 	 Loss: 12.740149
Train Epoch: 38 	 Loss: 12.650333
Train Epoch: 39 	 Loss: 13.071750
Train Epoch: 40 	 Loss: 12.458057
Train Epoch: 41 	 Loss: 12.774050
Train Epoch: 42 	 Loss: 12.612755
Train Epoch: 43 	 Loss: 13.565748
Train Epoch: 44 	 Loss: 13.049988
Train Epoch: 45 	 Loss: 12.496593
Train Epoch: 46 	 Loss: 13.450869
Train Epoch: 47 	 Loss: 12.575868
Train Epoch: 48 	 Loss: 12.922686
Train Epoch: 49 	 Loss: 12.745399

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.885

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.522

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.873

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.848

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.933

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.913

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.928

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.886

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.892

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.879

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.811

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.896
------
f1 mean across methods is 0.855


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33701299e-01 4.42097403e-01 1.89155844e-02 5.08441558e-03
 2.01298701e-04] 

Train Epoch: 0 	 Loss: 20.977776
Train Epoch: 1 	 Loss: 19.036173
Train Epoch: 2 	 Loss: 17.713612
Train Epoch: 3 	 Loss: 15.808073
Train Epoch: 4 	 Loss: 16.641342
Train Epoch: 5 	 Loss: 15.847010
Train Epoch: 6 	 Loss: 15.715226
Train Epoch: 7 	 Loss: 16.171562
Train Epoch: 8 	 Loss: 16.196419
Train Epoch: 9 	 Loss: 15.224559
Train Epoch: 10 	 Loss: 15.615200
Train Epoch: 11 	 Loss: 15.218260
Train Epoch: 12 	 Loss: 15.091032
Train Epoch: 13 	 Loss: 14.874824
Train Epoch: 14 	 Loss: 16.032413
Train Epoch: 15 	 Loss: 15.585551
Train Epoch: 16 	 Loss: 15.503254
Train Epoch: 17 	 Loss: 15.094671
Train Epoch: 18 	 Loss: 15.058113
Train Epoch: 19 	 Loss: 14.766940
Train Epoch: 20 	 Loss: 14.891520
Train Epoch: 21 	 Loss: 14.951584
Train Epoch: 22 	 Loss: 14.985665
Train Epoch: 23 	 Loss: 15.163151
Train Epoch: 24 	 Loss: 15.386421
Train Epoch: 25 	 Loss: 14.986592
Train Epoch: 26 	 Loss: 14.822124
Train Epoch: 27 	 Loss: 15.069535
Train Epoch: 28 	 Loss: 14.690845
Train Epoch: 29 	 Loss: 14.552021
Train Epoch: 30 	 Loss: 14.396025
Train Epoch: 31 	 Loss: 15.097040
Train Epoch: 32 	 Loss: 14.811792
Train Epoch: 33 	 Loss: 15.002204
Train Epoch: 34 	 Loss: 15.343508
Train Epoch: 35 	 Loss: 14.807454
Train Epoch: 36 	 Loss: 14.901407
Train Epoch: 37 	 Loss: 14.773432
Train Epoch: 38 	 Loss: 14.930378
Train Epoch: 39 	 Loss: 14.780108
Train Epoch: 40 	 Loss: 14.980212
Train Epoch: 41 	 Loss: 14.902532
Train Epoch: 42 	 Loss: 14.816650
Train Epoch: 43 	 Loss: 14.889445
Train Epoch: 44 	 Loss: 15.049582
Train Epoch: 45 	 Loss: 14.779749
Train Epoch: 46 	 Loss: 14.936058
Train Epoch: 47 	 Loss: 14.891916
Train Epoch: 48 	 Loss: 14.949423
Train Epoch: 49 	 Loss: 15.148677

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.932

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.212

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.855

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.397

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.926

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.949

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.934

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.909

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.971

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.391

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.734

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.941
------
f1 mean across methods is 0.762


 ---------------------------------------- 


For each of the methods
Average F1:  [0.86412117 0.38757598 0.84785844 0.73977766 0.91952436 0.92812066
 0.92531013 0.73115271 0.92149531 0.78916855 0.8137664  0.90317278]
Std F1:  [0.04790632 0.1538608  0.03623183 0.17443934 0.01556917 0.0186454
 0.00527049 0.20073951 0.03248948 0.19978333 0.04843705 0.02522596]
Average over repetitions across all methods
Average f1 score:  0.8142536798235858
Std F1:  0.0336205486598429

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 50, 'order_hermite': 100, 'subsampled_rate': 0.35} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.70986458e-01 4.06980280e-01 1.71018658e-02 4.73461697e-03
 1.96778791e-04] 

Train Epoch: 0 	 Loss: 21.271843
Train Epoch: 1 	 Loss: 29.326416
Train Epoch: 2 	 Loss: 18.194500
Train Epoch: 3 	 Loss: 18.339497
Train Epoch: 4 	 Loss: 16.250267
Train Epoch: 5 	 Loss: 15.912570
Train Epoch: 6 	 Loss: 15.652973
Train Epoch: 7 	 Loss: 15.580916
Train Epoch: 8 	 Loss: 15.462950
Train Epoch: 9 	 Loss: 15.760167
Train Epoch: 10 	 Loss: 15.397459
Train Epoch: 11 	 Loss: 16.675936
Train Epoch: 12 	 Loss: 15.473196
Train Epoch: 13 	 Loss: 15.462730
Train Epoch: 14 	 Loss: 15.626511
Train Epoch: 15 	 Loss: 15.878704
Train Epoch: 16 	 Loss: 15.733768
Train Epoch: 17 	 Loss: 16.160664
Train Epoch: 18 	 Loss: 15.497747
Train Epoch: 19 	 Loss: 15.295212
Train Epoch: 20 	 Loss: 15.019875
Train Epoch: 21 	 Loss: 15.120539
Train Epoch: 22 	 Loss: 15.577351
Train Epoch: 23 	 Loss: 14.947339
Train Epoch: 24 	 Loss: 15.241943
Train Epoch: 25 	 Loss: 14.731872
Train Epoch: 26 	 Loss: 15.129975
Train Epoch: 27 	 Loss: 15.104790
Train Epoch: 28 	 Loss: 14.656416
Train Epoch: 29 	 Loss: 14.866449
Train Epoch: 30 	 Loss: 14.675309
Train Epoch: 31 	 Loss: 14.668457
Train Epoch: 32 	 Loss: 14.575427
Train Epoch: 33 	 Loss: 14.508097
Train Epoch: 34 	 Loss: 14.630623
Train Epoch: 35 	 Loss: 14.422058
Train Epoch: 36 	 Loss: 14.410121
Train Epoch: 37 	 Loss: 14.629196
Train Epoch: 38 	 Loss: 15.582566
Train Epoch: 39 	 Loss: 14.574297
Train Epoch: 40 	 Loss: 14.484294
Train Epoch: 41 	 Loss: 14.479143
Train Epoch: 42 	 Loss: 14.717615
Train Epoch: 43 	 Loss: 14.490339
Train Epoch: 44 	 Loss: 14.741209
Train Epoch: 45 	 Loss: 14.534243
Train Epoch: 46 	 Loss: 14.602354
Train Epoch: 47 	 Loss: 14.408014
Train Epoch: 48 	 Loss: 14.589401
Train Epoch: 49 	 Loss: 14.285722

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.898

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.538

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.904

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.772

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.627

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.916

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.918

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.633

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.928

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.552

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.858

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.917
------
f1 mean across methods is 0.788


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.72673985e-01 4.04982678e-01 1.74298305e-02 4.69883901e-03
 2.14667772e-04] 

Train Epoch: 0 	 Loss: 19.321215
Train Epoch: 1 	 Loss: 18.013813
Train Epoch: 2 	 Loss: 15.996483
Train Epoch: 3 	 Loss: 15.219669
Train Epoch: 4 	 Loss: 14.525190
Train Epoch: 5 	 Loss: 14.367851
Train Epoch: 6 	 Loss: 14.308488
Train Epoch: 7 	 Loss: 14.493030
Train Epoch: 8 	 Loss: 14.325880
Train Epoch: 9 	 Loss: 13.938850
Train Epoch: 10 	 Loss: 13.979457
Train Epoch: 11 	 Loss: 13.315141
Train Epoch: 12 	 Loss: 14.000931
Train Epoch: 13 	 Loss: 13.998360
Train Epoch: 14 	 Loss: 14.760712
Train Epoch: 15 	 Loss: 13.638721
Train Epoch: 16 	 Loss: 13.948290
Train Epoch: 17 	 Loss: 14.238091
Train Epoch: 18 	 Loss: 14.505786
Train Epoch: 19 	 Loss: 13.400430
Train Epoch: 20 	 Loss: 14.255934
Train Epoch: 21 	 Loss: 13.672576
Train Epoch: 22 	 Loss: 13.651161
Train Epoch: 23 	 Loss: 15.284055
Train Epoch: 24 	 Loss: 13.993242
Train Epoch: 25 	 Loss: 13.785440
Train Epoch: 26 	 Loss: 13.711067
Train Epoch: 27 	 Loss: 13.670256
Train Epoch: 28 	 Loss: 13.459883
Train Epoch: 29 	 Loss: 13.864140
Train Epoch: 30 	 Loss: 14.489058
Train Epoch: 31 	 Loss: 13.329272
Train Epoch: 32 	 Loss: 12.961977
Train Epoch: 33 	 Loss: 13.146927
Train Epoch: 34 	 Loss: 13.343163
Train Epoch: 35 	 Loss: 13.347634
Train Epoch: 36 	 Loss: 13.583390
Train Epoch: 37 	 Loss: 12.996784
Train Epoch: 38 	 Loss: 13.298353
Train Epoch: 39 	 Loss: 13.338230
Train Epoch: 40 	 Loss: 14.094306
Train Epoch: 41 	 Loss: 13.186279
Train Epoch: 42 	 Loss: 13.448229
Train Epoch: 43 	 Loss: 13.514275
Train Epoch: 44 	 Loss: 14.736780
Train Epoch: 45 	 Loss: 13.572954
Train Epoch: 46 	 Loss: 13.539842
Train Epoch: 47 	 Loss: 13.323099
Train Epoch: 48 	 Loss: 13.301264
Train Epoch: 49 	 Loss: 13.099410

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.894

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.576

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.919

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.786

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.895

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.931

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.914

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.905

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.921

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.897

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.819

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.927
------
f1 mean across methods is 0.865


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.71177274e-01 4.06825243e-01 1.70601249e-02 4.71076499e-03
 2.26593759e-04] 

Train Epoch: 0 	 Loss: 22.726540
Train Epoch: 1 	 Loss: 20.296799
Train Epoch: 2 	 Loss: 17.027317
Train Epoch: 3 	 Loss: 16.201218
Train Epoch: 4 	 Loss: 16.543770
Train Epoch: 5 	 Loss: 15.908819
Train Epoch: 6 	 Loss: 15.731807
Train Epoch: 7 	 Loss: 15.131413
Train Epoch: 8 	 Loss: 15.144037
Train Epoch: 9 	 Loss: 15.612741
Train Epoch: 10 	 Loss: 15.120200
Train Epoch: 11 	 Loss: 15.527723
Train Epoch: 12 	 Loss: 15.358079
Train Epoch: 13 	 Loss: 15.312952
Train Epoch: 14 	 Loss: 14.911950
Train Epoch: 15 	 Loss: 14.859470
Train Epoch: 16 	 Loss: 14.509459
Train Epoch: 17 	 Loss: 14.986992
Train Epoch: 18 	 Loss: 15.316808
Train Epoch: 19 	 Loss: 14.800516
Train Epoch: 20 	 Loss: 14.666468
Train Epoch: 21 	 Loss: 15.252573
Train Epoch: 22 	 Loss: 14.904260
Train Epoch: 23 	 Loss: 14.770006
Train Epoch: 24 	 Loss: 14.901228
Train Epoch: 25 	 Loss: 15.182317
Train Epoch: 26 	 Loss: 14.801869
Train Epoch: 27 	 Loss: 14.818495
Train Epoch: 28 	 Loss: 14.645732
Train Epoch: 29 	 Loss: 14.998598
Train Epoch: 30 	 Loss: 14.643133
Train Epoch: 31 	 Loss: 14.796716
Train Epoch: 32 	 Loss: 14.841713
Train Epoch: 33 	 Loss: 15.093020
Train Epoch: 34 	 Loss: 15.862427
Train Epoch: 35 	 Loss: 14.616749
Train Epoch: 36 	 Loss: 14.721386
Train Epoch: 37 	 Loss: 14.851698
Train Epoch: 38 	 Loss: 14.790199
Train Epoch: 39 	 Loss: 14.710068
Train Epoch: 40 	 Loss: 15.047953
Train Epoch: 41 	 Loss: 14.730989
Train Epoch: 42 	 Loss: 14.866686
Train Epoch: 43 	 Loss: 14.690462
Train Epoch: 44 	 Loss: 14.756067
Train Epoch: 45 	 Loss: 14.928290
Train Epoch: 46 	 Loss: 14.764551
Train Epoch: 47 	 Loss: 14.609037
Train Epoch: 48 	 Loss: 15.026970
Train Epoch: 49 	 Loss: 14.628656

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.920

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.573

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.825

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.870

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.934

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.927

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.926

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.932

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.935

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.919

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.871

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.942
------
f1 mean across methods is 0.881


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.71827240e-01 4.06103720e-01 1.70004949e-02 4.84791385e-03
 2.20630765e-04] 

Train Epoch: 0 	 Loss: 19.100679
Train Epoch: 1 	 Loss: 20.505306
Train Epoch: 2 	 Loss: 16.974510
Train Epoch: 3 	 Loss: 15.948702
Train Epoch: 4 	 Loss: 15.659086
Train Epoch: 5 	 Loss: 16.117170
Train Epoch: 6 	 Loss: 15.463129
Train Epoch: 7 	 Loss: 15.823549
Train Epoch: 8 	 Loss: 15.194588
Train Epoch: 9 	 Loss: 15.591482
Train Epoch: 10 	 Loss: 15.413458
Train Epoch: 11 	 Loss: 15.188766
Train Epoch: 12 	 Loss: 15.919544
Train Epoch: 13 	 Loss: 15.889439
Train Epoch: 14 	 Loss: 15.116441
Train Epoch: 15 	 Loss: 15.316081
Train Epoch: 16 	 Loss: 14.976188
Train Epoch: 17 	 Loss: 15.392298
Train Epoch: 18 	 Loss: 15.057352
Train Epoch: 19 	 Loss: 14.987516
Train Epoch: 20 	 Loss: 15.189729
Train Epoch: 21 	 Loss: 14.672184
Train Epoch: 22 	 Loss: 14.497608
Train Epoch: 23 	 Loss: 14.554349
Train Epoch: 24 	 Loss: 14.822350
Train Epoch: 25 	 Loss: 14.569210
Train Epoch: 26 	 Loss: 15.211822
Train Epoch: 27 	 Loss: 14.442937
Train Epoch: 28 	 Loss: 14.831923
Train Epoch: 29 	 Loss: 14.650133
Train Epoch: 30 	 Loss: 14.703964
Train Epoch: 31 	 Loss: 14.542929
Train Epoch: 32 	 Loss: 14.645514
Train Epoch: 33 	 Loss: 14.412385
Train Epoch: 34 	 Loss: 14.825587
Train Epoch: 35 	 Loss: 15.025500
Train Epoch: 36 	 Loss: 14.460002
Train Epoch: 37 	 Loss: 14.582522
Train Epoch: 38 	 Loss: 14.587347
Train Epoch: 39 	 Loss: 14.295532
Train Epoch: 40 	 Loss: 14.400139
Train Epoch: 41 	 Loss: 14.655754
Train Epoch: 42 	 Loss: 14.171917
Train Epoch: 43 	 Loss: 14.534424
Train Epoch: 44 	 Loss: 14.082247
Train Epoch: 45 	 Loss: 14.789993
Train Epoch: 46 	 Loss: 14.474105
Train Epoch: 47 	 Loss: 14.473932
Train Epoch: 48 	 Loss: 14.499219
Train Epoch: 49 	 Loss: 14.015260

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.916

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.545

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.675

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.779

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.872

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.938

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.855

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.549

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.690

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.890

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.820

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.923
------
f1 mean across methods is 0.788


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 50
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=50_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.72214835e-01 4.05346420e-01 1.74357935e-02 4.77635792e-03
 2.26593759e-04] 

Train Epoch: 0 	 Loss: 18.827286
Train Epoch: 1 	 Loss: 19.763214
Train Epoch: 2 	 Loss: 18.135563
Train Epoch: 3 	 Loss: 15.538446
Train Epoch: 4 	 Loss: 15.408185
Train Epoch: 5 	 Loss: 15.180393
Train Epoch: 6 	 Loss: 14.641037
Train Epoch: 7 	 Loss: 15.273319
Train Epoch: 8 	 Loss: 15.589016
Train Epoch: 9 	 Loss: 15.203291
Train Epoch: 10 	 Loss: 14.175568
Train Epoch: 11 	 Loss: 15.070732
Train Epoch: 12 	 Loss: 14.874092
Train Epoch: 13 	 Loss: 15.945785
Train Epoch: 14 	 Loss: 15.621142
Train Epoch: 15 	 Loss: 14.427052
Train Epoch: 16 	 Loss: 14.938961
Train Epoch: 17 	 Loss: 14.404445
Train Epoch: 18 	 Loss: 15.045200
Train Epoch: 19 	 Loss: 14.398851
Train Epoch: 20 	 Loss: 14.203238
Train Epoch: 21 	 Loss: 14.667276
Train Epoch: 22 	 Loss: 14.311169
Train Epoch: 23 	 Loss: 14.266376
Train Epoch: 24 	 Loss: 15.126063
Train Epoch: 25 	 Loss: 14.623957
Train Epoch: 26 	 Loss: 14.214857
Train Epoch: 27 	 Loss: 14.457817
Train Epoch: 28 	 Loss: 14.514391
Train Epoch: 29 	 Loss: 13.634158
Train Epoch: 30 	 Loss: 14.048091
Train Epoch: 31 	 Loss: 13.874741
Train Epoch: 32 	 Loss: 13.861597
Train Epoch: 33 	 Loss: 14.072034
Train Epoch: 34 	 Loss: 14.796848
Train Epoch: 35 	 Loss: 13.810776
Train Epoch: 36 	 Loss: 14.481969
Train Epoch: 37 	 Loss: 14.151770
Train Epoch: 38 	 Loss: 14.191940
Train Epoch: 39 	 Loss: 14.654088
Train Epoch: 40 	 Loss: 13.962189
Train Epoch: 41 	 Loss: 13.609307
Train Epoch: 42 	 Loss: 13.635046
Train Epoch: 43 	 Loss: 13.690277
Train Epoch: 44 	 Loss: 13.593102
Train Epoch: 45 	 Loss: 13.511868
Train Epoch: 46 	 Loss: 13.744110
Train Epoch: 47 	 Loss: 13.546261
Train Epoch: 48 	 Loss: 13.593955
Train Epoch: 49 	 Loss: 13.419299

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.926

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.560

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.919

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.868

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.909

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.933

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.947

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.877

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.931

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.882

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.826

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.931
------
f1 mean across methods is 0.876


 ---------------------------------------- 


For each of the methods
Average F1:  [0.91077559 0.55823772 0.84874169 0.81509414 0.84734741 0.92895762
 0.91222119 0.77940206 0.88086194 0.82787263 0.83894912 0.92806994]
Std F1:  [0.01256957 0.01509741 0.09347291 0.04411248 0.11175216 0.00724682
 0.03066174 0.15709581 0.09562737 0.13846643 0.02161751 0.00851279]
Average over repetitions across all methods
Average f1 score:  0.8397109217648611
Std F1:  0.04249984646332645

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 100, 'order_hermite': 100, 'subsampled_rate': 0.25} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.87498040e-01 4.86150907e-01 2.04208185e-02 5.70927597e-03
 2.20958246e-04] 

Train Epoch: 0 	 Loss: 18.809235
Train Epoch: 1 	 Loss: 16.549591
Train Epoch: 2 	 Loss: 16.960922
Train Epoch: 3 	 Loss: 15.138869
Train Epoch: 4 	 Loss: 14.100001
Train Epoch: 5 	 Loss: 13.955934
Train Epoch: 6 	 Loss: 14.192864
Train Epoch: 7 	 Loss: 14.326310
Train Epoch: 8 	 Loss: 13.166633
Train Epoch: 9 	 Loss: 13.926596
Train Epoch: 10 	 Loss: 13.383691
Train Epoch: 11 	 Loss: 14.788601
Train Epoch: 12 	 Loss: 13.634327
Train Epoch: 13 	 Loss: 13.327723
Train Epoch: 14 	 Loss: 13.273476
Train Epoch: 15 	 Loss: 13.095053
Train Epoch: 16 	 Loss: 13.193202
Train Epoch: 17 	 Loss: 13.268832
Train Epoch: 18 	 Loss: 13.339077
Train Epoch: 19 	 Loss: 13.260905
Train Epoch: 20 	 Loss: 12.934954
Train Epoch: 21 	 Loss: 13.119478
Train Epoch: 22 	 Loss: 12.855990
Train Epoch: 23 	 Loss: 12.436115
Train Epoch: 24 	 Loss: 12.505122
Train Epoch: 25 	 Loss: 12.499081
Train Epoch: 26 	 Loss: 12.929853
Train Epoch: 27 	 Loss: 12.486073
Train Epoch: 28 	 Loss: 12.418959
Train Epoch: 29 	 Loss: 13.274702
Train Epoch: 30 	 Loss: 12.703876
Train Epoch: 31 	 Loss: 12.186183
Train Epoch: 32 	 Loss: 12.398494
Train Epoch: 33 	 Loss: 12.269348
Train Epoch: 34 	 Loss: 12.288755
Train Epoch: 35 	 Loss: 12.334499
Train Epoch: 36 	 Loss: 12.414355
Train Epoch: 37 	 Loss: 12.292651
Train Epoch: 38 	 Loss: 12.456481
Train Epoch: 39 	 Loss: 12.183017
Train Epoch: 40 	 Loss: 12.529209
Train Epoch: 41 	 Loss: 12.527587
Train Epoch: 42 	 Loss: 12.160927
Train Epoch: 43 	 Loss: 12.229760
Train Epoch: 44 	 Loss: 12.693281
Train Epoch: 45 	 Loss: 12.284288
Train Epoch: 46 	 Loss: 12.212368
Train Epoch: 47 	 Loss: 12.523168
Train Epoch: 48 	 Loss: 12.479455
Train Epoch: 49 	 Loss: 12.325186
Train Epoch: 50 	 Loss: 12.132357
Train Epoch: 51 	 Loss: 12.532385
Train Epoch: 52 	 Loss: 12.197290
Train Epoch: 53 	 Loss: 12.297790
Train Epoch: 54 	 Loss: 12.135551
Train Epoch: 55 	 Loss: 12.219246
Train Epoch: 56 	 Loss: 12.668849
Train Epoch: 57 	 Loss: 12.146348
Train Epoch: 58 	 Loss: 12.088600
Train Epoch: 59 	 Loss: 12.836809
Train Epoch: 60 	 Loss: 12.238997
Train Epoch: 61 	 Loss: 12.296017
Train Epoch: 62 	 Loss: 13.065228
Train Epoch: 63 	 Loss: 13.726729
Train Epoch: 64 	 Loss: 12.179901
Train Epoch: 65 	 Loss: 12.232065
Train Epoch: 66 	 Loss: 12.672710
Train Epoch: 67 	 Loss: 13.405649
Train Epoch: 68 	 Loss: 12.518568
Train Epoch: 69 	 Loss: 12.911906
Train Epoch: 70 	 Loss: 12.348503
Train Epoch: 71 	 Loss: 11.854805
Train Epoch: 72 	 Loss: 11.855164
Train Epoch: 73 	 Loss: 11.847919
Train Epoch: 74 	 Loss: 11.847832
Train Epoch: 75 	 Loss: 12.217323
Train Epoch: 76 	 Loss: 11.914181
Train Epoch: 77 	 Loss: 12.639876
Train Epoch: 78 	 Loss: 11.849255
Train Epoch: 79 	 Loss: 11.834484
Train Epoch: 80 	 Loss: 11.960769
Train Epoch: 81 	 Loss: 11.680993
Train Epoch: 82 	 Loss: 11.862198
Train Epoch: 83 	 Loss: 11.728198
Train Epoch: 84 	 Loss: 11.665464
Train Epoch: 85 	 Loss: 12.070828
Train Epoch: 86 	 Loss: 11.694295
Train Epoch: 87 	 Loss: 11.644592
Train Epoch: 88 	 Loss: 11.716146
Train Epoch: 89 	 Loss: 11.903358
Train Epoch: 90 	 Loss: 11.699900
Train Epoch: 91 	 Loss: 11.923655
Train Epoch: 92 	 Loss: 11.728679
Train Epoch: 93 	 Loss: 12.127914
Train Epoch: 94 	 Loss: 11.954017
Train Epoch: 95 	 Loss: 11.956161
Train Epoch: 96 	 Loss: 11.925880
Train Epoch: 97 	 Loss: 11.832879
Train Epoch: 98 	 Loss: 11.764971
Train Epoch: 99 	 Loss: 11.547961

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.872

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.351

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.854

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.864

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.908

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.006

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.935

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.886

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.894

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.873

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.884

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.892
------
f1 mean across methods is 0.768


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.88460277e-01 4.84981967e-01 2.07059260e-02 5.57384995e-03
 2.77979729e-04] 

Train Epoch: 0 	 Loss: 18.907181
Train Epoch: 1 	 Loss: 18.963669
Train Epoch: 2 	 Loss: 16.770912
Train Epoch: 3 	 Loss: 17.897282
Train Epoch: 4 	 Loss: 15.281303
Train Epoch: 5 	 Loss: 15.610877
Train Epoch: 6 	 Loss: 15.542579
Train Epoch: 7 	 Loss: 15.530376
Train Epoch: 8 	 Loss: 14.797357
Train Epoch: 9 	 Loss: 15.472689
Train Epoch: 10 	 Loss: 14.991510
Train Epoch: 11 	 Loss: 14.965213
Train Epoch: 12 	 Loss: 14.679521
Train Epoch: 13 	 Loss: 14.831177
Train Epoch: 14 	 Loss: 14.771500
Train Epoch: 15 	 Loss: 15.167527
Train Epoch: 16 	 Loss: 15.017368
Train Epoch: 17 	 Loss: 14.673021
Train Epoch: 18 	 Loss: 14.989475
Train Epoch: 19 	 Loss: 14.637239
Train Epoch: 20 	 Loss: 14.702169
Train Epoch: 21 	 Loss: 15.017925
Train Epoch: 22 	 Loss: 14.762213
Train Epoch: 23 	 Loss: 14.912262
Train Epoch: 24 	 Loss: 14.904059
Train Epoch: 25 	 Loss: 15.037033
Train Epoch: 26 	 Loss: 14.865501
Train Epoch: 27 	 Loss: 14.897630
Train Epoch: 28 	 Loss: 14.505774
Train Epoch: 29 	 Loss: 15.246307
Train Epoch: 30 	 Loss: 14.253559
Train Epoch: 31 	 Loss: 14.151787
Train Epoch: 32 	 Loss: 14.111817
Train Epoch: 33 	 Loss: 14.200358
Train Epoch: 34 	 Loss: 14.578861
Train Epoch: 35 	 Loss: 14.111174
Train Epoch: 36 	 Loss: 14.228472
Train Epoch: 37 	 Loss: 14.367091
Train Epoch: 38 	 Loss: 14.068093
Train Epoch: 39 	 Loss: 14.045944
Train Epoch: 40 	 Loss: 14.221702
Train Epoch: 41 	 Loss: 14.238457
Train Epoch: 42 	 Loss: 14.139301
Train Epoch: 43 	 Loss: 14.186527
Train Epoch: 44 	 Loss: 14.131162
Train Epoch: 45 	 Loss: 14.582428
Train Epoch: 46 	 Loss: 14.087896
Train Epoch: 47 	 Loss: 14.326012
Train Epoch: 48 	 Loss: 14.186008
Train Epoch: 49 	 Loss: 14.066088
Train Epoch: 50 	 Loss: 13.987093
Train Epoch: 51 	 Loss: 14.190218
Train Epoch: 52 	 Loss: 13.999451
Train Epoch: 53 	 Loss: 14.103880
Train Epoch: 54 	 Loss: 13.910238
Train Epoch: 55 	 Loss: 13.962784
Train Epoch: 56 	 Loss: 14.231116
Train Epoch: 57 	 Loss: 14.088804
Train Epoch: 58 	 Loss: 14.042519
Train Epoch: 59 	 Loss: 14.047869
Train Epoch: 60 	 Loss: 13.922640
Train Epoch: 61 	 Loss: 14.094879
Train Epoch: 62 	 Loss: 14.023111
Train Epoch: 63 	 Loss: 13.935433
Train Epoch: 64 	 Loss: 13.920546
Train Epoch: 65 	 Loss: 14.044033
Train Epoch: 66 	 Loss: 14.284560
Train Epoch: 67 	 Loss: 13.977576
Train Epoch: 68 	 Loss: 14.724566
Train Epoch: 69 	 Loss: 14.045126
Train Epoch: 70 	 Loss: 14.765903
Train Epoch: 71 	 Loss: 13.985203
Train Epoch: 72 	 Loss: 13.921907
Train Epoch: 73 	 Loss: 14.138154
Train Epoch: 74 	 Loss: 14.126581
Train Epoch: 75 	 Loss: 15.131617
Train Epoch: 76 	 Loss: 14.004778
Train Epoch: 77 	 Loss: 14.130717
Train Epoch: 78 	 Loss: 13.971250
Train Epoch: 79 	 Loss: 13.998471
Train Epoch: 80 	 Loss: 14.067634
Train Epoch: 81 	 Loss: 14.167732
Train Epoch: 82 	 Loss: 13.923982
Train Epoch: 83 	 Loss: 13.922037
Train Epoch: 84 	 Loss: 14.558326
Train Epoch: 85 	 Loss: 14.019150
Train Epoch: 86 	 Loss: 13.901604
Train Epoch: 87 	 Loss: 14.449364
Train Epoch: 88 	 Loss: 14.050638
Train Epoch: 89 	 Loss: 14.280478
Train Epoch: 90 	 Loss: 14.215369
Train Epoch: 91 	 Loss: 13.935742
Train Epoch: 92 	 Loss: 13.932545
Train Epoch: 93 	 Loss: 13.917476
Train Epoch: 94 	 Loss: 14.035803
Train Epoch: 95 	 Loss: 14.030567
Train Epoch: 96 	 Loss: 14.561719
Train Epoch: 97 	 Loss: 13.981652
Train Epoch: 98 	 Loss: 13.988923
Train Epoch: 99 	 Loss: 14.710863

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.848

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.497

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.857

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.825

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.909

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.933

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.920

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.905

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.908

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.908

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.858

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.903
------
f1 mean across methods is 0.856


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.87391125e-01 4.86136652e-01 2.05491169e-02 5.67363754e-03
 2.49468987e-04] 

Train Epoch: 0 	 Loss: 18.192329
Train Epoch: 1 	 Loss: 17.716799
Train Epoch: 2 	 Loss: 14.758415
Train Epoch: 3 	 Loss: 16.400713
Train Epoch: 4 	 Loss: 14.076618
Train Epoch: 5 	 Loss: 13.256849
Train Epoch: 6 	 Loss: 12.627378
Train Epoch: 7 	 Loss: 13.788892
Train Epoch: 8 	 Loss: 12.779586
Train Epoch: 9 	 Loss: 13.125313
Train Epoch: 10 	 Loss: 12.975014
Train Epoch: 11 	 Loss: 12.938739
Train Epoch: 12 	 Loss: 13.309793
Train Epoch: 13 	 Loss: 12.922800
Train Epoch: 14 	 Loss: 13.261813
Train Epoch: 15 	 Loss: 12.927517
Train Epoch: 16 	 Loss: 12.955182
Train Epoch: 17 	 Loss: 12.784422
Train Epoch: 18 	 Loss: 12.861346
Train Epoch: 19 	 Loss: 14.505521
Train Epoch: 20 	 Loss: 13.083912
Train Epoch: 21 	 Loss: 12.836444
Train Epoch: 22 	 Loss: 12.982740
Train Epoch: 23 	 Loss: 13.008986
Train Epoch: 24 	 Loss: 12.789005
Train Epoch: 25 	 Loss: 12.900202
Train Epoch: 26 	 Loss: 12.560989
Train Epoch: 27 	 Loss: 12.750494
Train Epoch: 28 	 Loss: 12.705344
Train Epoch: 29 	 Loss: 13.030827
Train Epoch: 30 	 Loss: 12.251642
Train Epoch: 31 	 Loss: 13.569281
Train Epoch: 32 	 Loss: 12.311536
Train Epoch: 33 	 Loss: 12.776628
Train Epoch: 34 	 Loss: 12.769583
Train Epoch: 35 	 Loss: 12.444687
Train Epoch: 36 	 Loss: 12.841512
Train Epoch: 37 	 Loss: 12.766171
Train Epoch: 38 	 Loss: 12.140371
Train Epoch: 39 	 Loss: 12.437811
Train Epoch: 40 	 Loss: 12.638812
Train Epoch: 41 	 Loss: 12.607935
Train Epoch: 42 	 Loss: 13.142113
Train Epoch: 43 	 Loss: 12.351418
Train Epoch: 44 	 Loss: 12.832616
Train Epoch: 45 	 Loss: 13.346367
Train Epoch: 46 	 Loss: 12.727041
Train Epoch: 47 	 Loss: 12.680024
Train Epoch: 48 	 Loss: 12.878389
Train Epoch: 49 	 Loss: 12.865765
Train Epoch: 50 	 Loss: 12.784367
Train Epoch: 51 	 Loss: 12.650723
Train Epoch: 52 	 Loss: 12.309275
Train Epoch: 53 	 Loss: 13.027169
Train Epoch: 54 	 Loss: 12.931063
Train Epoch: 55 	 Loss: 12.757025
Train Epoch: 56 	 Loss: 12.678971
Train Epoch: 57 	 Loss: 12.678820
Train Epoch: 58 	 Loss: 12.242399
Train Epoch: 59 	 Loss: 12.781639
Train Epoch: 60 	 Loss: 12.594043
Train Epoch: 61 	 Loss: 12.898857
Train Epoch: 62 	 Loss: 12.748728
Train Epoch: 63 	 Loss: 12.626867
Train Epoch: 64 	 Loss: 12.735108
Train Epoch: 65 	 Loss: 12.588689
Train Epoch: 66 	 Loss: 12.488214
Train Epoch: 67 	 Loss: 12.784679
Train Epoch: 68 	 Loss: 13.154765
Train Epoch: 69 	 Loss: 11.976122
Train Epoch: 70 	 Loss: 12.681433
Train Epoch: 71 	 Loss: 12.550003
Train Epoch: 72 	 Loss: 11.982224
Train Epoch: 73 	 Loss: 12.376171
Train Epoch: 74 	 Loss: 12.626457
Train Epoch: 75 	 Loss: 12.436636
Train Epoch: 76 	 Loss: 12.765590
Train Epoch: 77 	 Loss: 12.239129
Train Epoch: 78 	 Loss: 13.263525
Train Epoch: 79 	 Loss: 12.573921
Train Epoch: 80 	 Loss: 11.961924
Train Epoch: 81 	 Loss: 12.074488
Train Epoch: 82 	 Loss: 12.556001
Train Epoch: 83 	 Loss: 11.922836
Train Epoch: 84 	 Loss: 12.305798
Train Epoch: 85 	 Loss: 12.165046
Train Epoch: 86 	 Loss: 13.442413
Train Epoch: 87 	 Loss: 12.649659
Train Epoch: 88 	 Loss: 12.530109
Train Epoch: 89 	 Loss: 13.034596
Train Epoch: 90 	 Loss: 12.415028
Train Epoch: 91 	 Loss: 12.268458
Train Epoch: 92 	 Loss: 12.383427
Train Epoch: 93 	 Loss: 12.095179
Train Epoch: 94 	 Loss: 12.710312
Train Epoch: 95 	 Loss: 12.217723
Train Epoch: 96 	 Loss: 11.396145
Train Epoch: 97 	 Loss: 11.592546
Train Epoch: 98 	 Loss: 12.338749
Train Epoch: 99 	 Loss: 12.316886

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.836

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.494

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.892

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.798

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.867

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.920

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.861

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.874

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.907

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.875

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.807

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.899
------
f1 mean across methods is 0.836


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.89322727e-01 4.84254943e-01 2.03637971e-02 5.80193588e-03
 2.56596673e-04] 

Train Epoch: 0 	 Loss: 17.396931
Train Epoch: 1 	 Loss: 15.945757
Train Epoch: 2 	 Loss: 14.639256
Train Epoch: 3 	 Loss: 13.069117
Train Epoch: 4 	 Loss: 12.414313
Train Epoch: 5 	 Loss: 12.346584
Train Epoch: 6 	 Loss: 12.262420
Train Epoch: 7 	 Loss: 12.014982
Train Epoch: 8 	 Loss: 12.181913
Train Epoch: 9 	 Loss: 11.949559
Train Epoch: 10 	 Loss: 11.893595
Train Epoch: 11 	 Loss: 11.876225
Train Epoch: 12 	 Loss: 11.976704
Train Epoch: 13 	 Loss: 11.706290
Train Epoch: 14 	 Loss: 12.629150
Train Epoch: 15 	 Loss: 12.438309
Train Epoch: 16 	 Loss: 12.228034
Train Epoch: 17 	 Loss: 11.687622
Train Epoch: 18 	 Loss: 11.727238
Train Epoch: 19 	 Loss: 11.964186
Train Epoch: 20 	 Loss: 12.186316
Train Epoch: 21 	 Loss: 11.938961
Train Epoch: 22 	 Loss: 11.952782
Train Epoch: 23 	 Loss: 11.710384
Train Epoch: 24 	 Loss: 11.622515
Train Epoch: 25 	 Loss: 11.844446
Train Epoch: 26 	 Loss: 12.093629
Train Epoch: 27 	 Loss: 11.856691
Train Epoch: 28 	 Loss: 11.593931
Train Epoch: 29 	 Loss: 11.706863
Train Epoch: 30 	 Loss: 12.022540
Train Epoch: 31 	 Loss: 11.702431
Train Epoch: 32 	 Loss: 12.317022
Train Epoch: 33 	 Loss: 11.873013
Train Epoch: 34 	 Loss: 11.900638
Train Epoch: 35 	 Loss: 11.905680
Train Epoch: 36 	 Loss: 11.710123
Train Epoch: 37 	 Loss: 11.939098
Train Epoch: 38 	 Loss: 11.755975
Train Epoch: 39 	 Loss: 11.779154
Train Epoch: 40 	 Loss: 12.079727
Train Epoch: 41 	 Loss: 11.849564
Train Epoch: 42 	 Loss: 11.925310
Train Epoch: 43 	 Loss: 11.856694
Train Epoch: 44 	 Loss: 11.831905
Train Epoch: 45 	 Loss: 11.423883
Train Epoch: 46 	 Loss: 11.903979
Train Epoch: 47 	 Loss: 12.125465
Train Epoch: 48 	 Loss: 11.894119
Train Epoch: 49 	 Loss: 12.199076
Train Epoch: 50 	 Loss: 11.651499
Train Epoch: 51 	 Loss: 11.805943
Train Epoch: 52 	 Loss: 12.645615
Train Epoch: 53 	 Loss: 12.302978
Train Epoch: 54 	 Loss: 11.974632
Train Epoch: 55 	 Loss: 11.842711
Train Epoch: 56 	 Loss: 11.944183
Train Epoch: 57 	 Loss: 11.693958
Train Epoch: 58 	 Loss: 11.817875
Train Epoch: 59 	 Loss: 11.803352
Train Epoch: 60 	 Loss: 11.780832
Train Epoch: 61 	 Loss: 11.802080
Train Epoch: 62 	 Loss: 12.430999
Train Epoch: 63 	 Loss: 11.681751
Train Epoch: 64 	 Loss: 12.146479
Train Epoch: 65 	 Loss: 12.097234
Train Epoch: 66 	 Loss: 11.701262
Train Epoch: 67 	 Loss: 11.435692
Train Epoch: 68 	 Loss: 11.904333
Train Epoch: 69 	 Loss: 11.575572
Train Epoch: 70 	 Loss: 11.556230
Train Epoch: 71 	 Loss: 11.880298
Train Epoch: 72 	 Loss: 12.105214
Train Epoch: 73 	 Loss: 11.796593
Train Epoch: 74 	 Loss: 11.805658
Train Epoch: 75 	 Loss: 11.707878
Train Epoch: 76 	 Loss: 11.722972
Train Epoch: 77 	 Loss: 12.114182
Train Epoch: 78 	 Loss: 11.707262
Train Epoch: 79 	 Loss: 12.084269
Train Epoch: 80 	 Loss: 11.681674
Train Epoch: 81 	 Loss: 11.831752
Train Epoch: 82 	 Loss: 11.950826
Train Epoch: 83 	 Loss: 11.773390
Train Epoch: 84 	 Loss: 11.662889
Train Epoch: 85 	 Loss: 11.933163
Train Epoch: 86 	 Loss: 11.995975
Train Epoch: 87 	 Loss: 11.771065
Train Epoch: 88 	 Loss: 11.847990
Train Epoch: 89 	 Loss: 11.965131
Train Epoch: 90 	 Loss: 11.811256
Train Epoch: 91 	 Loss: 11.524270
Train Epoch: 92 	 Loss: 11.549944
Train Epoch: 93 	 Loss: 11.687618
Train Epoch: 94 	 Loss: 11.958128
Train Epoch: 95 	 Loss: 11.632339
Train Epoch: 96 	 Loss: 11.945601
Train Epoch: 97 	 Loss: 11.782656
Train Epoch: 98 	 Loss: 11.701842
Train Epoch: 99 	 Loss: 11.769068

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.615

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.836

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.687

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.318

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.845

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.886

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.922

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.654

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.462

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.894

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.322

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.905
------
f1 mean across methods is 0.696


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.88766768e-01 4.84896435e-01 2.05562446e-02 5.52395615e-03
 2.56596673e-04] 

Train Epoch: 0 	 Loss: 18.922192
Train Epoch: 1 	 Loss: 17.903818
Train Epoch: 2 	 Loss: 15.931541
Train Epoch: 3 	 Loss: 14.247856
Train Epoch: 4 	 Loss: 14.700611
Train Epoch: 5 	 Loss: 13.729069
Train Epoch: 6 	 Loss: 14.546904
Train Epoch: 7 	 Loss: 13.889200
Train Epoch: 8 	 Loss: 13.911611
Train Epoch: 9 	 Loss: 13.883633
Train Epoch: 10 	 Loss: 14.021912
Train Epoch: 11 	 Loss: 14.245755
Train Epoch: 12 	 Loss: 13.552754
Train Epoch: 13 	 Loss: 13.680307
Train Epoch: 14 	 Loss: 14.022622
Train Epoch: 15 	 Loss: 13.610122
Train Epoch: 16 	 Loss: 14.380266
Train Epoch: 17 	 Loss: 14.424944
Train Epoch: 18 	 Loss: 13.825761
Train Epoch: 19 	 Loss: 14.218194
Train Epoch: 20 	 Loss: 13.913939
Train Epoch: 21 	 Loss: 14.615968
Train Epoch: 22 	 Loss: 13.408525
Train Epoch: 23 	 Loss: 13.435347
Train Epoch: 24 	 Loss: 13.943546
Train Epoch: 25 	 Loss: 13.593967
Train Epoch: 26 	 Loss: 13.670713
Train Epoch: 27 	 Loss: 13.090506
Train Epoch: 28 	 Loss: 13.439477
Train Epoch: 29 	 Loss: 13.213194
Train Epoch: 30 	 Loss: 13.467354
Train Epoch: 31 	 Loss: 13.119328
Train Epoch: 32 	 Loss: 13.520796
Train Epoch: 33 	 Loss: 13.244184
Train Epoch: 34 	 Loss: 13.040520
Train Epoch: 35 	 Loss: 12.976731
Train Epoch: 36 	 Loss: 12.940205
Train Epoch: 37 	 Loss: 13.249205
Train Epoch: 38 	 Loss: 13.726066
Train Epoch: 39 	 Loss: 12.958315
Train Epoch: 40 	 Loss: 12.947561
Train Epoch: 41 	 Loss: 13.212751
Train Epoch: 42 	 Loss: 13.071884
Train Epoch: 43 	 Loss: 13.456596
Train Epoch: 44 	 Loss: 13.136139
Train Epoch: 45 	 Loss: 12.977013
Train Epoch: 46 	 Loss: 12.814360
Train Epoch: 47 	 Loss: 13.311665
Train Epoch: 48 	 Loss: 12.815949
Train Epoch: 49 	 Loss: 12.875729
Train Epoch: 50 	 Loss: 12.635168
Train Epoch: 51 	 Loss: 13.097879
Train Epoch: 52 	 Loss: 12.736656
Train Epoch: 53 	 Loss: 12.953640
Train Epoch: 54 	 Loss: 13.240101
Train Epoch: 55 	 Loss: 13.529055
Train Epoch: 56 	 Loss: 13.055929
Train Epoch: 57 	 Loss: 12.909550
Train Epoch: 58 	 Loss: 13.049274
Train Epoch: 59 	 Loss: 12.770225
Train Epoch: 60 	 Loss: 12.791718
Train Epoch: 61 	 Loss: 12.977793
Train Epoch: 62 	 Loss: 12.909750
Train Epoch: 63 	 Loss: 12.978099
Train Epoch: 64 	 Loss: 12.762839
Train Epoch: 65 	 Loss: 13.576540
Train Epoch: 66 	 Loss: 13.588488
Train Epoch: 67 	 Loss: 13.017561
Train Epoch: 68 	 Loss: 12.824148
Train Epoch: 69 	 Loss: 13.176210
Train Epoch: 70 	 Loss: 12.888571
Train Epoch: 71 	 Loss: 12.983038
Train Epoch: 72 	 Loss: 13.009201
Train Epoch: 73 	 Loss: 13.061596
Train Epoch: 74 	 Loss: 12.798480
Train Epoch: 75 	 Loss: 12.907301
Train Epoch: 76 	 Loss: 12.907965
Train Epoch: 77 	 Loss: 12.882883
Train Epoch: 78 	 Loss: 12.705094
Train Epoch: 79 	 Loss: 12.783419
Train Epoch: 80 	 Loss: 12.979468
Train Epoch: 81 	 Loss: 12.925182
Train Epoch: 82 	 Loss: 12.654950
Train Epoch: 83 	 Loss: 12.755335
Train Epoch: 84 	 Loss: 12.792111
Train Epoch: 85 	 Loss: 12.947670
Train Epoch: 86 	 Loss: 12.673718
Train Epoch: 87 	 Loss: 13.102387
Train Epoch: 88 	 Loss: 12.756247
Train Epoch: 89 	 Loss: 12.822690
Train Epoch: 90 	 Loss: 13.241392
Train Epoch: 91 	 Loss: 13.150480
Train Epoch: 92 	 Loss: 13.232804
Train Epoch: 93 	 Loss: 13.567744
Train Epoch: 94 	 Loss: 13.329335
Train Epoch: 95 	 Loss: 13.281105
Train Epoch: 96 	 Loss: 13.685761
Train Epoch: 97 	 Loss: 13.014853
Train Epoch: 98 	 Loss: 13.516006
Train Epoch: 99 	 Loss: 13.126849

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.646

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.467

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.820

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.736

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.904

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.292

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.912

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.862

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.941

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.806

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.756

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.918
------
f1 mean across methods is 0.755


 ---------------------------------------- 


For each of the methods
Average F1:  [0.76338406 0.52900246 0.82223892 0.70811912 0.88661951 0.60731695
 0.90992824 0.83603002 0.82247414 0.87139649 0.72554272 0.90361265]
Std F1:  [0.10975786 0.16265696 0.07123197 0.19925909 0.0259095  0.3852991
 0.02557777 0.09221378 0.18075209 0.03485914 0.20645345 0.0085693 ]
Average over repetitions across all methods
Average f1 score:  0.7821387732561937
Std F1:  0.05788919463399476

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 100, 'order_hermite': 100, 'subsampled_rate': 0.3} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33142857e-01 4.43051948e-01 1.84025974e-02 5.18831169e-03
 2.14285714e-04] 

Train Epoch: 0 	 Loss: 19.144522
Train Epoch: 1 	 Loss: 19.588211
Train Epoch: 2 	 Loss: 17.651449
Train Epoch: 3 	 Loss: 16.122971
Train Epoch: 4 	 Loss: 16.016134
Train Epoch: 5 	 Loss: 16.204262
Train Epoch: 6 	 Loss: 15.341318
Train Epoch: 7 	 Loss: 14.934291
Train Epoch: 8 	 Loss: 14.897141
Train Epoch: 9 	 Loss: 14.804132
Train Epoch: 10 	 Loss: 14.496420
Train Epoch: 11 	 Loss: 14.626291
Train Epoch: 12 	 Loss: 14.752504
Train Epoch: 13 	 Loss: 14.595014
Train Epoch: 14 	 Loss: 14.525738
Train Epoch: 15 	 Loss: 14.597544
Train Epoch: 16 	 Loss: 14.602007
Train Epoch: 17 	 Loss: 14.439458
Train Epoch: 18 	 Loss: 14.357130
Train Epoch: 19 	 Loss: 14.624179
Train Epoch: 20 	 Loss: 14.859174
Train Epoch: 21 	 Loss: 14.358953
Train Epoch: 22 	 Loss: 14.799799
Train Epoch: 23 	 Loss: 14.358579
Train Epoch: 24 	 Loss: 14.263575
Train Epoch: 25 	 Loss: 14.210407
Train Epoch: 26 	 Loss: 14.030540
Train Epoch: 27 	 Loss: 14.290657
Train Epoch: 28 	 Loss: 14.359901
Train Epoch: 29 	 Loss: 14.348560
Train Epoch: 30 	 Loss: 14.532791
Train Epoch: 31 	 Loss: 14.241428
Train Epoch: 32 	 Loss: 14.286105
Train Epoch: 33 	 Loss: 14.033591
Train Epoch: 34 	 Loss: 14.384226
Train Epoch: 35 	 Loss: 13.828279
Train Epoch: 36 	 Loss: 14.425438
Train Epoch: 37 	 Loss: 14.233273
Train Epoch: 38 	 Loss: 14.241979
Train Epoch: 39 	 Loss: 14.634806
Train Epoch: 40 	 Loss: 14.184274
Train Epoch: 41 	 Loss: 14.329414
Train Epoch: 42 	 Loss: 14.120823
Train Epoch: 43 	 Loss: 14.492842
Train Epoch: 44 	 Loss: 14.412132
Train Epoch: 45 	 Loss: 14.670590
Train Epoch: 46 	 Loss: 14.169340
Train Epoch: 47 	 Loss: 14.103454
Train Epoch: 48 	 Loss: 13.961655
Train Epoch: 49 	 Loss: 14.240009
Train Epoch: 50 	 Loss: 14.310624
Train Epoch: 51 	 Loss: 14.041561
Train Epoch: 52 	 Loss: 14.007963
Train Epoch: 53 	 Loss: 14.002942
Train Epoch: 54 	 Loss: 14.416217
Train Epoch: 55 	 Loss: 14.052011
Train Epoch: 56 	 Loss: 14.276868
Train Epoch: 57 	 Loss: 14.042115
Train Epoch: 58 	 Loss: 14.185779
Train Epoch: 59 	 Loss: 14.262557
Train Epoch: 60 	 Loss: 14.272461
Train Epoch: 61 	 Loss: 14.010170
Train Epoch: 62 	 Loss: 14.180986
Train Epoch: 63 	 Loss: 14.377742
Train Epoch: 64 	 Loss: 13.962361
Train Epoch: 65 	 Loss: 13.986710
Train Epoch: 66 	 Loss: 13.894290
Train Epoch: 67 	 Loss: 14.262258
Train Epoch: 68 	 Loss: 13.667583
Train Epoch: 69 	 Loss: 14.083480
Train Epoch: 70 	 Loss: 14.039097
Train Epoch: 71 	 Loss: 13.661047
Train Epoch: 72 	 Loss: 14.184382
Train Epoch: 73 	 Loss: 14.183268
Train Epoch: 74 	 Loss: 13.929293
Train Epoch: 75 	 Loss: 14.321244
Train Epoch: 76 	 Loss: 14.041365
Train Epoch: 77 	 Loss: 14.160582
Train Epoch: 78 	 Loss: 14.113342
Train Epoch: 79 	 Loss: 14.026298
Train Epoch: 80 	 Loss: 14.062395
Train Epoch: 81 	 Loss: 14.161888
Train Epoch: 82 	 Loss: 13.884436
Train Epoch: 83 	 Loss: 14.014536
Train Epoch: 84 	 Loss: 14.249616
Train Epoch: 85 	 Loss: 14.170710
Train Epoch: 86 	 Loss: 14.010597
Train Epoch: 87 	 Loss: 14.151196
Train Epoch: 88 	 Loss: 13.971986
Train Epoch: 89 	 Loss: 14.119176
Train Epoch: 90 	 Loss: 14.133591
Train Epoch: 91 	 Loss: 14.239960
Train Epoch: 92 	 Loss: 14.111074
Train Epoch: 93 	 Loss: 14.344243
Train Epoch: 94 	 Loss: 14.400615
Train Epoch: 95 	 Loss: 14.247883
Train Epoch: 96 	 Loss: 14.058815
Train Epoch: 97 	 Loss: 14.407993
Train Epoch: 98 	 Loss: 14.110975
Train Epoch: 99 	 Loss: 14.265868

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.899

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.212

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.828

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.858

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.907

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.909

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.942

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.921

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.936

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.894

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.873

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.924
------
f1 mean across methods is 0.842


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.34577922e-01 4.41084416e-01 1.90324675e-02 5.05844156e-03
 2.46753247e-04] 

Train Epoch: 0 	 Loss: 18.124853
Train Epoch: 1 	 Loss: 18.631969
Train Epoch: 2 	 Loss: 17.296181
Train Epoch: 3 	 Loss: 14.880435
Train Epoch: 4 	 Loss: 14.732220
Train Epoch: 5 	 Loss: 14.093728
Train Epoch: 6 	 Loss: 14.487486
Train Epoch: 7 	 Loss: 13.800469
Train Epoch: 8 	 Loss: 13.911339
Train Epoch: 9 	 Loss: 13.198982
Train Epoch: 10 	 Loss: 13.845094
Train Epoch: 11 	 Loss: 13.910299
Train Epoch: 12 	 Loss: 13.474617
Train Epoch: 13 	 Loss: 13.547945
Train Epoch: 14 	 Loss: 13.112865
Train Epoch: 15 	 Loss: 12.909004
Train Epoch: 16 	 Loss: 13.781166
Train Epoch: 17 	 Loss: 13.292560
Train Epoch: 18 	 Loss: 13.710798
Train Epoch: 19 	 Loss: 13.131785
Train Epoch: 20 	 Loss: 13.271366
Train Epoch: 21 	 Loss: 12.967505
Train Epoch: 22 	 Loss: 13.135360
Train Epoch: 23 	 Loss: 12.922943
Train Epoch: 24 	 Loss: 12.922305
Train Epoch: 25 	 Loss: 12.848589
Train Epoch: 26 	 Loss: 13.001250
Train Epoch: 27 	 Loss: 13.341163
Train Epoch: 28 	 Loss: 12.610598
Train Epoch: 29 	 Loss: 12.662455
Train Epoch: 30 	 Loss: 12.722017
Train Epoch: 31 	 Loss: 12.835283
Train Epoch: 32 	 Loss: 14.280836
Train Epoch: 33 	 Loss: 12.668760
Train Epoch: 34 	 Loss: 12.782228
Train Epoch: 35 	 Loss: 13.011469
Train Epoch: 36 	 Loss: 12.450033
Train Epoch: 37 	 Loss: 12.619995
Train Epoch: 38 	 Loss: 13.069895
Train Epoch: 39 	 Loss: 12.798365
Train Epoch: 40 	 Loss: 12.595342
Train Epoch: 41 	 Loss: 12.913449
Train Epoch: 42 	 Loss: 12.644095
Train Epoch: 43 	 Loss: 12.685526
Train Epoch: 44 	 Loss: 12.616435
Train Epoch: 45 	 Loss: 12.714490
Train Epoch: 46 	 Loss: 13.134279
Train Epoch: 47 	 Loss: 12.602060
Train Epoch: 48 	 Loss: 12.800508
Train Epoch: 49 	 Loss: 12.856371
Train Epoch: 50 	 Loss: 12.875090
Train Epoch: 51 	 Loss: 12.773582
Train Epoch: 52 	 Loss: 13.210861
Train Epoch: 53 	 Loss: 12.959438
Train Epoch: 54 	 Loss: 12.891850
Train Epoch: 55 	 Loss: 12.518320
Train Epoch: 56 	 Loss: 12.575484
Train Epoch: 57 	 Loss: 12.493258
Train Epoch: 58 	 Loss: 12.700705
Train Epoch: 59 	 Loss: 12.544965
Train Epoch: 60 	 Loss: 12.779249
Train Epoch: 61 	 Loss: 12.695405
Train Epoch: 62 	 Loss: 13.109056
Train Epoch: 63 	 Loss: 12.685787
Train Epoch: 64 	 Loss: 12.656931
Train Epoch: 65 	 Loss: 12.490608
Train Epoch: 66 	 Loss: 12.683909
Train Epoch: 67 	 Loss: 12.462630
Train Epoch: 68 	 Loss: 12.615356
Train Epoch: 69 	 Loss: 12.731173
Train Epoch: 70 	 Loss: 12.491970
Train Epoch: 71 	 Loss: 12.807375
Train Epoch: 72 	 Loss: 12.740666
Train Epoch: 73 	 Loss: 12.768351
Train Epoch: 74 	 Loss: 12.841529
Train Epoch: 75 	 Loss: 12.958569
Train Epoch: 76 	 Loss: 12.985478
Train Epoch: 77 	 Loss: 13.136726
Train Epoch: 78 	 Loss: 13.148418
Train Epoch: 79 	 Loss: 13.074400
Train Epoch: 80 	 Loss: 13.017067
Train Epoch: 81 	 Loss: 12.905858
Train Epoch: 82 	 Loss: 13.045444
Train Epoch: 83 	 Loss: 13.194515
Train Epoch: 84 	 Loss: 13.370889
Train Epoch: 85 	 Loss: 12.788685
Train Epoch: 86 	 Loss: 13.587141
Train Epoch: 87 	 Loss: 12.647490
Train Epoch: 88 	 Loss: 12.558365
Train Epoch: 89 	 Loss: 12.705017
Train Epoch: 90 	 Loss: 12.954728
Train Epoch: 91 	 Loss: 14.103370
Train Epoch: 92 	 Loss: 12.763224
Train Epoch: 93 	 Loss: 12.737278
Train Epoch: 94 	 Loss: 12.457731
Train Epoch: 95 	 Loss: 12.492089
Train Epoch: 96 	 Loss: 12.504740
Train Epoch: 97 	 Loss: 12.466950
Train Epoch: 98 	 Loss: 12.703510
Train Epoch: 99 	 Loss: 13.351461

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.863

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.514

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.822

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.838

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.930

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.926

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.928

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.895

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.915

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.913

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.779

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.934
------
f1 mean across methods is 0.855


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.32896104e-01 4.43051948e-01 1.85844156e-02 5.24675325e-03
 2.20779221e-04] 

Train Epoch: 0 	 Loss: 20.874138
Train Epoch: 1 	 Loss: 19.796021
Train Epoch: 2 	 Loss: 17.812675
Train Epoch: 3 	 Loss: 16.871151
Train Epoch: 4 	 Loss: 16.448935
Train Epoch: 5 	 Loss: 16.995855
Train Epoch: 6 	 Loss: 15.719928
Train Epoch: 7 	 Loss: 15.348774
Train Epoch: 8 	 Loss: 15.658410
Train Epoch: 9 	 Loss: 16.034039
Train Epoch: 10 	 Loss: 15.969364
Train Epoch: 11 	 Loss: 15.431383
Train Epoch: 12 	 Loss: 15.806292
Train Epoch: 13 	 Loss: 15.449503
Train Epoch: 14 	 Loss: 15.052010
Train Epoch: 15 	 Loss: 15.814001
Train Epoch: 16 	 Loss: 15.740526
Train Epoch: 17 	 Loss: 15.716660
Train Epoch: 18 	 Loss: 15.137035
Train Epoch: 19 	 Loss: 14.825912
Train Epoch: 20 	 Loss: 14.827227
Train Epoch: 21 	 Loss: 15.410015
Train Epoch: 22 	 Loss: 15.079855
Train Epoch: 23 	 Loss: 15.712200
Train Epoch: 24 	 Loss: 14.965808
Train Epoch: 25 	 Loss: 14.789942
Train Epoch: 26 	 Loss: 14.820366
Train Epoch: 27 	 Loss: 15.026153
Train Epoch: 28 	 Loss: 16.031643
Train Epoch: 29 	 Loss: 15.274955
Train Epoch: 30 	 Loss: 14.692184
Train Epoch: 31 	 Loss: 16.340918
Train Epoch: 32 	 Loss: 15.532509
Train Epoch: 33 	 Loss: 15.437057
Train Epoch: 34 	 Loss: 14.853823
Train Epoch: 35 	 Loss: 15.073911
Train Epoch: 36 	 Loss: 15.039131
Train Epoch: 37 	 Loss: 14.706065
Train Epoch: 38 	 Loss: 14.802957
Train Epoch: 39 	 Loss: 15.359533
Train Epoch: 40 	 Loss: 14.636162
Train Epoch: 41 	 Loss: 14.178110
Train Epoch: 42 	 Loss: 14.741489
Train Epoch: 43 	 Loss: 14.712596
Train Epoch: 44 	 Loss: 14.685606
Train Epoch: 45 	 Loss: 14.498716
Train Epoch: 46 	 Loss: 14.138613
Train Epoch: 47 	 Loss: 14.580093
Train Epoch: 48 	 Loss: 14.390778
Train Epoch: 49 	 Loss: 14.535331
Train Epoch: 50 	 Loss: 15.326144
Train Epoch: 51 	 Loss: 14.604027
Train Epoch: 52 	 Loss: 14.710457
Train Epoch: 53 	 Loss: 14.812831
Train Epoch: 54 	 Loss: 14.753220
Train Epoch: 55 	 Loss: 14.332342
Train Epoch: 56 	 Loss: 14.314315
Train Epoch: 57 	 Loss: 14.165824
Train Epoch: 58 	 Loss: 13.613750
Train Epoch: 59 	 Loss: 15.007983
Train Epoch: 60 	 Loss: 15.318485
Train Epoch: 61 	 Loss: 14.443429
Train Epoch: 62 	 Loss: 13.686503
Train Epoch: 63 	 Loss: 14.216562
Train Epoch: 64 	 Loss: 13.987743
Train Epoch: 65 	 Loss: 13.943589
Train Epoch: 66 	 Loss: 14.839745
Train Epoch: 67 	 Loss: 14.002029
Train Epoch: 68 	 Loss: 14.901577
Train Epoch: 69 	 Loss: 13.996595
Train Epoch: 70 	 Loss: 13.969364
Train Epoch: 71 	 Loss: 14.269505
Train Epoch: 72 	 Loss: 14.120985
Train Epoch: 73 	 Loss: 13.916895
Train Epoch: 74 	 Loss: 14.246515
Train Epoch: 75 	 Loss: 14.503222
Train Epoch: 76 	 Loss: 14.213232
Train Epoch: 77 	 Loss: 14.239704
Train Epoch: 78 	 Loss: 14.675268
Train Epoch: 79 	 Loss: 14.988167
Train Epoch: 80 	 Loss: 14.263517
Train Epoch: 81 	 Loss: 14.744651
Train Epoch: 82 	 Loss: 14.437950
Train Epoch: 83 	 Loss: 14.028419
Train Epoch: 84 	 Loss: 13.822789
Train Epoch: 85 	 Loss: 14.042795
Train Epoch: 86 	 Loss: 13.977705
Train Epoch: 87 	 Loss: 13.766373
Train Epoch: 88 	 Loss: 13.628066
Train Epoch: 89 	 Loss: 13.964288
Train Epoch: 90 	 Loss: 14.108099
Train Epoch: 91 	 Loss: 14.328738
Train Epoch: 92 	 Loss: 13.890357
Train Epoch: 93 	 Loss: 14.475936
Train Epoch: 94 	 Loss: 14.391314
Train Epoch: 95 	 Loss: 14.395068
Train Epoch: 96 	 Loss: 14.218904
Train Epoch: 97 	 Loss: 14.481829
Train Epoch: 98 	 Loss: 14.018847
Train Epoch: 99 	 Loss: 14.051621

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.856

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.499

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.807

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.849

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.922

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.925

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.925

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.898

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.908

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.896

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.862

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.923
------
f1 mean across methods is 0.856


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33707792e-01 4.42259740e-01 1.85194805e-02 5.28571429e-03
 2.27272727e-04] 

Train Epoch: 0 	 Loss: 18.195457
Train Epoch: 1 	 Loss: 17.594297
Train Epoch: 2 	 Loss: 16.121742
Train Epoch: 3 	 Loss: 14.152334
Train Epoch: 4 	 Loss: 13.950711
Train Epoch: 5 	 Loss: 13.846212
Train Epoch: 6 	 Loss: 13.488369
Train Epoch: 7 	 Loss: 13.684174
Train Epoch: 8 	 Loss: 13.474729
Train Epoch: 9 	 Loss: 13.275349
Train Epoch: 10 	 Loss: 12.885484
Train Epoch: 11 	 Loss: 13.178287
Train Epoch: 12 	 Loss: 12.740669
Train Epoch: 13 	 Loss: 13.081150
Train Epoch: 14 	 Loss: 12.876423
Train Epoch: 15 	 Loss: 12.730478
Train Epoch: 16 	 Loss: 12.966283
Train Epoch: 17 	 Loss: 13.369546
Train Epoch: 18 	 Loss: 13.121220
Train Epoch: 19 	 Loss: 12.503836
Train Epoch: 20 	 Loss: 12.793516
Train Epoch: 21 	 Loss: 13.159192
Train Epoch: 22 	 Loss: 13.064341
Train Epoch: 23 	 Loss: 12.614951
Train Epoch: 24 	 Loss: 13.033438
Train Epoch: 25 	 Loss: 12.665775
Train Epoch: 26 	 Loss: 12.618650
Train Epoch: 27 	 Loss: 12.363590
Train Epoch: 28 	 Loss: 12.751139
Train Epoch: 29 	 Loss: 12.829481
Train Epoch: 30 	 Loss: 12.889157
Train Epoch: 31 	 Loss: 13.093529
Train Epoch: 32 	 Loss: 12.744843
Train Epoch: 33 	 Loss: 12.669792
Train Epoch: 34 	 Loss: 12.663002
Train Epoch: 35 	 Loss: 12.512404
Train Epoch: 36 	 Loss: 12.364799
Train Epoch: 37 	 Loss: 12.655624
Train Epoch: 38 	 Loss: 12.432058
Train Epoch: 39 	 Loss: 12.360291
Train Epoch: 40 	 Loss: 12.391401
Train Epoch: 41 	 Loss: 12.408863
Train Epoch: 42 	 Loss: 12.460465
Train Epoch: 43 	 Loss: 12.803367
Train Epoch: 44 	 Loss: 12.523514
Train Epoch: 45 	 Loss: 12.524124
Train Epoch: 46 	 Loss: 12.738698
Train Epoch: 47 	 Loss: 12.400144
Train Epoch: 48 	 Loss: 12.756339
Train Epoch: 49 	 Loss: 12.287663
Train Epoch: 50 	 Loss: 12.536085
Train Epoch: 51 	 Loss: 12.345355
Train Epoch: 52 	 Loss: 12.306797
Train Epoch: 53 	 Loss: 12.582758
Train Epoch: 54 	 Loss: 12.397156
Train Epoch: 55 	 Loss: 12.374218
Train Epoch: 56 	 Loss: 12.217711
Train Epoch: 57 	 Loss: 12.193665
Train Epoch: 58 	 Loss: 12.216087
Train Epoch: 59 	 Loss: 12.230973
Train Epoch: 60 	 Loss: 12.406490
Train Epoch: 61 	 Loss: 12.483911
Train Epoch: 62 	 Loss: 12.312506
Train Epoch: 63 	 Loss: 12.346819
Train Epoch: 64 	 Loss: 12.188952
Train Epoch: 65 	 Loss: 12.434017
Train Epoch: 66 	 Loss: 12.320064
Train Epoch: 67 	 Loss: 12.276440
Train Epoch: 68 	 Loss: 12.256042
Train Epoch: 69 	 Loss: 12.260512
Train Epoch: 70 	 Loss: 12.024727
Train Epoch: 71 	 Loss: 12.356818
Train Epoch: 72 	 Loss: 12.273771
Train Epoch: 73 	 Loss: 12.614729
Train Epoch: 74 	 Loss: 12.333183
Train Epoch: 75 	 Loss: 12.195968
Train Epoch: 76 	 Loss: 12.344252
Train Epoch: 77 	 Loss: 12.529660
Train Epoch: 78 	 Loss: 12.447969
Train Epoch: 79 	 Loss: 12.487697
Train Epoch: 80 	 Loss: 12.227678
Train Epoch: 81 	 Loss: 12.434479
Train Epoch: 82 	 Loss: 12.098355
Train Epoch: 83 	 Loss: 12.068283
Train Epoch: 84 	 Loss: 12.484797
Train Epoch: 85 	 Loss: 12.949636
Train Epoch: 86 	 Loss: 13.509980
Train Epoch: 87 	 Loss: 12.700275
Train Epoch: 88 	 Loss: 13.382055
Train Epoch: 89 	 Loss: 12.556948
Train Epoch: 90 	 Loss: 12.497818
Train Epoch: 91 	 Loss: 12.286068
Train Epoch: 92 	 Loss: 12.436430
Train Epoch: 93 	 Loss: 12.244827
Train Epoch: 94 	 Loss: 12.153341
Train Epoch: 95 	 Loss: 12.115412
Train Epoch: 96 	 Loss: 12.642561
Train Epoch: 97 	 Loss: 12.157507
Train Epoch: 98 	 Loss: 11.980001
Train Epoch: 99 	 Loss: 12.170408

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.899

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.510

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.908

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.855

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.889

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.917

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.913

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.893

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.900

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.906

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.873

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.909
------
f1 mean across methods is 0.864


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33701299e-01 4.42097403e-01 1.89155844e-02 5.08441558e-03
 2.01298701e-04] 

Train Epoch: 0 	 Loss: 20.776377
Train Epoch: 1 	 Loss: 19.164221
Train Epoch: 2 	 Loss: 17.042656
Train Epoch: 3 	 Loss: 14.994326
Train Epoch: 4 	 Loss: 14.841972
Train Epoch: 5 	 Loss: 14.534538
Train Epoch: 6 	 Loss: 15.083827
Train Epoch: 7 	 Loss: 14.564001
Train Epoch: 8 	 Loss: 15.216332
Train Epoch: 9 	 Loss: 14.702068
Train Epoch: 10 	 Loss: 14.718060
Train Epoch: 11 	 Loss: 14.245117
Train Epoch: 12 	 Loss: 14.309975
Train Epoch: 13 	 Loss: 14.468809
Train Epoch: 14 	 Loss: 15.209157
Train Epoch: 15 	 Loss: 14.360945
Train Epoch: 16 	 Loss: 14.185789
Train Epoch: 17 	 Loss: 15.048300
Train Epoch: 18 	 Loss: 15.447106
Train Epoch: 19 	 Loss: 14.615128
Train Epoch: 20 	 Loss: 14.499645
Train Epoch: 21 	 Loss: 15.196897
Train Epoch: 22 	 Loss: 14.120544
Train Epoch: 23 	 Loss: 14.017151
Train Epoch: 24 	 Loss: 14.302438
Train Epoch: 25 	 Loss: 14.132799
Train Epoch: 26 	 Loss: 14.014935
Train Epoch: 27 	 Loss: 14.345749
Train Epoch: 28 	 Loss: 13.991926
Train Epoch: 29 	 Loss: 14.345984
Train Epoch: 30 	 Loss: 13.786742
Train Epoch: 31 	 Loss: 14.303919
Train Epoch: 32 	 Loss: 14.141318
Train Epoch: 33 	 Loss: 13.851747
Train Epoch: 34 	 Loss: 14.171982
Train Epoch: 35 	 Loss: 13.898417
Train Epoch: 36 	 Loss: 13.968186
Train Epoch: 37 	 Loss: 13.934003
Train Epoch: 38 	 Loss: 13.937574
Train Epoch: 39 	 Loss: 13.818976
Train Epoch: 40 	 Loss: 13.887242
Train Epoch: 41 	 Loss: 14.116334
Train Epoch: 42 	 Loss: 13.653538
Train Epoch: 43 	 Loss: 13.792737
Train Epoch: 44 	 Loss: 14.389322
Train Epoch: 45 	 Loss: 14.568588
Train Epoch: 46 	 Loss: 14.630435
Train Epoch: 47 	 Loss: 13.770823
Train Epoch: 48 	 Loss: 13.990319
Train Epoch: 49 	 Loss: 13.888586
Train Epoch: 50 	 Loss: 13.965976
Train Epoch: 51 	 Loss: 14.062792
Train Epoch: 52 	 Loss: 13.759802
Train Epoch: 53 	 Loss: 14.236927
Train Epoch: 54 	 Loss: 14.606467
Train Epoch: 55 	 Loss: 13.846302
Train Epoch: 56 	 Loss: 13.900917
Train Epoch: 57 	 Loss: 14.422901
Train Epoch: 58 	 Loss: 13.903508
Train Epoch: 59 	 Loss: 13.693917
Train Epoch: 60 	 Loss: 13.941113
Train Epoch: 61 	 Loss: 13.680580
Train Epoch: 62 	 Loss: 14.065952
Train Epoch: 63 	 Loss: 13.651041
Train Epoch: 64 	 Loss: 13.795249
Train Epoch: 65 	 Loss: 13.804272
Train Epoch: 66 	 Loss: 13.815193
Train Epoch: 67 	 Loss: 14.180124
Train Epoch: 68 	 Loss: 13.806890
Train Epoch: 69 	 Loss: 13.966453
Train Epoch: 70 	 Loss: 14.244006
Train Epoch: 71 	 Loss: 14.065477
Train Epoch: 72 	 Loss: 14.737224
Train Epoch: 73 	 Loss: 13.851089
Train Epoch: 74 	 Loss: 13.837624
Train Epoch: 75 	 Loss: 13.685274
Train Epoch: 76 	 Loss: 13.970950
Train Epoch: 77 	 Loss: 13.808129
Train Epoch: 78 	 Loss: 13.646973
Train Epoch: 79 	 Loss: 14.068430
Train Epoch: 80 	 Loss: 13.878825
Train Epoch: 81 	 Loss: 14.181379
Train Epoch: 82 	 Loss: 13.847582
Train Epoch: 83 	 Loss: 13.797745
Train Epoch: 84 	 Loss: 13.735661
Train Epoch: 85 	 Loss: 13.799366
Train Epoch: 86 	 Loss: 14.898909
Train Epoch: 87 	 Loss: 13.870283
Train Epoch: 88 	 Loss: 13.717607
Train Epoch: 89 	 Loss: 14.303270
Train Epoch: 90 	 Loss: 14.305172
Train Epoch: 91 	 Loss: 13.844888
Train Epoch: 92 	 Loss: 13.750898
Train Epoch: 93 	 Loss: 13.700689
Train Epoch: 94 	 Loss: 13.655646
Train Epoch: 95 	 Loss: 13.984053
Train Epoch: 96 	 Loss: 13.762762
Train Epoch: 97 	 Loss: 13.837141
Train Epoch: 98 	 Loss: 15.102402
Train Epoch: 99 	 Loss: 14.554123

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.873

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.499

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.870

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.875

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.927

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.943

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.917

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.867

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.900

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.873

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.871

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.906
------
f1 mean across methods is 0.860


 ---------------------------------------- 


For each of the methods
Average F1:  [0.87793966 0.44684986 0.84697605 0.85482048 0.91514702 0.92397695
 0.92513257 0.894786   0.91181245 0.8962901  0.85165745 0.91906112]
Std F1:  [0.01793739 0.11759113 0.03719423 0.01196055 0.01524534 0.01134047
 0.01016924 0.01724744 0.01307741 0.01345645 0.03669967 0.01055619]
Average over repetitions across all methods
Average f1 score:  0.8553708099514283
Std F1:  0.007555300768172536

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 100, 'order_hermite': 100, 'subsampled_rate': 0.35} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.70986458e-01 4.06980280e-01 1.71018658e-02 4.73461697e-03
 1.96778791e-04] 

Train Epoch: 0 	 Loss: 20.811316
Train Epoch: 1 	 Loss: 19.983551
Train Epoch: 2 	 Loss: 17.276768
Train Epoch: 3 	 Loss: 16.535875
Train Epoch: 4 	 Loss: 15.916771
Train Epoch: 5 	 Loss: 15.164578
Train Epoch: 6 	 Loss: 15.318086
Train Epoch: 7 	 Loss: 15.160239
Train Epoch: 8 	 Loss: 14.758664
Train Epoch: 9 	 Loss: 14.676625
Train Epoch: 10 	 Loss: 15.120211
Train Epoch: 11 	 Loss: 15.015071
Train Epoch: 12 	 Loss: 14.571696
Train Epoch: 13 	 Loss: 14.899988
Train Epoch: 14 	 Loss: 14.394508
Train Epoch: 15 	 Loss: 15.470976
Train Epoch: 16 	 Loss: 14.699060
Train Epoch: 17 	 Loss: 14.764431
Train Epoch: 18 	 Loss: 14.168644
Train Epoch: 19 	 Loss: 14.419707
Train Epoch: 20 	 Loss: 14.100105
Train Epoch: 21 	 Loss: 13.911255
Train Epoch: 22 	 Loss: 14.777864
Train Epoch: 23 	 Loss: 14.713539
Train Epoch: 24 	 Loss: 15.575183
Train Epoch: 25 	 Loss: 15.128736
Train Epoch: 26 	 Loss: 14.907680
Train Epoch: 27 	 Loss: 14.787310
Train Epoch: 28 	 Loss: 14.175578
Train Epoch: 29 	 Loss: 14.709157
Train Epoch: 30 	 Loss: 14.354911
Train Epoch: 31 	 Loss: 14.040731
Train Epoch: 32 	 Loss: 14.119297
Train Epoch: 33 	 Loss: 14.350216
Train Epoch: 34 	 Loss: 14.079094
Train Epoch: 35 	 Loss: 14.976688
Train Epoch: 36 	 Loss: 14.075720
Train Epoch: 37 	 Loss: 14.220105
Train Epoch: 38 	 Loss: 14.307342
Train Epoch: 39 	 Loss: 13.859253
Train Epoch: 40 	 Loss: 14.051283
Train Epoch: 41 	 Loss: 13.856730
Train Epoch: 42 	 Loss: 14.054708
Train Epoch: 43 	 Loss: 14.119502
Train Epoch: 44 	 Loss: 14.361579
Train Epoch: 45 	 Loss: 13.822387
Train Epoch: 46 	 Loss: 14.001991
Train Epoch: 47 	 Loss: 14.282759
Train Epoch: 48 	 Loss: 14.411063
Train Epoch: 49 	 Loss: 13.937147
Train Epoch: 50 	 Loss: 13.883197
Train Epoch: 51 	 Loss: 14.247435
Train Epoch: 52 	 Loss: 14.555028
Train Epoch: 53 	 Loss: 14.520420
Train Epoch: 54 	 Loss: 14.519900
Train Epoch: 55 	 Loss: 14.630398
Train Epoch: 56 	 Loss: 14.230894
Train Epoch: 57 	 Loss: 14.420856
Train Epoch: 58 	 Loss: 14.441621
Train Epoch: 59 	 Loss: 14.278111
Train Epoch: 60 	 Loss: 14.765759
Train Epoch: 61 	 Loss: 14.105165
Train Epoch: 62 	 Loss: 14.637812
Train Epoch: 63 	 Loss: 14.307257
Train Epoch: 64 	 Loss: 14.216940
Train Epoch: 65 	 Loss: 14.865355
Train Epoch: 66 	 Loss: 14.589238
Train Epoch: 67 	 Loss: 14.339640
Train Epoch: 68 	 Loss: 14.607741
Train Epoch: 69 	 Loss: 14.017376
Train Epoch: 70 	 Loss: 14.339184
Train Epoch: 71 	 Loss: 14.580618
Train Epoch: 72 	 Loss: 13.830433
Train Epoch: 73 	 Loss: 14.563593
Train Epoch: 74 	 Loss: 14.340851
Train Epoch: 75 	 Loss: 14.867854
Train Epoch: 76 	 Loss: 13.916796
Train Epoch: 77 	 Loss: 13.850730
Train Epoch: 78 	 Loss: 13.901884
Train Epoch: 79 	 Loss: 13.793362
Train Epoch: 80 	 Loss: 13.576310
Train Epoch: 81 	 Loss: 13.690702
Train Epoch: 82 	 Loss: 13.579054
Train Epoch: 83 	 Loss: 13.781507
Train Epoch: 84 	 Loss: 13.973753
Train Epoch: 85 	 Loss: 13.883442
Train Epoch: 86 	 Loss: 14.043914
Train Epoch: 87 	 Loss: 13.889343
Train Epoch: 88 	 Loss: 13.930170
Train Epoch: 89 	 Loss: 13.762793
Train Epoch: 90 	 Loss: 13.738171
Train Epoch: 91 	 Loss: 13.815311
Train Epoch: 92 	 Loss: 13.959972
Train Epoch: 93 	 Loss: 14.138816
Train Epoch: 94 	 Loss: 13.620378
Train Epoch: 95 	 Loss: 13.592774
Train Epoch: 96 	 Loss: 13.799055
Train Epoch: 97 	 Loss: 13.742640
Train Epoch: 98 	 Loss: 13.781967
Train Epoch: 99 	 Loss: 13.672823

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.894

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.542

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.830

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.789

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.885

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.935

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.908

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.891

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.915

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.895

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.822

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.909
------
f1 mean across methods is 0.851


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.72673985e-01 4.04982678e-01 1.74298305e-02 4.69883901e-03
 2.14667772e-04] 

Train Epoch: 0 	 Loss: 20.708115
Train Epoch: 1 	 Loss: 17.625168
Train Epoch: 2 	 Loss: 16.815933
Train Epoch: 3 	 Loss: 14.861944
Train Epoch: 4 	 Loss: 14.153839
Train Epoch: 5 	 Loss: 15.035372
Train Epoch: 6 	 Loss: 14.675892
Train Epoch: 7 	 Loss: 14.454618
Train Epoch: 8 	 Loss: 13.996503
Train Epoch: 9 	 Loss: 14.154429
Train Epoch: 10 	 Loss: 14.383446
Train Epoch: 11 	 Loss: 13.435101
Train Epoch: 12 	 Loss: 13.636555
Train Epoch: 13 	 Loss: 13.747276
Train Epoch: 14 	 Loss: 13.801238
Train Epoch: 15 	 Loss: 13.529932
Train Epoch: 16 	 Loss: 13.616354
Train Epoch: 17 	 Loss: 14.044277
Train Epoch: 18 	 Loss: 13.652905
Train Epoch: 19 	 Loss: 13.647727
Train Epoch: 20 	 Loss: 13.912196
Train Epoch: 21 	 Loss: 13.535635
Train Epoch: 22 	 Loss: 13.786608
Train Epoch: 23 	 Loss: 13.852478
Train Epoch: 24 	 Loss: 13.563737
Train Epoch: 25 	 Loss: 13.608121
Train Epoch: 26 	 Loss: 13.718850
Train Epoch: 27 	 Loss: 13.721016
Train Epoch: 28 	 Loss: 13.570337
Train Epoch: 29 	 Loss: 13.873543
Train Epoch: 30 	 Loss: 13.696108
Train Epoch: 31 	 Loss: 13.601690
Train Epoch: 32 	 Loss: 13.467325
Train Epoch: 33 	 Loss: 13.648407
Train Epoch: 34 	 Loss: 13.601497
Train Epoch: 35 	 Loss: 13.526550
Train Epoch: 36 	 Loss: 13.464156
Train Epoch: 37 	 Loss: 13.635847
Train Epoch: 38 	 Loss: 13.564861
Train Epoch: 39 	 Loss: 13.549910
Train Epoch: 40 	 Loss: 13.522638
Train Epoch: 41 	 Loss: 13.676427
Train Epoch: 42 	 Loss: 13.731427
Train Epoch: 43 	 Loss: 13.860572
Train Epoch: 44 	 Loss: 13.559841
Train Epoch: 45 	 Loss: 14.405584
Train Epoch: 46 	 Loss: 13.705132
Train Epoch: 47 	 Loss: 13.612766
Train Epoch: 48 	 Loss: 13.568264
Train Epoch: 49 	 Loss: 13.871552
Train Epoch: 50 	 Loss: 13.424494
Train Epoch: 51 	 Loss: 13.291674
Train Epoch: 52 	 Loss: 13.304395
Train Epoch: 53 	 Loss: 13.657948
Train Epoch: 54 	 Loss: 13.570508
Train Epoch: 55 	 Loss: 13.470032
Train Epoch: 56 	 Loss: 13.015033
Train Epoch: 57 	 Loss: 13.535794
Train Epoch: 58 	 Loss: 13.588124
Train Epoch: 59 	 Loss: 13.469505
Train Epoch: 60 	 Loss: 13.724588
Train Epoch: 61 	 Loss: 13.378197
Train Epoch: 62 	 Loss: 14.152955
Train Epoch: 63 	 Loss: 13.536604
Train Epoch: 64 	 Loss: 13.387977
Train Epoch: 65 	 Loss: 14.007628
Train Epoch: 66 	 Loss: 13.535193
Train Epoch: 67 	 Loss: 13.338799
Train Epoch: 68 	 Loss: 13.775781
Train Epoch: 69 	 Loss: 13.485443
Train Epoch: 70 	 Loss: 13.116775
Train Epoch: 71 	 Loss: 13.830474
Train Epoch: 72 	 Loss: 13.782892
Train Epoch: 73 	 Loss: 13.446461
Train Epoch: 74 	 Loss: 13.189259
Train Epoch: 75 	 Loss: 13.464684
Train Epoch: 76 	 Loss: 13.478632
Train Epoch: 77 	 Loss: 13.132256
Train Epoch: 78 	 Loss: 13.033474
Train Epoch: 79 	 Loss: 13.628307
Train Epoch: 80 	 Loss: 13.461352
Train Epoch: 81 	 Loss: 13.223774
Train Epoch: 82 	 Loss: 13.903373
Train Epoch: 83 	 Loss: 13.552649
Train Epoch: 84 	 Loss: 14.077555
Train Epoch: 85 	 Loss: 13.597929
Train Epoch: 86 	 Loss: 13.371737
Train Epoch: 87 	 Loss: 13.657670
Train Epoch: 88 	 Loss: 13.443741
Train Epoch: 89 	 Loss: 13.150141
Train Epoch: 90 	 Loss: 13.178677
Train Epoch: 91 	 Loss: 13.457485
Train Epoch: 92 	 Loss: 13.030863
Train Epoch: 93 	 Loss: 13.052221
Train Epoch: 94 	 Loss: 13.641840
Train Epoch: 95 	 Loss: 13.078381
Train Epoch: 96 	 Loss: 13.050396
Train Epoch: 97 	 Loss: 13.575636
Train Epoch: 98 	 Loss: 13.492036
Train Epoch: 99 	 Loss: 13.477845

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.876

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.583

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.911

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.864

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.913

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.910

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.919

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.552

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.923

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.887

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.866

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.930
------
f1 mean across methods is 0.845


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.71177274e-01 4.06825243e-01 1.70601249e-02 4.71076499e-03
 2.26593759e-04] 

Train Epoch: 0 	 Loss: 19.706982
Train Epoch: 1 	 Loss: 19.833538
Train Epoch: 2 	 Loss: 16.778717
Train Epoch: 3 	 Loss: 16.589228
Train Epoch: 4 	 Loss: 15.559140
Train Epoch: 5 	 Loss: 15.363296
Train Epoch: 6 	 Loss: 15.381789
Train Epoch: 7 	 Loss: 15.146602
Train Epoch: 8 	 Loss: 14.693118
Train Epoch: 9 	 Loss: 14.317488
Train Epoch: 10 	 Loss: 15.259876
Train Epoch: 11 	 Loss: 14.784695
Train Epoch: 12 	 Loss: 15.227962
Train Epoch: 13 	 Loss: 14.126287
Train Epoch: 14 	 Loss: 15.294503
Train Epoch: 15 	 Loss: 14.492936
Train Epoch: 16 	 Loss: 14.101059
Train Epoch: 17 	 Loss: 14.843322
Train Epoch: 18 	 Loss: 14.605682
Train Epoch: 19 	 Loss: 14.428868
Train Epoch: 20 	 Loss: 14.050385
Train Epoch: 21 	 Loss: 13.991690
Train Epoch: 22 	 Loss: 14.442643
Train Epoch: 23 	 Loss: 13.895335
Train Epoch: 24 	 Loss: 14.505063
Train Epoch: 25 	 Loss: 14.679615
Train Epoch: 26 	 Loss: 13.824694
Train Epoch: 27 	 Loss: 14.918701
Train Epoch: 28 	 Loss: 13.974817
Train Epoch: 29 	 Loss: 14.132752
Train Epoch: 30 	 Loss: 14.580694
Train Epoch: 31 	 Loss: 13.990442
Train Epoch: 32 	 Loss: 13.829081
Train Epoch: 33 	 Loss: 14.076021
Train Epoch: 34 	 Loss: 14.417023
Train Epoch: 35 	 Loss: 14.495189
Train Epoch: 36 	 Loss: 14.051528
Train Epoch: 37 	 Loss: 14.462469
Train Epoch: 38 	 Loss: 14.212267
Train Epoch: 39 	 Loss: 13.637046
Train Epoch: 40 	 Loss: 14.136106
Train Epoch: 41 	 Loss: 13.805424
Train Epoch: 42 	 Loss: 14.628228
Train Epoch: 43 	 Loss: 14.265146
Train Epoch: 44 	 Loss: 13.823686
Train Epoch: 45 	 Loss: 13.830942
Train Epoch: 46 	 Loss: 14.756637
Train Epoch: 47 	 Loss: 13.249604
Train Epoch: 48 	 Loss: 13.771091
Train Epoch: 49 	 Loss: 13.544180
Train Epoch: 50 	 Loss: 13.457897
Train Epoch: 51 	 Loss: 13.772649
Train Epoch: 52 	 Loss: 13.736532
Train Epoch: 53 	 Loss: 13.781473
Train Epoch: 54 	 Loss: 13.582138
Train Epoch: 55 	 Loss: 13.213012
Train Epoch: 56 	 Loss: 13.957389
Train Epoch: 57 	 Loss: 13.719127
Train Epoch: 58 	 Loss: 14.893415
Train Epoch: 59 	 Loss: 13.759264
Train Epoch: 60 	 Loss: 13.501514
Train Epoch: 61 	 Loss: 13.311099
Train Epoch: 62 	 Loss: 13.330303
Train Epoch: 63 	 Loss: 13.679934
Train Epoch: 64 	 Loss: 13.400314
Train Epoch: 65 	 Loss: 14.171390
Train Epoch: 66 	 Loss: 13.391811
Train Epoch: 67 	 Loss: 14.162910
Train Epoch: 68 	 Loss: 13.387807
Train Epoch: 69 	 Loss: 13.180758
Train Epoch: 70 	 Loss: 13.352742
Train Epoch: 71 	 Loss: 13.127682
Train Epoch: 72 	 Loss: 13.634925
Train Epoch: 73 	 Loss: 13.380883
Train Epoch: 74 	 Loss: 13.895458
Train Epoch: 75 	 Loss: 13.291813
Train Epoch: 76 	 Loss: 14.454600
Train Epoch: 77 	 Loss: 13.604958
Train Epoch: 78 	 Loss: 13.163025
Train Epoch: 79 	 Loss: 13.538428
Train Epoch: 80 	 Loss: 14.036571
Train Epoch: 81 	 Loss: 13.645968
Train Epoch: 82 	 Loss: 13.229656
Train Epoch: 83 	 Loss: 13.563062
Train Epoch: 84 	 Loss: 13.471789
Train Epoch: 85 	 Loss: 13.250158
Train Epoch: 86 	 Loss: 13.159979
Train Epoch: 87 	 Loss: 13.044069
Train Epoch: 88 	 Loss: 13.214287
Train Epoch: 89 	 Loss: 13.122253
Train Epoch: 90 	 Loss: 13.146614
Train Epoch: 91 	 Loss: 13.562964
Train Epoch: 92 	 Loss: 13.323689
Train Epoch: 93 	 Loss: 13.217020
Train Epoch: 94 	 Loss: 13.233178
Train Epoch: 95 	 Loss: 13.269574
Train Epoch: 96 	 Loss: 13.069304
Train Epoch: 97 	 Loss: 13.520058
Train Epoch: 98 	 Loss: 13.516932
Train Epoch: 99 	 Loss: 14.012840

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.816

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.001

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.836

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.820

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.805

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.923

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.931

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.570

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.902

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.860

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.874

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.905
------
f1 mean across methods is 0.770


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.71827240e-01 4.06103720e-01 1.70004949e-02 4.84791385e-03
 2.20630765e-04] 

Train Epoch: 0 	 Loss: 21.250061
Train Epoch: 1 	 Loss: 20.187405
Train Epoch: 2 	 Loss: 18.860146
Train Epoch: 3 	 Loss: 17.166483
Train Epoch: 4 	 Loss: 16.717728
Train Epoch: 5 	 Loss: 16.193176
Train Epoch: 6 	 Loss: 15.882866
Train Epoch: 7 	 Loss: 15.581833
Train Epoch: 8 	 Loss: 15.623131
Train Epoch: 9 	 Loss: 15.601439
Train Epoch: 10 	 Loss: 15.654183
Train Epoch: 11 	 Loss: 15.609565
Train Epoch: 12 	 Loss: 16.219816
Train Epoch: 13 	 Loss: 15.056623
Train Epoch: 14 	 Loss: 14.396891
Train Epoch: 15 	 Loss: 15.357598
Train Epoch: 16 	 Loss: 15.103251
Train Epoch: 17 	 Loss: 16.229782
Train Epoch: 18 	 Loss: 14.698919
Train Epoch: 19 	 Loss: 15.267347
Train Epoch: 20 	 Loss: 15.243611
Train Epoch: 21 	 Loss: 14.924707
Train Epoch: 22 	 Loss: 15.937640
Train Epoch: 23 	 Loss: 15.408141
Train Epoch: 24 	 Loss: 15.189185
Train Epoch: 25 	 Loss: 15.045030
Train Epoch: 26 	 Loss: 15.654853
Train Epoch: 27 	 Loss: 14.333430
Train Epoch: 28 	 Loss: 15.950260
Train Epoch: 29 	 Loss: 15.326692
Train Epoch: 30 	 Loss: 15.187585
Train Epoch: 31 	 Loss: 15.786698
Train Epoch: 32 	 Loss: 15.659225
Train Epoch: 33 	 Loss: 15.234646
Train Epoch: 34 	 Loss: 14.754759
Train Epoch: 35 	 Loss: 15.311841
Train Epoch: 36 	 Loss: 14.918716
Train Epoch: 37 	 Loss: 14.697026
Train Epoch: 38 	 Loss: 14.725241
Train Epoch: 39 	 Loss: 14.968160
Train Epoch: 40 	 Loss: 15.638258
Train Epoch: 41 	 Loss: 15.159120
Train Epoch: 42 	 Loss: 14.887281
Train Epoch: 43 	 Loss: 15.018787
Train Epoch: 44 	 Loss: 14.675939
Train Epoch: 45 	 Loss: 15.446058
Train Epoch: 46 	 Loss: 14.568682
Train Epoch: 47 	 Loss: 14.790538
Train Epoch: 48 	 Loss: 14.349133
Train Epoch: 49 	 Loss: 14.428959
Train Epoch: 50 	 Loss: 14.304914
Train Epoch: 51 	 Loss: 14.284281
Train Epoch: 52 	 Loss: 14.839731
Train Epoch: 53 	 Loss: 14.235174
Train Epoch: 54 	 Loss: 14.362925
Train Epoch: 55 	 Loss: 14.401834
Train Epoch: 56 	 Loss: 14.084051
Train Epoch: 57 	 Loss: 14.440166
Train Epoch: 58 	 Loss: 14.665209
Train Epoch: 59 	 Loss: 14.429020
Train Epoch: 60 	 Loss: 14.268448
Train Epoch: 61 	 Loss: 14.333427
Train Epoch: 62 	 Loss: 14.289717
Train Epoch: 63 	 Loss: 14.226193
Train Epoch: 64 	 Loss: 14.496712
Train Epoch: 65 	 Loss: 14.535868
Train Epoch: 66 	 Loss: 14.302393
Train Epoch: 67 	 Loss: 14.658732
Train Epoch: 68 	 Loss: 14.165390
Train Epoch: 69 	 Loss: 14.494502
Train Epoch: 70 	 Loss: 14.252884
Train Epoch: 71 	 Loss: 14.142742
Train Epoch: 72 	 Loss: 14.297186
Train Epoch: 73 	 Loss: 14.163880
Train Epoch: 74 	 Loss: 14.239535
Train Epoch: 75 	 Loss: 14.485727
Train Epoch: 76 	 Loss: 14.267632
Train Epoch: 77 	 Loss: 14.123164
Train Epoch: 78 	 Loss: 14.138645
Train Epoch: 79 	 Loss: 14.634212
Train Epoch: 80 	 Loss: 15.014180
Train Epoch: 81 	 Loss: 14.018566
Train Epoch: 82 	 Loss: 14.185902
Train Epoch: 83 	 Loss: 14.288378
Train Epoch: 84 	 Loss: 14.134653
Train Epoch: 85 	 Loss: 14.694300
Train Epoch: 86 	 Loss: 14.443357
Train Epoch: 87 	 Loss: 14.523067
Train Epoch: 88 	 Loss: 14.452818
Train Epoch: 89 	 Loss: 14.268217
Train Epoch: 90 	 Loss: 14.224191
Train Epoch: 91 	 Loss: 14.308331
Train Epoch: 92 	 Loss: 14.426468
Train Epoch: 93 	 Loss: 14.086071
Train Epoch: 94 	 Loss: 14.206195
Train Epoch: 95 	 Loss: 14.176332
Train Epoch: 96 	 Loss: 14.372274
Train Epoch: 97 	 Loss: 14.094435
Train Epoch: 98 	 Loss: 14.775149
Train Epoch: 99 	 Loss: 14.219166

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.898

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.552

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.905

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.881

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.916

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.917

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.953

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.893

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.934

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.909

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.920

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.937
------
f1 mean across methods is 0.885


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 100
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=100_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.72214835e-01 4.05346420e-01 1.74357935e-02 4.77635792e-03
 2.26593759e-04] 

Train Epoch: 0 	 Loss: 24.096287
Train Epoch: 1 	 Loss: 19.124714
Train Epoch: 2 	 Loss: 18.591822
Train Epoch: 3 	 Loss: 17.867411
Train Epoch: 4 	 Loss: 15.324113
Train Epoch: 5 	 Loss: 15.834972
Train Epoch: 6 	 Loss: 15.493461
Train Epoch: 7 	 Loss: 15.317044
Train Epoch: 8 	 Loss: 15.672036
Train Epoch: 9 	 Loss: 15.565271
Train Epoch: 10 	 Loss: 15.054582
Train Epoch: 11 	 Loss: 15.221082
Train Epoch: 12 	 Loss: 15.110078
Train Epoch: 13 	 Loss: 15.220978
Train Epoch: 14 	 Loss: 15.097260
Train Epoch: 15 	 Loss: 15.202297
Train Epoch: 16 	 Loss: 15.157656
Train Epoch: 17 	 Loss: 15.002655
Train Epoch: 18 	 Loss: 15.203716
Train Epoch: 19 	 Loss: 15.112126
Train Epoch: 20 	 Loss: 14.943263
Train Epoch: 21 	 Loss: 15.128139
Train Epoch: 22 	 Loss: 15.141733
Train Epoch: 23 	 Loss: 15.090483
Train Epoch: 24 	 Loss: 15.017954
Train Epoch: 25 	 Loss: 14.913519
Train Epoch: 26 	 Loss: 15.118487
Train Epoch: 27 	 Loss: 14.896723
Train Epoch: 28 	 Loss: 14.864248
Train Epoch: 29 	 Loss: 15.151459
Train Epoch: 30 	 Loss: 14.992167
Train Epoch: 31 	 Loss: 15.044974
Train Epoch: 32 	 Loss: 14.993871
Train Epoch: 33 	 Loss: 14.867694
Train Epoch: 34 	 Loss: 15.158800
Train Epoch: 35 	 Loss: 14.925111
Train Epoch: 36 	 Loss: 14.980672
Train Epoch: 37 	 Loss: 14.924602
Train Epoch: 38 	 Loss: 14.968487
Train Epoch: 39 	 Loss: 14.855051
Train Epoch: 40 	 Loss: 15.050917
Train Epoch: 41 	 Loss: 15.157759
Train Epoch: 42 	 Loss: 15.419739
Train Epoch: 43 	 Loss: 15.211288
Train Epoch: 44 	 Loss: 14.877187
Train Epoch: 45 	 Loss: 15.061714
Train Epoch: 46 	 Loss: 14.919502
Train Epoch: 47 	 Loss: 14.922246
Train Epoch: 48 	 Loss: 15.105901
Train Epoch: 49 	 Loss: 15.113706
Train Epoch: 50 	 Loss: 14.866570
Train Epoch: 51 	 Loss: 14.841454
Train Epoch: 52 	 Loss: 14.895975
Train Epoch: 53 	 Loss: 14.900373
Train Epoch: 54 	 Loss: 15.019763
Train Epoch: 55 	 Loss: 15.113974
Train Epoch: 56 	 Loss: 15.044460
Train Epoch: 57 	 Loss: 15.141797
Train Epoch: 58 	 Loss: 14.961922
Train Epoch: 59 	 Loss: 14.714619
Train Epoch: 60 	 Loss: 14.804485
Train Epoch: 61 	 Loss: 15.027971
Train Epoch: 62 	 Loss: 14.773474
Train Epoch: 63 	 Loss: 14.867937
Train Epoch: 64 	 Loss: 14.809269
Train Epoch: 65 	 Loss: 14.811172
Train Epoch: 66 	 Loss: 14.915403
Train Epoch: 67 	 Loss: 14.817514
Train Epoch: 68 	 Loss: 15.068035
Train Epoch: 69 	 Loss: 15.162529
Train Epoch: 70 	 Loss: 14.958971
Train Epoch: 71 	 Loss: 14.943444
Train Epoch: 72 	 Loss: 15.112584
Train Epoch: 73 	 Loss: 14.665298
Train Epoch: 74 	 Loss: 15.014473
Train Epoch: 75 	 Loss: 14.779116
Train Epoch: 76 	 Loss: 15.055283
Train Epoch: 77 	 Loss: 15.078936
Train Epoch: 78 	 Loss: 14.667061
Train Epoch: 79 	 Loss: 14.757816
Train Epoch: 80 	 Loss: 14.993711
Train Epoch: 81 	 Loss: 14.794811
Train Epoch: 82 	 Loss: 14.968138
Train Epoch: 83 	 Loss: 14.887139
Train Epoch: 84 	 Loss: 14.647919
Train Epoch: 85 	 Loss: 14.961134
Train Epoch: 86 	 Loss: 14.642786
Train Epoch: 87 	 Loss: 14.784159
Train Epoch: 88 	 Loss: 14.639862
Train Epoch: 89 	 Loss: 14.555925
Train Epoch: 90 	 Loss: 14.545252
Train Epoch: 91 	 Loss: 14.648039
Train Epoch: 92 	 Loss: 14.505360
Train Epoch: 93 	 Loss: 14.708728
Train Epoch: 94 	 Loss: 14.592328
Train Epoch: 95 	 Loss: 14.619639
Train Epoch: 96 	 Loss: 14.459814
Train Epoch: 97 	 Loss: 14.841257
Train Epoch: 98 	 Loss: 14.457386
Train Epoch: 99 	 Loss: 14.483378

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.899

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.568

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.914

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.877

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.922

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.930

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.880

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.922

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.922

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.894

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.884

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.932
------
f1 mean across methods is 0.879


 ---------------------------------------- 


For each of the methods
Average F1:  [0.87660487 0.44921569 0.87916051 0.84616917 0.88826429 0.92304105
 0.9181731  0.76568442 0.91925663 0.88916123 0.87328317 0.92254027]
Std F1:  [0.03114631 0.22452843 0.03787871 0.03588792 0.04338999 0.00880041
 0.02422857 0.16762081 0.01047754 0.01606269 0.03139168 0.01311672]
Average over repetitions across all methods
Average f1 score:  0.8458795324598951
Std F1:  0.04081395044173489

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 200, 'order_hermite': 100, 'subsampled_rate': 0.25} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.87498040e-01 4.86150907e-01 2.04208185e-02 5.70927597e-03
 2.20958246e-04] 

Train Epoch: 0 	 Loss: 18.528973
Train Epoch: 1 	 Loss: 17.283230
Train Epoch: 2 	 Loss: 16.206160
Train Epoch: 3 	 Loss: 14.947141
Train Epoch: 4 	 Loss: 12.973431
Train Epoch: 5 	 Loss: 13.445113
Train Epoch: 6 	 Loss: 13.547962
Train Epoch: 7 	 Loss: 13.479898
Train Epoch: 8 	 Loss: 13.243132
Train Epoch: 9 	 Loss: 13.012473
Train Epoch: 10 	 Loss: 13.302443
Train Epoch: 11 	 Loss: 12.725496
Train Epoch: 12 	 Loss: 13.033857
Train Epoch: 13 	 Loss: 13.249907
Train Epoch: 14 	 Loss: 13.044310
Train Epoch: 15 	 Loss: 13.287687
Train Epoch: 16 	 Loss: 13.400658
Train Epoch: 17 	 Loss: 13.244169
Train Epoch: 18 	 Loss: 12.792285
Train Epoch: 19 	 Loss: 12.556683
Train Epoch: 20 	 Loss: 12.703605
Train Epoch: 21 	 Loss: 12.568476
Train Epoch: 22 	 Loss: 12.637297
Train Epoch: 23 	 Loss: 12.289103
Train Epoch: 24 	 Loss: 12.644835
Train Epoch: 25 	 Loss: 13.248354
Train Epoch: 26 	 Loss: 13.163497
Train Epoch: 27 	 Loss: 13.045753
Train Epoch: 28 	 Loss: 12.830132
Train Epoch: 29 	 Loss: 13.391517
Train Epoch: 30 	 Loss: 12.187574
Train Epoch: 31 	 Loss: 12.982445
Train Epoch: 32 	 Loss: 12.912203
Train Epoch: 33 	 Loss: 12.552365
Train Epoch: 34 	 Loss: 12.199282
Train Epoch: 35 	 Loss: 11.965538
Train Epoch: 36 	 Loss: 12.653164
Train Epoch: 37 	 Loss: 13.185941
Train Epoch: 38 	 Loss: 12.826359
Train Epoch: 39 	 Loss: 12.748232
Train Epoch: 40 	 Loss: 12.544435
Train Epoch: 41 	 Loss: 12.795214
Train Epoch: 42 	 Loss: 12.440227
Train Epoch: 43 	 Loss: 13.655365
Train Epoch: 44 	 Loss: 13.257570
Train Epoch: 45 	 Loss: 12.721210
Train Epoch: 46 	 Loss: 13.583573
Train Epoch: 47 	 Loss: 12.591849
Train Epoch: 48 	 Loss: 12.369946
Train Epoch: 49 	 Loss: 12.797724
Train Epoch: 50 	 Loss: 12.353275
Train Epoch: 51 	 Loss: 12.734778
Train Epoch: 52 	 Loss: 12.346397
Train Epoch: 53 	 Loss: 12.638021
Train Epoch: 54 	 Loss: 12.182352
Train Epoch: 55 	 Loss: 12.205387
Train Epoch: 56 	 Loss: 12.754283
Train Epoch: 57 	 Loss: 12.014388
Train Epoch: 58 	 Loss: 12.299181
Train Epoch: 59 	 Loss: 12.750216
Train Epoch: 60 	 Loss: 12.236949
Train Epoch: 61 	 Loss: 12.320889
Train Epoch: 62 	 Loss: 12.297531
Train Epoch: 63 	 Loss: 12.618268
Train Epoch: 64 	 Loss: 13.335085
Train Epoch: 65 	 Loss: 12.483041
Train Epoch: 66 	 Loss: 12.704830
Train Epoch: 67 	 Loss: 13.032682
Train Epoch: 68 	 Loss: 12.060061
Train Epoch: 69 	 Loss: 12.457895
Train Epoch: 70 	 Loss: 12.414152
Train Epoch: 71 	 Loss: 12.326415
Train Epoch: 72 	 Loss: 12.298765
Train Epoch: 73 	 Loss: 12.589799
Train Epoch: 74 	 Loss: 12.600358
Train Epoch: 75 	 Loss: 12.279084
Train Epoch: 76 	 Loss: 12.775147
Train Epoch: 77 	 Loss: 13.014454
Train Epoch: 78 	 Loss: 13.341399
Train Epoch: 79 	 Loss: 11.939458
Train Epoch: 80 	 Loss: 12.280617
Train Epoch: 81 	 Loss: 12.026218
Train Epoch: 82 	 Loss: 12.341536
Train Epoch: 83 	 Loss: 12.653501
Train Epoch: 84 	 Loss: 12.648802
Train Epoch: 85 	 Loss: 12.466467
Train Epoch: 86 	 Loss: 12.330187
Train Epoch: 87 	 Loss: 14.605205
Train Epoch: 88 	 Loss: 12.560238
Train Epoch: 89 	 Loss: 12.520501
Train Epoch: 90 	 Loss: 11.976317
Train Epoch: 91 	 Loss: 12.249661
Train Epoch: 92 	 Loss: 12.747105
Train Epoch: 93 	 Loss: 12.080790
Train Epoch: 94 	 Loss: 13.098117
Train Epoch: 95 	 Loss: 12.378302
Train Epoch: 96 	 Loss: 12.098969
Train Epoch: 97 	 Loss: 13.341173
Train Epoch: 98 	 Loss: 12.579203
Train Epoch: 99 	 Loss: 11.701193
Train Epoch: 100 	 Loss: 12.426523
Train Epoch: 101 	 Loss: 12.108864
Train Epoch: 102 	 Loss: 12.946085
Train Epoch: 103 	 Loss: 12.508962
Train Epoch: 104 	 Loss: 11.911878
Train Epoch: 105 	 Loss: 12.241417
Train Epoch: 106 	 Loss: 12.299748
Train Epoch: 107 	 Loss: 12.296118
Train Epoch: 108 	 Loss: 12.655977
Train Epoch: 109 	 Loss: 12.268950
Train Epoch: 110 	 Loss: 12.189857
Train Epoch: 111 	 Loss: 12.306429
Train Epoch: 112 	 Loss: 12.063114
Train Epoch: 113 	 Loss: 12.376193
Train Epoch: 114 	 Loss: 12.386964
Train Epoch: 115 	 Loss: 12.496105
Train Epoch: 116 	 Loss: 12.090629
Train Epoch: 117 	 Loss: 11.894231
Train Epoch: 118 	 Loss: 12.199615
Train Epoch: 119 	 Loss: 12.254267
Train Epoch: 120 	 Loss: 12.037544
Train Epoch: 121 	 Loss: 11.901548
Train Epoch: 122 	 Loss: 12.028454
Train Epoch: 123 	 Loss: 11.607439
Train Epoch: 124 	 Loss: 11.930367
Train Epoch: 125 	 Loss: 12.285740
Train Epoch: 126 	 Loss: 12.398186
Train Epoch: 127 	 Loss: 11.804087
Train Epoch: 128 	 Loss: 11.903965
Train Epoch: 129 	 Loss: 11.426966
Train Epoch: 130 	 Loss: 12.436656
Train Epoch: 131 	 Loss: 12.111406
Train Epoch: 132 	 Loss: 11.937284
Train Epoch: 133 	 Loss: 11.808692
Train Epoch: 134 	 Loss: 12.216493
Train Epoch: 135 	 Loss: 12.277388
Train Epoch: 136 	 Loss: 11.994215
Train Epoch: 137 	 Loss: 12.213196
Train Epoch: 138 	 Loss: 11.889680
Train Epoch: 139 	 Loss: 11.881210
Train Epoch: 140 	 Loss: 12.769604
Train Epoch: 141 	 Loss: 12.020676
Train Epoch: 142 	 Loss: 12.145471
Train Epoch: 143 	 Loss: 11.800137
Train Epoch: 144 	 Loss: 11.958754
Train Epoch: 145 	 Loss: 11.925965
Train Epoch: 146 	 Loss: 12.115657
Train Epoch: 147 	 Loss: 12.049128
Train Epoch: 148 	 Loss: 12.259434
Train Epoch: 149 	 Loss: 11.740438
Train Epoch: 150 	 Loss: 12.014616
Train Epoch: 151 	 Loss: 12.319488
Train Epoch: 152 	 Loss: 12.242632
Train Epoch: 153 	 Loss: 11.873421
Train Epoch: 154 	 Loss: 12.013278
Train Epoch: 155 	 Loss: 12.394265
Train Epoch: 156 	 Loss: 11.749295
Train Epoch: 157 	 Loss: 11.717945
Train Epoch: 158 	 Loss: 11.785302
Train Epoch: 159 	 Loss: 12.278971
Train Epoch: 160 	 Loss: 13.745230
Train Epoch: 161 	 Loss: 11.866823
Train Epoch: 162 	 Loss: 12.100997
Train Epoch: 163 	 Loss: 12.244007
Train Epoch: 164 	 Loss: 12.205186
Train Epoch: 165 	 Loss: 12.059582
Train Epoch: 166 	 Loss: 12.019779
Train Epoch: 167 	 Loss: 11.894810
Train Epoch: 168 	 Loss: 12.133073
Train Epoch: 169 	 Loss: 12.163517
Train Epoch: 170 	 Loss: 11.603924
Train Epoch: 171 	 Loss: 12.070650
Train Epoch: 172 	 Loss: 11.860279
Train Epoch: 173 	 Loss: 13.196336
Train Epoch: 174 	 Loss: 11.657990
Train Epoch: 175 	 Loss: 12.051332
Train Epoch: 176 	 Loss: 12.042476
Train Epoch: 177 	 Loss: 11.772299
Train Epoch: 178 	 Loss: 11.766034
Train Epoch: 179 	 Loss: 12.186365
Train Epoch: 180 	 Loss: 11.880041
Train Epoch: 181 	 Loss: 11.821632
Train Epoch: 182 	 Loss: 11.888876
Train Epoch: 183 	 Loss: 11.714609
Train Epoch: 184 	 Loss: 11.828096
Train Epoch: 185 	 Loss: 11.973537
Train Epoch: 186 	 Loss: 11.805761
Train Epoch: 187 	 Loss: 12.080868
Train Epoch: 188 	 Loss: 11.928100
Train Epoch: 189 	 Loss: 12.274253
Train Epoch: 190 	 Loss: 11.681746
Train Epoch: 191 	 Loss: 11.812111
Train Epoch: 192 	 Loss: 12.412636
Train Epoch: 193 	 Loss: 11.848213
Train Epoch: 194 	 Loss: 12.392550
Train Epoch: 195 	 Loss: 11.879070
Train Epoch: 196 	 Loss: 11.976111
Train Epoch: 197 	 Loss: 11.899237
Train Epoch: 198 	 Loss: 11.995353
Train Epoch: 199 	 Loss: 12.580091

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.863

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.677

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.856

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.786

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.832

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.904

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.929

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.888

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.935

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.892

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.839

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.897
------
f1 mean across methods is 0.858


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.88460277e-01 4.84981967e-01 2.07059260e-02 5.57384995e-03
 2.77979729e-04] 

Train Epoch: 0 	 Loss: 19.247570
Train Epoch: 1 	 Loss: 18.346031
Train Epoch: 2 	 Loss: 23.326048
Train Epoch: 3 	 Loss: 15.695314
Train Epoch: 4 	 Loss: 14.967838
Train Epoch: 5 	 Loss: 14.850238
Train Epoch: 6 	 Loss: 14.541012
Train Epoch: 7 	 Loss: 14.574184
Train Epoch: 8 	 Loss: 14.325006
Train Epoch: 9 	 Loss: 14.963793
Train Epoch: 10 	 Loss: 15.571102
Train Epoch: 11 	 Loss: 14.453799
Train Epoch: 12 	 Loss: 14.495455
Train Epoch: 13 	 Loss: 14.005266
Train Epoch: 14 	 Loss: 14.825835
Train Epoch: 15 	 Loss: 14.340459
Train Epoch: 16 	 Loss: 13.848598
Train Epoch: 17 	 Loss: 14.164077
Train Epoch: 18 	 Loss: 14.094484
Train Epoch: 19 	 Loss: 14.172420
Train Epoch: 20 	 Loss: 14.194151
Train Epoch: 21 	 Loss: 13.893641
Train Epoch: 22 	 Loss: 13.898317
Train Epoch: 23 	 Loss: 14.940257
Train Epoch: 24 	 Loss: 14.671414
Train Epoch: 25 	 Loss: 13.737236
Train Epoch: 26 	 Loss: 13.890252
Train Epoch: 27 	 Loss: 14.215856
Train Epoch: 28 	 Loss: 13.987105
Train Epoch: 29 	 Loss: 14.100686
Train Epoch: 30 	 Loss: 14.069630
Train Epoch: 31 	 Loss: 14.472956
Train Epoch: 32 	 Loss: 14.491545
Train Epoch: 33 	 Loss: 14.386770
Train Epoch: 34 	 Loss: 13.573357
Train Epoch: 35 	 Loss: 14.196589
Train Epoch: 36 	 Loss: 14.339309
Train Epoch: 37 	 Loss: 14.070650
Train Epoch: 38 	 Loss: 13.997196
Train Epoch: 39 	 Loss: 14.106562
Train Epoch: 40 	 Loss: 14.198277
Train Epoch: 41 	 Loss: 14.169151
Train Epoch: 42 	 Loss: 13.992687
Train Epoch: 43 	 Loss: 14.201994
Train Epoch: 44 	 Loss: 13.666218
Train Epoch: 45 	 Loss: 13.778500
Train Epoch: 46 	 Loss: 13.739884
Train Epoch: 47 	 Loss: 14.476471
Train Epoch: 48 	 Loss: 14.049692
Train Epoch: 49 	 Loss: 13.582826
Train Epoch: 50 	 Loss: 14.490345
Train Epoch: 51 	 Loss: 13.780500
Train Epoch: 52 	 Loss: 13.948435
Train Epoch: 53 	 Loss: 14.276503
Train Epoch: 54 	 Loss: 13.658317
Train Epoch: 55 	 Loss: 14.239168
Train Epoch: 56 	 Loss: 14.447062
Train Epoch: 57 	 Loss: 14.020850
Train Epoch: 58 	 Loss: 14.081274
Train Epoch: 59 	 Loss: 13.609969
Train Epoch: 60 	 Loss: 13.905516
Train Epoch: 61 	 Loss: 14.282665
Train Epoch: 62 	 Loss: 14.476789
Train Epoch: 63 	 Loss: 13.634481
Train Epoch: 64 	 Loss: 14.186123
Train Epoch: 65 	 Loss: 14.417957
Train Epoch: 66 	 Loss: 14.349169
Train Epoch: 67 	 Loss: 14.489760
Train Epoch: 68 	 Loss: 14.011948
Train Epoch: 69 	 Loss: 13.753372
Train Epoch: 70 	 Loss: 13.694582
Train Epoch: 71 	 Loss: 13.572947
Train Epoch: 72 	 Loss: 13.772346
Train Epoch: 73 	 Loss: 14.211587
Train Epoch: 74 	 Loss: 13.947216
Train Epoch: 75 	 Loss: 13.712866
Train Epoch: 76 	 Loss: 13.950809
Train Epoch: 77 	 Loss: 13.968909
Train Epoch: 78 	 Loss: 14.961859
Train Epoch: 79 	 Loss: 13.607607
Train Epoch: 80 	 Loss: 13.796653
Train Epoch: 81 	 Loss: 13.802117
Train Epoch: 82 	 Loss: 13.704330
Train Epoch: 83 	 Loss: 13.721638
Train Epoch: 84 	 Loss: 13.536328
Train Epoch: 85 	 Loss: 13.723990
Train Epoch: 86 	 Loss: 13.937641
Train Epoch: 87 	 Loss: 13.565534
Train Epoch: 88 	 Loss: 14.015862
Train Epoch: 89 	 Loss: 13.577598
Train Epoch: 90 	 Loss: 13.947340
Train Epoch: 91 	 Loss: 13.563938
Train Epoch: 92 	 Loss: 13.352024
Train Epoch: 93 	 Loss: 13.560486
Train Epoch: 94 	 Loss: 13.489132
Train Epoch: 95 	 Loss: 13.599055
Train Epoch: 96 	 Loss: 14.475766
Train Epoch: 97 	 Loss: 14.377518
Train Epoch: 98 	 Loss: 14.185813
Train Epoch: 99 	 Loss: 14.665306
Train Epoch: 100 	 Loss: 14.060395
Train Epoch: 101 	 Loss: 14.007023
Train Epoch: 102 	 Loss: 13.717766
Train Epoch: 103 	 Loss: 13.597260
Train Epoch: 104 	 Loss: 13.904465
Train Epoch: 105 	 Loss: 13.776597
Train Epoch: 106 	 Loss: 13.640873
Train Epoch: 107 	 Loss: 13.734875
Train Epoch: 108 	 Loss: 13.863100
Train Epoch: 109 	 Loss: 13.495182
Train Epoch: 110 	 Loss: 13.457897
Train Epoch: 111 	 Loss: 13.543859
Train Epoch: 112 	 Loss: 13.416408
Train Epoch: 113 	 Loss: 13.243385
Train Epoch: 114 	 Loss: 13.552073
Train Epoch: 115 	 Loss: 13.052413
Train Epoch: 116 	 Loss: 13.522324
Train Epoch: 117 	 Loss: 13.804482
Train Epoch: 118 	 Loss: 13.478838
Train Epoch: 119 	 Loss: 13.786465
Train Epoch: 120 	 Loss: 13.379094
Train Epoch: 121 	 Loss: 13.381815
Train Epoch: 122 	 Loss: 13.332158
Train Epoch: 123 	 Loss: 13.504086
Train Epoch: 124 	 Loss: 13.763603
Train Epoch: 125 	 Loss: 13.535839
Train Epoch: 126 	 Loss: 13.635267
Train Epoch: 127 	 Loss: 13.639078
Train Epoch: 128 	 Loss: 13.414286
Train Epoch: 129 	 Loss: 13.850824
Train Epoch: 130 	 Loss: 13.281932
Train Epoch: 131 	 Loss: 13.531958
Train Epoch: 132 	 Loss: 13.930182
Train Epoch: 133 	 Loss: 13.503989
Train Epoch: 134 	 Loss: 13.684021
Train Epoch: 135 	 Loss: 14.316889
Train Epoch: 136 	 Loss: 14.205859
Train Epoch: 137 	 Loss: 14.295204
Train Epoch: 138 	 Loss: 13.462675
Train Epoch: 139 	 Loss: 13.347815
Train Epoch: 140 	 Loss: 13.586326
Train Epoch: 141 	 Loss: 13.684899
Train Epoch: 142 	 Loss: 14.011162
Train Epoch: 143 	 Loss: 13.455526
Train Epoch: 144 	 Loss: 13.572227
Train Epoch: 145 	 Loss: 13.734215
Train Epoch: 146 	 Loss: 13.265852
Train Epoch: 147 	 Loss: 13.568990
Train Epoch: 148 	 Loss: 13.395198
Train Epoch: 149 	 Loss: 13.900051
Train Epoch: 150 	 Loss: 13.331632
Train Epoch: 151 	 Loss: 13.432185
Train Epoch: 152 	 Loss: 13.552233
Train Epoch: 153 	 Loss: 13.748602
Train Epoch: 154 	 Loss: 13.895352
Train Epoch: 155 	 Loss: 13.767532
Train Epoch: 156 	 Loss: 13.307550
Train Epoch: 157 	 Loss: 13.278461
Train Epoch: 158 	 Loss: 13.649371
Train Epoch: 159 	 Loss: 13.392673
Train Epoch: 160 	 Loss: 13.514699
Train Epoch: 161 	 Loss: 13.438307
Train Epoch: 162 	 Loss: 13.265329
Train Epoch: 163 	 Loss: 13.783237
Train Epoch: 164 	 Loss: 13.385414
Train Epoch: 165 	 Loss: 13.368755
Train Epoch: 166 	 Loss: 13.540491
Train Epoch: 167 	 Loss: 13.290918
Train Epoch: 168 	 Loss: 14.005730
Train Epoch: 169 	 Loss: 13.473643
Train Epoch: 170 	 Loss: 13.391628
Train Epoch: 171 	 Loss: 13.666999
Train Epoch: 172 	 Loss: 13.620702
Train Epoch: 173 	 Loss: 13.417818
Train Epoch: 174 	 Loss: 13.882450
Train Epoch: 175 	 Loss: 13.445570
Train Epoch: 176 	 Loss: 13.337571
Train Epoch: 177 	 Loss: 13.711828
Train Epoch: 178 	 Loss: 13.475100
Train Epoch: 179 	 Loss: 13.430710
Train Epoch: 180 	 Loss: 13.615714
Train Epoch: 181 	 Loss: 13.452950
Train Epoch: 182 	 Loss: 13.405034
Train Epoch: 183 	 Loss: 13.502401
Train Epoch: 184 	 Loss: 13.217889
Train Epoch: 185 	 Loss: 13.464902
Train Epoch: 186 	 Loss: 13.515949
Train Epoch: 187 	 Loss: 13.299271
Train Epoch: 188 	 Loss: 13.283911
Train Epoch: 189 	 Loss: 13.387737
Train Epoch: 190 	 Loss: 13.337498
Train Epoch: 191 	 Loss: 13.431739
Train Epoch: 192 	 Loss: 13.338224
Train Epoch: 193 	 Loss: 13.498031
Train Epoch: 194 	 Loss: 13.721934
Train Epoch: 195 	 Loss: 13.383576
Train Epoch: 196 	 Loss: 13.320604
Train Epoch: 197 	 Loss: 13.398998
Train Epoch: 198 	 Loss: 13.712202
Train Epoch: 199 	 Loss: 14.153811

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.407

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.911

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.847

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.322

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.924

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.915

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.917

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.894

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.980

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.936

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.562

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.944
------
f1 mean across methods is 0.797


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.87391125e-01 4.86136652e-01 2.05491169e-02 5.67363754e-03
 2.49468987e-04] 

Train Epoch: 0 	 Loss: 17.884266
Train Epoch: 1 	 Loss: 17.769243
Train Epoch: 2 	 Loss: 15.718351
Train Epoch: 3 	 Loss: 15.385706
Train Epoch: 4 	 Loss: 13.192670
Train Epoch: 5 	 Loss: 13.038163
Train Epoch: 6 	 Loss: 13.078893
Train Epoch: 7 	 Loss: 12.542231
Train Epoch: 8 	 Loss: 12.955443
Train Epoch: 9 	 Loss: 14.039755
Train Epoch: 10 	 Loss: 13.387880
Train Epoch: 11 	 Loss: 13.294045
Train Epoch: 12 	 Loss: 12.834570
Train Epoch: 13 	 Loss: 13.428889
Train Epoch: 14 	 Loss: 12.593278
Train Epoch: 15 	 Loss: 13.083397
Train Epoch: 16 	 Loss: 12.608314
Train Epoch: 17 	 Loss: 12.455286
Train Epoch: 18 	 Loss: 12.499355
Train Epoch: 19 	 Loss: 13.105927
Train Epoch: 20 	 Loss: 12.892302
Train Epoch: 21 	 Loss: 12.655891
Train Epoch: 22 	 Loss: 12.838367
Train Epoch: 23 	 Loss: 12.302696
Train Epoch: 24 	 Loss: 12.863276
Train Epoch: 25 	 Loss: 12.191905
Train Epoch: 26 	 Loss: 11.690545
Train Epoch: 27 	 Loss: 13.070810
Train Epoch: 28 	 Loss: 12.474219
Train Epoch: 29 	 Loss: 13.383599
Train Epoch: 30 	 Loss: 12.412332
Train Epoch: 31 	 Loss: 13.485567
Train Epoch: 32 	 Loss: 12.154436
Train Epoch: 33 	 Loss: 12.592643
Train Epoch: 34 	 Loss: 12.814973
Train Epoch: 35 	 Loss: 11.886908
Train Epoch: 36 	 Loss: 12.306977
Train Epoch: 37 	 Loss: 12.058743
Train Epoch: 38 	 Loss: 11.937893
Train Epoch: 39 	 Loss: 12.459032
Train Epoch: 40 	 Loss: 11.876404
Train Epoch: 41 	 Loss: 12.960514
Train Epoch: 42 	 Loss: 13.023052
Train Epoch: 43 	 Loss: 11.678987
Train Epoch: 44 	 Loss: 12.033123
Train Epoch: 45 	 Loss: 12.986029
Train Epoch: 46 	 Loss: 12.013359
Train Epoch: 47 	 Loss: 12.449194
Train Epoch: 48 	 Loss: 12.631203
Train Epoch: 49 	 Loss: 12.402357
Train Epoch: 50 	 Loss: 12.580887
Train Epoch: 51 	 Loss: 12.203188
Train Epoch: 52 	 Loss: 12.698212
Train Epoch: 53 	 Loss: 12.169140
Train Epoch: 54 	 Loss: 12.121801
Train Epoch: 55 	 Loss: 12.139791
Train Epoch: 56 	 Loss: 12.545637
Train Epoch: 57 	 Loss: 12.183275
Train Epoch: 58 	 Loss: 11.571287
Train Epoch: 59 	 Loss: 12.602242
Train Epoch: 60 	 Loss: 12.153915
Train Epoch: 61 	 Loss: 12.057369
Train Epoch: 62 	 Loss: 12.456930
Train Epoch: 63 	 Loss: 12.433319
Train Epoch: 64 	 Loss: 12.158825
Train Epoch: 65 	 Loss: 12.048883
Train Epoch: 66 	 Loss: 12.580402
Train Epoch: 67 	 Loss: 11.929719
Train Epoch: 68 	 Loss: 13.439461
Train Epoch: 69 	 Loss: 11.578266
Train Epoch: 70 	 Loss: 12.099831
Train Epoch: 71 	 Loss: 12.611376
Train Epoch: 72 	 Loss: 12.581016
Train Epoch: 73 	 Loss: 12.834199
Train Epoch: 74 	 Loss: 12.496096
Train Epoch: 75 	 Loss: 12.564843
Train Epoch: 76 	 Loss: 11.710794
Train Epoch: 77 	 Loss: 11.316255
Train Epoch: 78 	 Loss: 13.294524
Train Epoch: 79 	 Loss: 11.833525
Train Epoch: 80 	 Loss: 12.434208
Train Epoch: 81 	 Loss: 12.185202
Train Epoch: 82 	 Loss: 12.313654
Train Epoch: 83 	 Loss: 12.134109
Train Epoch: 84 	 Loss: 11.288921
Train Epoch: 85 	 Loss: 12.388194
Train Epoch: 86 	 Loss: 11.130198
Train Epoch: 87 	 Loss: 11.824839
Train Epoch: 88 	 Loss: 12.136148
Train Epoch: 89 	 Loss: 12.345833
Train Epoch: 90 	 Loss: 11.739408
Train Epoch: 91 	 Loss: 12.370867
Train Epoch: 92 	 Loss: 11.944306
Train Epoch: 93 	 Loss: 11.243297
Train Epoch: 94 	 Loss: 11.692539
Train Epoch: 95 	 Loss: 12.376785
Train Epoch: 96 	 Loss: 12.116620
Train Epoch: 97 	 Loss: 11.802530
Train Epoch: 98 	 Loss: 12.139494
Train Epoch: 99 	 Loss: 11.442698
Train Epoch: 100 	 Loss: 11.793386
Train Epoch: 101 	 Loss: 12.667397
Train Epoch: 102 	 Loss: 11.808523
Train Epoch: 103 	 Loss: 12.016414
Train Epoch: 104 	 Loss: 12.039825
Train Epoch: 105 	 Loss: 11.508324
Train Epoch: 106 	 Loss: 11.886456
Train Epoch: 107 	 Loss: 11.828712
Train Epoch: 108 	 Loss: 12.386464
Train Epoch: 109 	 Loss: 11.204805
Train Epoch: 110 	 Loss: 11.894459
Train Epoch: 111 	 Loss: 11.671221
Train Epoch: 112 	 Loss: 11.573539
Train Epoch: 113 	 Loss: 12.197640
Train Epoch: 114 	 Loss: 11.126481
Train Epoch: 115 	 Loss: 11.437155
Train Epoch: 116 	 Loss: 11.934756
Train Epoch: 117 	 Loss: 12.241054
Train Epoch: 118 	 Loss: 11.745221
Train Epoch: 119 	 Loss: 11.818397
Train Epoch: 120 	 Loss: 11.457984
Train Epoch: 121 	 Loss: 11.420351
Train Epoch: 122 	 Loss: 11.702971
Train Epoch: 123 	 Loss: 12.026768
Train Epoch: 124 	 Loss: 11.777504
Train Epoch: 125 	 Loss: 11.853237
Train Epoch: 126 	 Loss: 11.715697
Train Epoch: 127 	 Loss: 11.762215
Train Epoch: 128 	 Loss: 11.672409
Train Epoch: 129 	 Loss: 12.285387
Train Epoch: 130 	 Loss: 12.404776
Train Epoch: 131 	 Loss: 11.761529
Train Epoch: 132 	 Loss: 12.228099
Train Epoch: 133 	 Loss: 12.123468
Train Epoch: 134 	 Loss: 11.464081
Train Epoch: 135 	 Loss: 11.856103
Train Epoch: 136 	 Loss: 11.645008
Train Epoch: 137 	 Loss: 12.563574
Train Epoch: 138 	 Loss: 12.229790
Train Epoch: 139 	 Loss: 11.609702
Train Epoch: 140 	 Loss: 11.664386
Train Epoch: 141 	 Loss: 11.555347
Train Epoch: 142 	 Loss: 12.201912
Train Epoch: 143 	 Loss: 11.795981
Train Epoch: 144 	 Loss: 11.718990
Train Epoch: 145 	 Loss: 11.882954
Train Epoch: 146 	 Loss: 12.372576
Train Epoch: 147 	 Loss: 11.752247
Train Epoch: 148 	 Loss: 11.623411
Train Epoch: 149 	 Loss: 11.691756
Train Epoch: 150 	 Loss: 12.577544
Train Epoch: 151 	 Loss: 11.520996
Train Epoch: 152 	 Loss: 12.162250
Train Epoch: 153 	 Loss: 12.007198
Train Epoch: 154 	 Loss: 11.835430
Train Epoch: 155 	 Loss: 12.203606
Train Epoch: 156 	 Loss: 11.620192
Train Epoch: 157 	 Loss: 11.843182
Train Epoch: 158 	 Loss: 11.889345
Train Epoch: 159 	 Loss: 12.459762
Train Epoch: 160 	 Loss: 11.645169
Train Epoch: 161 	 Loss: 11.418633
Train Epoch: 162 	 Loss: 11.969828
Train Epoch: 163 	 Loss: 12.137448
Train Epoch: 164 	 Loss: 12.119342
Train Epoch: 165 	 Loss: 11.967691
Train Epoch: 166 	 Loss: 12.238894
Train Epoch: 167 	 Loss: 11.916733
Train Epoch: 168 	 Loss: 11.917992
Train Epoch: 169 	 Loss: 11.632730
Train Epoch: 170 	 Loss: 11.943027
Train Epoch: 171 	 Loss: 12.481478
Train Epoch: 172 	 Loss: 11.768651
Train Epoch: 173 	 Loss: 11.957254
Train Epoch: 174 	 Loss: 11.550991
Train Epoch: 175 	 Loss: 11.921850
Train Epoch: 176 	 Loss: 11.642975
Train Epoch: 177 	 Loss: 12.281597
Train Epoch: 178 	 Loss: 11.668231
Train Epoch: 179 	 Loss: 11.728176
Train Epoch: 180 	 Loss: 12.444724
Train Epoch: 181 	 Loss: 11.789627
Train Epoch: 182 	 Loss: 11.717652
Train Epoch: 183 	 Loss: 11.803526
Train Epoch: 184 	 Loss: 11.827033
Train Epoch: 185 	 Loss: 11.210692
Train Epoch: 186 	 Loss: 11.917725
Train Epoch: 187 	 Loss: 12.127824
Train Epoch: 188 	 Loss: 11.773115
Train Epoch: 189 	 Loss: 11.839401
Train Epoch: 190 	 Loss: 11.875808
Train Epoch: 191 	 Loss: 11.731236
Train Epoch: 192 	 Loss: 11.968384
Train Epoch: 193 	 Loss: 11.896560
Train Epoch: 194 	 Loss: 12.090485
Train Epoch: 195 	 Loss: 11.704641
Train Epoch: 196 	 Loss: 12.033812
Train Epoch: 197 	 Loss: 11.975043
Train Epoch: 198 	 Loss: 11.967091
Train Epoch: 199 	 Loss: 11.641609

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.863

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.458

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.843

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.864

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.890

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.617

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.916

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.899

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.907

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.779

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.863

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.916
------
f1 mean across methods is 0.818


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.89322727e-01 4.84254943e-01 2.03637971e-02 5.80193588e-03
 2.56596673e-04] 

Train Epoch: 0 	 Loss: 19.593885
Train Epoch: 1 	 Loss: 20.451330
Train Epoch: 2 	 Loss: 16.251886
Train Epoch: 3 	 Loss: 15.449133
Train Epoch: 4 	 Loss: 15.484089
Train Epoch: 5 	 Loss: 14.829618
Train Epoch: 6 	 Loss: 14.539691
Train Epoch: 7 	 Loss: 14.420154
Train Epoch: 8 	 Loss: 13.693208
Train Epoch: 9 	 Loss: 14.811162
Train Epoch: 10 	 Loss: 14.449881
Train Epoch: 11 	 Loss: 14.750081
Train Epoch: 12 	 Loss: 14.500326
Train Epoch: 13 	 Loss: 15.102135
Train Epoch: 14 	 Loss: 15.044150
Train Epoch: 15 	 Loss: 14.282931
Train Epoch: 16 	 Loss: 13.781724
Train Epoch: 17 	 Loss: 13.896811
Train Epoch: 18 	 Loss: 13.539671
Train Epoch: 19 	 Loss: 13.694145
Train Epoch: 20 	 Loss: 14.629709
Train Epoch: 21 	 Loss: 14.065921
Train Epoch: 22 	 Loss: 13.645401
Train Epoch: 23 	 Loss: 13.634562
Train Epoch: 24 	 Loss: 13.212332
Train Epoch: 25 	 Loss: 13.348001
Train Epoch: 26 	 Loss: 13.404367
Train Epoch: 27 	 Loss: 13.389865
Train Epoch: 28 	 Loss: 13.888622
Train Epoch: 29 	 Loss: 14.530165
Train Epoch: 30 	 Loss: 14.443147
Train Epoch: 31 	 Loss: 13.202190
Train Epoch: 32 	 Loss: 13.874705
Train Epoch: 33 	 Loss: 13.968861
Train Epoch: 34 	 Loss: 13.326408
Train Epoch: 35 	 Loss: 13.455612
Train Epoch: 36 	 Loss: 14.055353
Train Epoch: 37 	 Loss: 13.909695
Train Epoch: 38 	 Loss: 13.417538
Train Epoch: 39 	 Loss: 13.449670
Train Epoch: 40 	 Loss: 13.357971
Train Epoch: 41 	 Loss: 13.707412
Train Epoch: 42 	 Loss: 13.476488
Train Epoch: 43 	 Loss: 13.429212
Train Epoch: 44 	 Loss: 13.369404
Train Epoch: 45 	 Loss: 13.057080
Train Epoch: 46 	 Loss: 13.718905
Train Epoch: 47 	 Loss: 13.487007
Train Epoch: 48 	 Loss: 13.601987
Train Epoch: 49 	 Loss: 14.111530
Train Epoch: 50 	 Loss: 13.630261
Train Epoch: 51 	 Loss: 13.476132
Train Epoch: 52 	 Loss: 13.189845
Train Epoch: 53 	 Loss: 14.206687
Train Epoch: 54 	 Loss: 14.175173
Train Epoch: 55 	 Loss: 13.178577
Train Epoch: 56 	 Loss: 13.156615
Train Epoch: 57 	 Loss: 12.753431
Train Epoch: 58 	 Loss: 13.286795
Train Epoch: 59 	 Loss: 13.023944
Train Epoch: 60 	 Loss: 13.125088
Train Epoch: 61 	 Loss: 13.435555
Train Epoch: 62 	 Loss: 13.583307
Train Epoch: 63 	 Loss: 14.695127
Train Epoch: 64 	 Loss: 13.327421
Train Epoch: 65 	 Loss: 13.885090
Train Epoch: 66 	 Loss: 13.650082
Train Epoch: 67 	 Loss: 12.892219
Train Epoch: 68 	 Loss: 12.994087
Train Epoch: 69 	 Loss: 13.719632
Train Epoch: 70 	 Loss: 13.137539
Train Epoch: 71 	 Loss: 14.329703
Train Epoch: 72 	 Loss: 13.383123
Train Epoch: 73 	 Loss: 13.305230
Train Epoch: 74 	 Loss: 13.388566
Train Epoch: 75 	 Loss: 13.532429
Train Epoch: 76 	 Loss: 12.937360
Train Epoch: 77 	 Loss: 12.765871
Train Epoch: 78 	 Loss: 13.043262
Train Epoch: 79 	 Loss: 13.290027
Train Epoch: 80 	 Loss: 13.416156
Train Epoch: 81 	 Loss: 14.103170
Train Epoch: 82 	 Loss: 13.365856
Train Epoch: 83 	 Loss: 12.959429
Train Epoch: 84 	 Loss: 13.718971
Train Epoch: 85 	 Loss: 13.656578
Train Epoch: 86 	 Loss: 14.434956
Train Epoch: 87 	 Loss: 13.457060
Train Epoch: 88 	 Loss: 13.569963
Train Epoch: 89 	 Loss: 13.292389
Train Epoch: 90 	 Loss: 13.321919
Train Epoch: 91 	 Loss: 12.982442
Train Epoch: 92 	 Loss: 13.273433
Train Epoch: 93 	 Loss: 12.960868
Train Epoch: 94 	 Loss: 12.869470
Train Epoch: 95 	 Loss: 13.293189
Train Epoch: 96 	 Loss: 13.829070
Train Epoch: 97 	 Loss: 13.320654
Train Epoch: 98 	 Loss: 12.971206
Train Epoch: 99 	 Loss: 13.150087
Train Epoch: 100 	 Loss: 13.117066
Train Epoch: 101 	 Loss: 13.213585
Train Epoch: 102 	 Loss: 13.179469
Train Epoch: 103 	 Loss: 13.962797
Train Epoch: 104 	 Loss: 13.238306
Train Epoch: 105 	 Loss: 13.480268
Train Epoch: 106 	 Loss: 12.919635
Train Epoch: 107 	 Loss: 13.272776
Train Epoch: 108 	 Loss: 13.384230
Train Epoch: 109 	 Loss: 12.826433
Train Epoch: 110 	 Loss: 13.067762
Train Epoch: 111 	 Loss: 13.287868
Train Epoch: 112 	 Loss: 13.279453
Train Epoch: 113 	 Loss: 12.898424
Train Epoch: 114 	 Loss: 12.884843
Train Epoch: 115 	 Loss: 12.738113
Train Epoch: 116 	 Loss: 13.146582
Train Epoch: 117 	 Loss: 12.947360
Train Epoch: 118 	 Loss: 12.994010
Train Epoch: 119 	 Loss: 13.155834
Train Epoch: 120 	 Loss: 13.339727
Train Epoch: 121 	 Loss: 13.466804
Train Epoch: 122 	 Loss: 12.927858
Train Epoch: 123 	 Loss: 13.366899
Train Epoch: 124 	 Loss: 13.073730
Train Epoch: 125 	 Loss: 13.095781
Train Epoch: 126 	 Loss: 12.976194
Train Epoch: 127 	 Loss: 12.822401
Train Epoch: 128 	 Loss: 13.793759
Train Epoch: 129 	 Loss: 13.124053
Train Epoch: 130 	 Loss: 12.354229
Train Epoch: 131 	 Loss: 13.103119
Train Epoch: 132 	 Loss: 13.580105
Train Epoch: 133 	 Loss: 13.535249
Train Epoch: 134 	 Loss: 12.851681
Train Epoch: 135 	 Loss: 14.160438
Train Epoch: 136 	 Loss: 13.263650
Train Epoch: 137 	 Loss: 12.993797
Train Epoch: 138 	 Loss: 12.987795
Train Epoch: 139 	 Loss: 13.048841
Train Epoch: 140 	 Loss: 12.879091
Train Epoch: 141 	 Loss: 12.879836
Train Epoch: 142 	 Loss: 12.763628
Train Epoch: 143 	 Loss: 12.849869
Train Epoch: 144 	 Loss: 12.811136
Train Epoch: 145 	 Loss: 12.859886
Train Epoch: 146 	 Loss: 13.831911
Train Epoch: 147 	 Loss: 13.045502
Train Epoch: 148 	 Loss: 12.899705
Train Epoch: 149 	 Loss: 12.290026
Train Epoch: 150 	 Loss: 13.058008
Train Epoch: 151 	 Loss: 13.063100
Train Epoch: 152 	 Loss: 13.185262
Train Epoch: 153 	 Loss: 12.984131
Train Epoch: 154 	 Loss: 13.183253
Train Epoch: 155 	 Loss: 12.902581
Train Epoch: 156 	 Loss: 13.080361
Train Epoch: 157 	 Loss: 12.960311
Train Epoch: 158 	 Loss: 12.808825
Train Epoch: 159 	 Loss: 13.403912
Train Epoch: 160 	 Loss: 13.317848
Train Epoch: 161 	 Loss: 13.262884
Train Epoch: 162 	 Loss: 12.951392
Train Epoch: 163 	 Loss: 12.465478
Train Epoch: 164 	 Loss: 12.996454
Train Epoch: 165 	 Loss: 12.913385
Train Epoch: 166 	 Loss: 13.078210
Train Epoch: 167 	 Loss: 13.220438
Train Epoch: 168 	 Loss: 12.900065
Train Epoch: 169 	 Loss: 13.191956
Train Epoch: 170 	 Loss: 13.173891
Train Epoch: 171 	 Loss: 12.911525
Train Epoch: 172 	 Loss: 13.102449
Train Epoch: 173 	 Loss: 14.071638
Train Epoch: 174 	 Loss: 13.192203
Train Epoch: 175 	 Loss: 12.999630
Train Epoch: 176 	 Loss: 12.780787
Train Epoch: 177 	 Loss: 12.983064
Train Epoch: 178 	 Loss: 12.587388
Train Epoch: 179 	 Loss: 12.660823
Train Epoch: 180 	 Loss: 13.127453
Train Epoch: 181 	 Loss: 12.917583
Train Epoch: 182 	 Loss: 12.866179
Train Epoch: 183 	 Loss: 12.837305
Train Epoch: 184 	 Loss: 12.639732
Train Epoch: 185 	 Loss: 13.046243
Train Epoch: 186 	 Loss: 12.991448
Train Epoch: 187 	 Loss: 12.928671
Train Epoch: 188 	 Loss: 12.776044
Train Epoch: 189 	 Loss: 12.856644
Train Epoch: 190 	 Loss: 12.653957
Train Epoch: 191 	 Loss: 12.847753
Train Epoch: 192 	 Loss: 13.426871
Train Epoch: 193 	 Loss: 12.552322
Train Epoch: 194 	 Loss: 12.338938
Train Epoch: 195 	 Loss: 12.846038
Train Epoch: 196 	 Loss: 12.551682
Train Epoch: 197 	 Loss: 12.836714
Train Epoch: 198 	 Loss: 12.943616
Train Epoch: 199 	 Loss: 13.044836

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.856

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.461

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.824

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.857

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.920

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.768

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.925

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.831

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.911

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.899

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.861

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.924
------
f1 mean across methods is 0.836


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.88766768e-01 4.84896435e-01 2.05562446e-02 5.52395615e-03
 2.56596673e-04] 

Train Epoch: 0 	 Loss: 19.531174
Train Epoch: 1 	 Loss: 16.870029
Train Epoch: 2 	 Loss: 18.171114
Train Epoch: 3 	 Loss: 14.366302
Train Epoch: 4 	 Loss: 14.095448
Train Epoch: 5 	 Loss: 13.731143
Train Epoch: 6 	 Loss: 14.335140
Train Epoch: 7 	 Loss: 13.859084
Train Epoch: 8 	 Loss: 13.393361
Train Epoch: 9 	 Loss: 13.889872
Train Epoch: 10 	 Loss: 13.701530
Train Epoch: 11 	 Loss: 14.161758
Train Epoch: 12 	 Loss: 13.662523
Train Epoch: 13 	 Loss: 13.649410
Train Epoch: 14 	 Loss: 13.831141
Train Epoch: 15 	 Loss: 13.067451
Train Epoch: 16 	 Loss: 13.742078
Train Epoch: 17 	 Loss: 13.664394
Train Epoch: 18 	 Loss: 13.631746
Train Epoch: 19 	 Loss: 13.566025
Train Epoch: 20 	 Loss: 13.750671
Train Epoch: 21 	 Loss: 13.978447
Train Epoch: 22 	 Loss: 13.798362
Train Epoch: 23 	 Loss: 13.511314
Train Epoch: 24 	 Loss: 13.847820
Train Epoch: 25 	 Loss: 13.757196
Train Epoch: 26 	 Loss: 13.590960
Train Epoch: 27 	 Loss: 13.290421
Train Epoch: 28 	 Loss: 14.055904
Train Epoch: 29 	 Loss: 13.537954
Train Epoch: 30 	 Loss: 13.598202
Train Epoch: 31 	 Loss: 13.501339
Train Epoch: 32 	 Loss: 13.821003
Train Epoch: 33 	 Loss: 13.537549
Train Epoch: 34 	 Loss: 13.634514
Train Epoch: 35 	 Loss: 13.363980
Train Epoch: 36 	 Loss: 13.901075
Train Epoch: 37 	 Loss: 13.367345
Train Epoch: 38 	 Loss: 13.777528
Train Epoch: 39 	 Loss: 13.472360
Train Epoch: 40 	 Loss: 13.747694
Train Epoch: 41 	 Loss: 13.576004
Train Epoch: 42 	 Loss: 13.391088
Train Epoch: 43 	 Loss: 13.495825
Train Epoch: 44 	 Loss: 13.735924
Train Epoch: 45 	 Loss: 13.559706
Train Epoch: 46 	 Loss: 13.730137
Train Epoch: 47 	 Loss: 13.677010
Train Epoch: 48 	 Loss: 13.743675
Train Epoch: 49 	 Loss: 13.800449
Train Epoch: 50 	 Loss: 13.745035
Train Epoch: 51 	 Loss: 13.909676
Train Epoch: 52 	 Loss: 13.511196
Train Epoch: 53 	 Loss: 13.891411
Train Epoch: 54 	 Loss: 13.549803
Train Epoch: 55 	 Loss: 13.702753
Train Epoch: 56 	 Loss: 13.382466
Train Epoch: 57 	 Loss: 13.642438
Train Epoch: 58 	 Loss: 14.209186
Train Epoch: 59 	 Loss: 13.469755
Train Epoch: 60 	 Loss: 13.399645
Train Epoch: 61 	 Loss: 13.366169
Train Epoch: 62 	 Loss: 13.520894
Train Epoch: 63 	 Loss: 13.581529
Train Epoch: 64 	 Loss: 13.689686
Train Epoch: 65 	 Loss: 13.434187
Train Epoch: 66 	 Loss: 13.451319
Train Epoch: 67 	 Loss: 13.522007
Train Epoch: 68 	 Loss: 13.460478
Train Epoch: 69 	 Loss: 13.291105
Train Epoch: 70 	 Loss: 13.348640
Train Epoch: 71 	 Loss: 13.674198
Train Epoch: 72 	 Loss: 13.963751
Train Epoch: 73 	 Loss: 13.390108
Train Epoch: 74 	 Loss: 13.449260
Train Epoch: 75 	 Loss: 13.552606
Train Epoch: 76 	 Loss: 13.276794
Train Epoch: 77 	 Loss: 13.403775
Train Epoch: 78 	 Loss: 13.384947
Train Epoch: 79 	 Loss: 13.462645
Train Epoch: 80 	 Loss: 13.625601
Train Epoch: 81 	 Loss: 13.467289
Train Epoch: 82 	 Loss: 13.749375
Train Epoch: 83 	 Loss: 13.417809
Train Epoch: 84 	 Loss: 13.291553
Train Epoch: 85 	 Loss: 13.616917
Train Epoch: 86 	 Loss: 13.272423
Train Epoch: 87 	 Loss: 13.593480
Train Epoch: 88 	 Loss: 14.637634
Train Epoch: 89 	 Loss: 13.392770
Train Epoch: 90 	 Loss: 13.328040
Train Epoch: 91 	 Loss: 13.273237
Train Epoch: 92 	 Loss: 13.441999
Train Epoch: 93 	 Loss: 13.513072
Train Epoch: 94 	 Loss: 13.882195
Train Epoch: 95 	 Loss: 14.112886
Train Epoch: 96 	 Loss: 13.371832
Train Epoch: 97 	 Loss: 13.353165
Train Epoch: 98 	 Loss: 13.870304
Train Epoch: 99 	 Loss: 13.175759
Train Epoch: 100 	 Loss: 13.326120
Train Epoch: 101 	 Loss: 13.527992
Train Epoch: 102 	 Loss: 13.809516
Train Epoch: 103 	 Loss: 13.673500
Train Epoch: 104 	 Loss: 13.389496
Train Epoch: 105 	 Loss: 13.335510
Train Epoch: 106 	 Loss: 13.646845
Train Epoch: 107 	 Loss: 13.511106
Train Epoch: 108 	 Loss: 13.252897
Train Epoch: 109 	 Loss: 13.221416
Train Epoch: 110 	 Loss: 13.275567
Train Epoch: 111 	 Loss: 13.368166
Train Epoch: 112 	 Loss: 13.295044
Train Epoch: 113 	 Loss: 13.616785
Train Epoch: 114 	 Loss: 13.246194
Train Epoch: 115 	 Loss: 13.430899
Train Epoch: 116 	 Loss: 13.456416
Train Epoch: 117 	 Loss: 13.362577
Train Epoch: 118 	 Loss: 13.249990
Train Epoch: 119 	 Loss: 13.518473
Train Epoch: 120 	 Loss: 13.494413
Train Epoch: 121 	 Loss: 13.237320
Train Epoch: 122 	 Loss: 13.587082
Train Epoch: 123 	 Loss: 13.311220
Train Epoch: 124 	 Loss: 13.447494
Train Epoch: 125 	 Loss: 13.270638
Train Epoch: 126 	 Loss: 13.229704
Train Epoch: 127 	 Loss: 13.471743
Train Epoch: 128 	 Loss: 13.340870
Train Epoch: 129 	 Loss: 13.204162
Train Epoch: 130 	 Loss: 13.974088
Train Epoch: 131 	 Loss: 13.418533
Train Epoch: 132 	 Loss: 13.331921
Train Epoch: 133 	 Loss: 13.374514
Train Epoch: 134 	 Loss: 13.213495
Train Epoch: 135 	 Loss: 13.192272
Train Epoch: 136 	 Loss: 13.726261
Train Epoch: 137 	 Loss: 13.297517
Train Epoch: 138 	 Loss: 13.252762
Train Epoch: 139 	 Loss: 13.411519
Train Epoch: 140 	 Loss: 13.420667
Train Epoch: 141 	 Loss: 13.560780
Train Epoch: 142 	 Loss: 13.347500
Train Epoch: 143 	 Loss: 13.579569
Train Epoch: 144 	 Loss: 13.405574
Train Epoch: 145 	 Loss: 13.499075
Train Epoch: 146 	 Loss: 13.272879
Train Epoch: 147 	 Loss: 13.349141
Train Epoch: 148 	 Loss: 13.336354
Train Epoch: 149 	 Loss: 14.334078
Train Epoch: 150 	 Loss: 13.330761
Train Epoch: 151 	 Loss: 13.370058
Train Epoch: 152 	 Loss: 13.320448
Train Epoch: 153 	 Loss: 13.334188
Train Epoch: 154 	 Loss: 13.457349
Train Epoch: 155 	 Loss: 13.382774
Train Epoch: 156 	 Loss: 13.509853
Train Epoch: 157 	 Loss: 13.273705
Train Epoch: 158 	 Loss: 13.217585
Train Epoch: 159 	 Loss: 13.375583
Train Epoch: 160 	 Loss: 13.348189
Train Epoch: 161 	 Loss: 13.325174
Train Epoch: 162 	 Loss: 13.270629
Train Epoch: 163 	 Loss: 13.387161
Train Epoch: 164 	 Loss: 13.344232
Train Epoch: 165 	 Loss: 13.210426
Train Epoch: 166 	 Loss: 13.239669
Train Epoch: 167 	 Loss: 13.295865
Train Epoch: 168 	 Loss: 13.935579
Train Epoch: 169 	 Loss: 13.277073
Train Epoch: 170 	 Loss: 13.243092
Train Epoch: 171 	 Loss: 13.257603
Train Epoch: 172 	 Loss: 13.338251
Train Epoch: 173 	 Loss: 13.260452
Train Epoch: 174 	 Loss: 13.397776
Train Epoch: 175 	 Loss: 13.345720
Train Epoch: 176 	 Loss: 13.555464
Train Epoch: 177 	 Loss: 13.265464
Train Epoch: 178 	 Loss: 13.409786
Train Epoch: 179 	 Loss: 13.304747
Train Epoch: 180 	 Loss: 13.729416
Train Epoch: 181 	 Loss: 13.429311
Train Epoch: 182 	 Loss: 13.551277
Train Epoch: 183 	 Loss: 13.199041
Train Epoch: 184 	 Loss: 13.599652
Train Epoch: 185 	 Loss: 13.673841
Train Epoch: 186 	 Loss: 13.303067
Train Epoch: 187 	 Loss: 13.554184
Train Epoch: 188 	 Loss: 13.441480
Train Epoch: 189 	 Loss: 13.369428
Train Epoch: 190 	 Loss: 13.602112
Train Epoch: 191 	 Loss: 13.246795
Train Epoch: 192 	 Loss: 13.426718
Train Epoch: 193 	 Loss: 13.186770
Train Epoch: 194 	 Loss: 13.180017
Train Epoch: 195 	 Loss: 13.162707
Train Epoch: 196 	 Loss: 13.415537
Train Epoch: 197 	 Loss: 13.731921
Train Epoch: 198 	 Loss: 13.263154
Train Epoch: 199 	 Loss: 13.328232

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.910

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.492

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.822

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.827

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.903

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.918

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.935

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.903

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.897

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.864

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.832

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.855
------
f1 mean across methods is 0.846


 ---------------------------------------- 


For each of the methods
Average F1:  [0.77983563 0.59975501 0.83858699 0.73133764 0.89378359 0.82413745
 0.92423462 0.88317613 0.92593791 0.87418038 0.79133876 0.90725117]
Std F1:  [0.18733069 0.17563482 0.013252   0.20644204 0.03300042 0.1179444
 0.00720635 0.02661058 0.02957608 0.0525779  0.11547543 0.03016186]
Average over repetitions across all methods
Average f1 score:  0.8311296070618213
Std F1:  0.021748641074351188

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 200, 'order_hermite': 100, 'subsampled_rate': 0.3} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33142857e-01 4.43051948e-01 1.84025974e-02 5.18831169e-03
 2.14285714e-04] 

Train Epoch: 0 	 Loss: 20.507759
Train Epoch: 1 	 Loss: 18.484753
Train Epoch: 2 	 Loss: 16.938225
Train Epoch: 3 	 Loss: 14.918806
Train Epoch: 4 	 Loss: 15.635619
Train Epoch: 5 	 Loss: 14.865778
Train Epoch: 6 	 Loss: 15.177988
Train Epoch: 7 	 Loss: 15.950171
Train Epoch: 8 	 Loss: 15.085869
Train Epoch: 9 	 Loss: 14.583050
Train Epoch: 10 	 Loss: 14.982102
Train Epoch: 11 	 Loss: 14.867114
Train Epoch: 12 	 Loss: 14.808883
Train Epoch: 13 	 Loss: 14.335775
Train Epoch: 14 	 Loss: 14.608631
Train Epoch: 15 	 Loss: 14.823000
Train Epoch: 16 	 Loss: 14.626603
Train Epoch: 17 	 Loss: 14.776644
Train Epoch: 18 	 Loss: 14.095229
Train Epoch: 19 	 Loss: 14.029200
Train Epoch: 20 	 Loss: 15.330235
Train Epoch: 21 	 Loss: 14.427507
Train Epoch: 22 	 Loss: 14.643389
Train Epoch: 23 	 Loss: 14.814434
Train Epoch: 24 	 Loss: 14.559431
Train Epoch: 25 	 Loss: 14.341380
Train Epoch: 26 	 Loss: 14.315393
Train Epoch: 27 	 Loss: 14.344837
Train Epoch: 28 	 Loss: 14.190143
Train Epoch: 29 	 Loss: 14.046687
Train Epoch: 30 	 Loss: 14.161726
Train Epoch: 31 	 Loss: 13.935946
Train Epoch: 32 	 Loss: 14.258646
Train Epoch: 33 	 Loss: 14.013357
Train Epoch: 34 	 Loss: 14.609686
Train Epoch: 35 	 Loss: 14.223537
Train Epoch: 36 	 Loss: 14.013241
Train Epoch: 37 	 Loss: 14.236413
Train Epoch: 38 	 Loss: 14.014703
Train Epoch: 39 	 Loss: 14.166252
Train Epoch: 40 	 Loss: 14.119590
Train Epoch: 41 	 Loss: 14.190563
Train Epoch: 42 	 Loss: 14.136228
Train Epoch: 43 	 Loss: 14.294732
Train Epoch: 44 	 Loss: 14.033035
Train Epoch: 45 	 Loss: 14.321990
Train Epoch: 46 	 Loss: 14.014029
Train Epoch: 47 	 Loss: 14.064760
Train Epoch: 48 	 Loss: 14.046772
Train Epoch: 49 	 Loss: 14.251871
Train Epoch: 50 	 Loss: 14.338320
Train Epoch: 51 	 Loss: 13.988565
Train Epoch: 52 	 Loss: 13.990206
Train Epoch: 53 	 Loss: 13.734505
Train Epoch: 54 	 Loss: 14.189192
Train Epoch: 55 	 Loss: 13.940496
Train Epoch: 56 	 Loss: 13.991110
Train Epoch: 57 	 Loss: 14.491084
Train Epoch: 58 	 Loss: 13.893629
Train Epoch: 59 	 Loss: 14.150683
Train Epoch: 60 	 Loss: 14.447964
Train Epoch: 61 	 Loss: 14.435293
Train Epoch: 62 	 Loss: 13.883924
Train Epoch: 63 	 Loss: 14.539528
Train Epoch: 64 	 Loss: 14.121308
Train Epoch: 65 	 Loss: 14.048038
Train Epoch: 66 	 Loss: 13.916086
Train Epoch: 67 	 Loss: 14.034397
Train Epoch: 68 	 Loss: 13.657216
Train Epoch: 69 	 Loss: 13.994547
Train Epoch: 70 	 Loss: 14.240038
Train Epoch: 71 	 Loss: 13.757980
Train Epoch: 72 	 Loss: 14.050101
Train Epoch: 73 	 Loss: 14.169398
Train Epoch: 74 	 Loss: 14.131828
Train Epoch: 75 	 Loss: 14.027692
Train Epoch: 76 	 Loss: 14.124718
Train Epoch: 77 	 Loss: 13.891620
Train Epoch: 78 	 Loss: 14.622565
Train Epoch: 79 	 Loss: 14.070471
Train Epoch: 80 	 Loss: 14.261206
Train Epoch: 81 	 Loss: 14.193610
Train Epoch: 82 	 Loss: 14.373755
Train Epoch: 83 	 Loss: 13.843733
Train Epoch: 84 	 Loss: 13.941296
Train Epoch: 85 	 Loss: 13.968803
Train Epoch: 86 	 Loss: 14.265450
Train Epoch: 87 	 Loss: 13.895776
Train Epoch: 88 	 Loss: 13.951200
Train Epoch: 89 	 Loss: 13.966784
Train Epoch: 90 	 Loss: 14.063364
Train Epoch: 91 	 Loss: 13.916075
Train Epoch: 92 	 Loss: 13.918459
Train Epoch: 93 	 Loss: 13.781787
Train Epoch: 94 	 Loss: 13.993038
Train Epoch: 95 	 Loss: 14.004254
Train Epoch: 96 	 Loss: 13.764531
Train Epoch: 97 	 Loss: 13.882991
Train Epoch: 98 	 Loss: 14.230682
Train Epoch: 99 	 Loss: 13.945318
Train Epoch: 100 	 Loss: 14.046362
Train Epoch: 101 	 Loss: 14.070583
Train Epoch: 102 	 Loss: 14.020211
Train Epoch: 103 	 Loss: 14.650812
Train Epoch: 104 	 Loss: 13.787889
Train Epoch: 105 	 Loss: 14.085884
Train Epoch: 106 	 Loss: 14.220892
Train Epoch: 107 	 Loss: 14.206619
Train Epoch: 108 	 Loss: 14.162106
Train Epoch: 109 	 Loss: 14.001951
Train Epoch: 110 	 Loss: 14.048920
Train Epoch: 111 	 Loss: 13.905933
Train Epoch: 112 	 Loss: 14.073249
Train Epoch: 113 	 Loss: 14.070028
Train Epoch: 114 	 Loss: 14.000979
Train Epoch: 115 	 Loss: 13.836937
Train Epoch: 116 	 Loss: 14.147267
Train Epoch: 117 	 Loss: 14.385674
Train Epoch: 118 	 Loss: 13.702696
Train Epoch: 119 	 Loss: 14.222765
Train Epoch: 120 	 Loss: 13.901056
Train Epoch: 121 	 Loss: 14.048933
Train Epoch: 122 	 Loss: 14.034397
Train Epoch: 123 	 Loss: 13.978477
Train Epoch: 124 	 Loss: 14.524712
Train Epoch: 125 	 Loss: 13.979396
Train Epoch: 126 	 Loss: 14.895656
Train Epoch: 127 	 Loss: 14.106950
Train Epoch: 128 	 Loss: 13.899676
Train Epoch: 129 	 Loss: 14.150030
Train Epoch: 130 	 Loss: 13.960323
Train Epoch: 131 	 Loss: 14.121238
Train Epoch: 132 	 Loss: 14.058357
Train Epoch: 133 	 Loss: 14.200113
Train Epoch: 134 	 Loss: 13.938551
Train Epoch: 135 	 Loss: 14.023823
Train Epoch: 136 	 Loss: 13.873755
Train Epoch: 137 	 Loss: 14.217413
Train Epoch: 138 	 Loss: 13.875670
Train Epoch: 139 	 Loss: 14.043962
Train Epoch: 140 	 Loss: 13.742437
Train Epoch: 141 	 Loss: 13.855581
Train Epoch: 142 	 Loss: 14.114287
Train Epoch: 143 	 Loss: 13.897282
Train Epoch: 144 	 Loss: 14.051874
Train Epoch: 145 	 Loss: 13.943047
Train Epoch: 146 	 Loss: 14.385799
Train Epoch: 147 	 Loss: 13.805878
Train Epoch: 148 	 Loss: 13.883763
Train Epoch: 149 	 Loss: 14.067305
Train Epoch: 150 	 Loss: 13.887953
Train Epoch: 151 	 Loss: 14.164384
Train Epoch: 152 	 Loss: 13.956395
Train Epoch: 153 	 Loss: 13.982101
Train Epoch: 154 	 Loss: 14.133068
Train Epoch: 155 	 Loss: 13.868608
Train Epoch: 156 	 Loss: 13.880394
Train Epoch: 157 	 Loss: 13.709158
Train Epoch: 158 	 Loss: 14.213056
Train Epoch: 159 	 Loss: 13.843884
Train Epoch: 160 	 Loss: 13.872262
Train Epoch: 161 	 Loss: 14.008866
Train Epoch: 162 	 Loss: 14.054422
Train Epoch: 163 	 Loss: 13.887264
Train Epoch: 164 	 Loss: 13.885640
Train Epoch: 165 	 Loss: 13.534071
Train Epoch: 166 	 Loss: 14.332489
Train Epoch: 167 	 Loss: 14.015226
Train Epoch: 168 	 Loss: 14.015659
Train Epoch: 169 	 Loss: 14.306859
Train Epoch: 170 	 Loss: 13.998542
Train Epoch: 171 	 Loss: 13.653355
Train Epoch: 172 	 Loss: 13.349496
Train Epoch: 173 	 Loss: 13.674532
Train Epoch: 174 	 Loss: 13.403381
Train Epoch: 175 	 Loss: 13.039900
Train Epoch: 176 	 Loss: 13.396804
Train Epoch: 177 	 Loss: 13.043750
Train Epoch: 178 	 Loss: 12.889892
Train Epoch: 179 	 Loss: 12.887237
Train Epoch: 180 	 Loss: 13.213790
Train Epoch: 181 	 Loss: 12.876890
Train Epoch: 182 	 Loss: 13.417440
Train Epoch: 183 	 Loss: 13.154772
Train Epoch: 184 	 Loss: 13.136402
Train Epoch: 185 	 Loss: 13.213608
Train Epoch: 186 	 Loss: 13.055802
Train Epoch: 187 	 Loss: 13.059319
Train Epoch: 188 	 Loss: 13.577868
Train Epoch: 189 	 Loss: 13.320138
Train Epoch: 190 	 Loss: 12.977013
Train Epoch: 191 	 Loss: 12.626358
Train Epoch: 192 	 Loss: 13.149543
Train Epoch: 193 	 Loss: 13.095908
Train Epoch: 194 	 Loss: 13.132891
Train Epoch: 195 	 Loss: 13.014607
Train Epoch: 196 	 Loss: 12.990217
Train Epoch: 197 	 Loss: 12.927399
Train Epoch: 198 	 Loss: 12.948271
Train Epoch: 199 	 Loss: 13.010181

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.869

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.535

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.917

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.868

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.932

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.660

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.919

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.932

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.908

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.914

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.851

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.931
------
f1 mean across methods is 0.853


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.34577922e-01 4.41084416e-01 1.90324675e-02 5.05844156e-03
 2.46753247e-04] 

Train Epoch: 0 	 Loss: 19.735664
Train Epoch: 1 	 Loss: 18.150330
Train Epoch: 2 	 Loss: 25.137770
Train Epoch: 3 	 Loss: 16.819305
Train Epoch: 4 	 Loss: 14.710007
Train Epoch: 5 	 Loss: 14.589354
Train Epoch: 6 	 Loss: 13.799108
Train Epoch: 7 	 Loss: 14.155449
Train Epoch: 8 	 Loss: 14.395424
Train Epoch: 9 	 Loss: 14.532301
Train Epoch: 10 	 Loss: 14.426280
Train Epoch: 11 	 Loss: 13.497541
Train Epoch: 12 	 Loss: 13.630243
Train Epoch: 13 	 Loss: 14.531803
Train Epoch: 14 	 Loss: 13.727175
Train Epoch: 15 	 Loss: 13.681722
Train Epoch: 16 	 Loss: 13.617529
Train Epoch: 17 	 Loss: 13.427762
Train Epoch: 18 	 Loss: 13.357928
Train Epoch: 19 	 Loss: 14.128045
Train Epoch: 20 	 Loss: 13.548584
Train Epoch: 21 	 Loss: 14.302647
Train Epoch: 22 	 Loss: 13.723618
Train Epoch: 23 	 Loss: 13.237418
Train Epoch: 24 	 Loss: 13.568548
Train Epoch: 25 	 Loss: 13.092426
Train Epoch: 26 	 Loss: 13.032246
Train Epoch: 27 	 Loss: 13.237183
Train Epoch: 28 	 Loss: 13.064079
Train Epoch: 29 	 Loss: 13.093624
Train Epoch: 30 	 Loss: 13.507108
Train Epoch: 31 	 Loss: 13.554644
Train Epoch: 32 	 Loss: 13.737972
Train Epoch: 33 	 Loss: 13.100462
Train Epoch: 34 	 Loss: 13.052418
Train Epoch: 35 	 Loss: 13.026937
Train Epoch: 36 	 Loss: 13.006819
Train Epoch: 37 	 Loss: 13.019718
Train Epoch: 38 	 Loss: 12.904106
Train Epoch: 39 	 Loss: 12.941821
Train Epoch: 40 	 Loss: 13.167784
Train Epoch: 41 	 Loss: 13.535316
Train Epoch: 42 	 Loss: 13.129383
Train Epoch: 43 	 Loss: 13.124128
Train Epoch: 44 	 Loss: 13.791226
Train Epoch: 45 	 Loss: 14.269657
Train Epoch: 46 	 Loss: 14.066359
Train Epoch: 47 	 Loss: 13.191980
Train Epoch: 48 	 Loss: 13.970705
Train Epoch: 49 	 Loss: 13.463108
Train Epoch: 50 	 Loss: 13.341504
Train Epoch: 51 	 Loss: 13.087698
Train Epoch: 52 	 Loss: 13.557215
Train Epoch: 53 	 Loss: 12.934971
Train Epoch: 54 	 Loss: 13.406782
Train Epoch: 55 	 Loss: 13.134251
Train Epoch: 56 	 Loss: 12.991634
Train Epoch: 57 	 Loss: 13.046936
Train Epoch: 58 	 Loss: 12.885078
Train Epoch: 59 	 Loss: 13.096080
Train Epoch: 60 	 Loss: 12.855917
Train Epoch: 61 	 Loss: 13.323412
Train Epoch: 62 	 Loss: 12.963229
Train Epoch: 63 	 Loss: 13.262968
Train Epoch: 64 	 Loss: 13.481682
Train Epoch: 65 	 Loss: 12.955995
Train Epoch: 66 	 Loss: 12.960577
Train Epoch: 67 	 Loss: 12.931297
Train Epoch: 68 	 Loss: 12.932928
Train Epoch: 69 	 Loss: 12.896622
Train Epoch: 70 	 Loss: 13.002131
Train Epoch: 71 	 Loss: 12.910219
Train Epoch: 72 	 Loss: 13.011402
Train Epoch: 73 	 Loss: 13.125236
Train Epoch: 74 	 Loss: 12.921903
Train Epoch: 75 	 Loss: 12.933455
Train Epoch: 76 	 Loss: 13.054955
Train Epoch: 77 	 Loss: 13.119971
Train Epoch: 78 	 Loss: 13.182912
Train Epoch: 79 	 Loss: 13.193624
Train Epoch: 80 	 Loss: 13.096430
Train Epoch: 81 	 Loss: 12.885765
Train Epoch: 82 	 Loss: 13.015530
Train Epoch: 83 	 Loss: 12.907076
Train Epoch: 84 	 Loss: 12.883518
Train Epoch: 85 	 Loss: 13.141566
Train Epoch: 86 	 Loss: 12.993457
Train Epoch: 87 	 Loss: 12.933100
Train Epoch: 88 	 Loss: 12.959918
Train Epoch: 89 	 Loss: 12.911000
Train Epoch: 90 	 Loss: 13.267404
Train Epoch: 91 	 Loss: 13.056486
Train Epoch: 92 	 Loss: 13.644909
Train Epoch: 93 	 Loss: 12.991518
Train Epoch: 94 	 Loss: 13.279825
Train Epoch: 95 	 Loss: 13.129589
Train Epoch: 96 	 Loss: 12.914091
Train Epoch: 97 	 Loss: 12.904249
Train Epoch: 98 	 Loss: 13.032025
Train Epoch: 99 	 Loss: 12.853786
Train Epoch: 100 	 Loss: 13.026284
Train Epoch: 101 	 Loss: 13.262465
Train Epoch: 102 	 Loss: 13.318477
Train Epoch: 103 	 Loss: 13.858182
Train Epoch: 104 	 Loss: 13.230885
Train Epoch: 105 	 Loss: 13.613031
Train Epoch: 106 	 Loss: 13.456918
Train Epoch: 107 	 Loss: 13.339516
Train Epoch: 108 	 Loss: 13.331247
Train Epoch: 109 	 Loss: 13.259642
Train Epoch: 110 	 Loss: 14.558458
Train Epoch: 111 	 Loss: 13.504551
Train Epoch: 112 	 Loss: 13.385032
Train Epoch: 113 	 Loss: 13.802014
Train Epoch: 114 	 Loss: 13.880896
Train Epoch: 115 	 Loss: 13.118049
Train Epoch: 116 	 Loss: 13.022224
Train Epoch: 117 	 Loss: 12.922691
Train Epoch: 118 	 Loss: 12.829793
Train Epoch: 119 	 Loss: 14.078379
Train Epoch: 120 	 Loss: 13.041754
Train Epoch: 121 	 Loss: 12.827128
Train Epoch: 122 	 Loss: 12.819015
Train Epoch: 123 	 Loss: 13.362308
Train Epoch: 124 	 Loss: 13.878549
Train Epoch: 125 	 Loss: 12.722301
Train Epoch: 126 	 Loss: 13.739346
Train Epoch: 127 	 Loss: 13.126657
Train Epoch: 128 	 Loss: 12.595109
Train Epoch: 129 	 Loss: 12.963219
Train Epoch: 130 	 Loss: 13.293181
Train Epoch: 131 	 Loss: 13.318061
Train Epoch: 132 	 Loss: 13.182953
Train Epoch: 133 	 Loss: 12.777762
Train Epoch: 134 	 Loss: 13.311089
Train Epoch: 135 	 Loss: 12.981221
Train Epoch: 136 	 Loss: 12.970171
Train Epoch: 137 	 Loss: 13.455246
Train Epoch: 138 	 Loss: 12.883921
Train Epoch: 139 	 Loss: 12.979624
Train Epoch: 140 	 Loss: 13.171325
Train Epoch: 141 	 Loss: 12.954246
Train Epoch: 142 	 Loss: 12.848963
Train Epoch: 143 	 Loss: 12.879687
Train Epoch: 144 	 Loss: 12.919168
Train Epoch: 145 	 Loss: 12.715677
Train Epoch: 146 	 Loss: 12.938182
Train Epoch: 147 	 Loss: 12.712671
Train Epoch: 148 	 Loss: 12.910936
Train Epoch: 149 	 Loss: 12.857757
Train Epoch: 150 	 Loss: 12.788704
Train Epoch: 151 	 Loss: 12.784068
Train Epoch: 152 	 Loss: 12.730175
Train Epoch: 153 	 Loss: 12.762697
Train Epoch: 154 	 Loss: 12.856362
Train Epoch: 155 	 Loss: 12.837744
Train Epoch: 156 	 Loss: 12.804539
Train Epoch: 157 	 Loss: 12.989867
Train Epoch: 158 	 Loss: 13.017702
Train Epoch: 159 	 Loss: 13.116728
Train Epoch: 160 	 Loss: 12.698973
Train Epoch: 161 	 Loss: 12.692423
Train Epoch: 162 	 Loss: 12.713017
Train Epoch: 163 	 Loss: 12.868828
Train Epoch: 164 	 Loss: 13.119578
Train Epoch: 165 	 Loss: 12.655757
Train Epoch: 166 	 Loss: 12.783232
Train Epoch: 167 	 Loss: 13.000166
Train Epoch: 168 	 Loss: 12.887954
Train Epoch: 169 	 Loss: 12.920359
Train Epoch: 170 	 Loss: 12.821467
Train Epoch: 171 	 Loss: 12.693422
Train Epoch: 172 	 Loss: 13.143009
Train Epoch: 173 	 Loss: 13.594562
Train Epoch: 174 	 Loss: 12.775381
Train Epoch: 175 	 Loss: 12.756093
Train Epoch: 176 	 Loss: 12.877362
Train Epoch: 177 	 Loss: 12.955221
Train Epoch: 178 	 Loss: 12.614590
Train Epoch: 179 	 Loss: 12.757010
Train Epoch: 180 	 Loss: 12.740491
Train Epoch: 181 	 Loss: 12.922089
Train Epoch: 182 	 Loss: 12.825819
Train Epoch: 183 	 Loss: 12.986085
Train Epoch: 184 	 Loss: 12.747337
Train Epoch: 185 	 Loss: 12.677998
Train Epoch: 186 	 Loss: 13.117090
Train Epoch: 187 	 Loss: 12.675200
Train Epoch: 188 	 Loss: 12.857182
Train Epoch: 189 	 Loss: 13.061799
Train Epoch: 190 	 Loss: 12.809134
Train Epoch: 191 	 Loss: 12.756159
Train Epoch: 192 	 Loss: 12.728741
Train Epoch: 193 	 Loss: 13.255595
Train Epoch: 194 	 Loss: 12.768219
Train Epoch: 195 	 Loss: 12.617032
Train Epoch: 196 	 Loss: 12.952869
Train Epoch: 197 	 Loss: 13.043824
Train Epoch: 198 	 Loss: 12.695609
Train Epoch: 199 	 Loss: 12.760282

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.882

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.533

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.928

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.876

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.903

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.923

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.934

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.904

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.911

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.897

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.887

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.916
------
f1 mean across methods is 0.874


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.32896104e-01 4.43051948e-01 1.85844156e-02 5.24675325e-03
 2.20779221e-04] 

Train Epoch: 0 	 Loss: 20.578266
Train Epoch: 1 	 Loss: 19.279501
Train Epoch: 2 	 Loss: 17.499725
Train Epoch: 3 	 Loss: 15.745131
Train Epoch: 4 	 Loss: 15.517332
Train Epoch: 5 	 Loss: 14.662672
Train Epoch: 6 	 Loss: 14.934088
Train Epoch: 7 	 Loss: 14.762060
Train Epoch: 8 	 Loss: 14.483593
Train Epoch: 9 	 Loss: 14.714225
Train Epoch: 10 	 Loss: 15.943753
Train Epoch: 11 	 Loss: 14.756603
Train Epoch: 12 	 Loss: 14.402853
Train Epoch: 13 	 Loss: 14.454334
Train Epoch: 14 	 Loss: 14.191427
Train Epoch: 15 	 Loss: 14.179205
Train Epoch: 16 	 Loss: 14.416315
Train Epoch: 17 	 Loss: 14.498332
Train Epoch: 18 	 Loss: 13.815159
Train Epoch: 19 	 Loss: 13.925951
Train Epoch: 20 	 Loss: 14.265785
Train Epoch: 21 	 Loss: 13.980083
Train Epoch: 22 	 Loss: 13.849648
Train Epoch: 23 	 Loss: 14.071624
Train Epoch: 24 	 Loss: 14.024957
Train Epoch: 25 	 Loss: 14.612908
Train Epoch: 26 	 Loss: 14.394461
Train Epoch: 27 	 Loss: 14.615995
Train Epoch: 28 	 Loss: 14.297890
Train Epoch: 29 	 Loss: 14.073934
Train Epoch: 30 	 Loss: 14.277666
Train Epoch: 31 	 Loss: 14.123228
Train Epoch: 32 	 Loss: 14.437141
Train Epoch: 33 	 Loss: 14.030947
Train Epoch: 34 	 Loss: 14.007584
Train Epoch: 35 	 Loss: 14.393542
Train Epoch: 36 	 Loss: 14.103671
Train Epoch: 37 	 Loss: 13.954223
Train Epoch: 38 	 Loss: 14.181667
Train Epoch: 39 	 Loss: 14.314631
Train Epoch: 40 	 Loss: 14.022545
Train Epoch: 41 	 Loss: 14.244339
Train Epoch: 42 	 Loss: 14.547176
Train Epoch: 43 	 Loss: 14.062870
Train Epoch: 44 	 Loss: 14.237924
Train Epoch: 45 	 Loss: 14.044855
Train Epoch: 46 	 Loss: 13.968671
Train Epoch: 47 	 Loss: 14.667725
Train Epoch: 48 	 Loss: 13.948483
Train Epoch: 49 	 Loss: 13.945967
Train Epoch: 50 	 Loss: 13.972836
Train Epoch: 51 	 Loss: 14.023971
Train Epoch: 52 	 Loss: 13.941730
Train Epoch: 53 	 Loss: 14.267046
Train Epoch: 54 	 Loss: 14.105182
Train Epoch: 55 	 Loss: 13.945888
Train Epoch: 56 	 Loss: 14.075645
Train Epoch: 57 	 Loss: 13.952512
Train Epoch: 58 	 Loss: 14.136466
Train Epoch: 59 	 Loss: 14.176770
Train Epoch: 60 	 Loss: 14.416883
Train Epoch: 61 	 Loss: 15.222341
Train Epoch: 62 	 Loss: 14.020923
Train Epoch: 63 	 Loss: 13.784616
Train Epoch: 64 	 Loss: 13.653826
Train Epoch: 65 	 Loss: 14.011861
Train Epoch: 66 	 Loss: 14.211748
Train Epoch: 67 	 Loss: 14.030710
Train Epoch: 68 	 Loss: 14.024395
Train Epoch: 69 	 Loss: 13.944643
Train Epoch: 70 	 Loss: 13.906294
Train Epoch: 71 	 Loss: 14.151249
Train Epoch: 72 	 Loss: 14.296497
Train Epoch: 73 	 Loss: 14.054100
Train Epoch: 74 	 Loss: 13.996778
Train Epoch: 75 	 Loss: 13.864882
Train Epoch: 76 	 Loss: 14.137977
Train Epoch: 77 	 Loss: 13.965918
Train Epoch: 78 	 Loss: 14.280405
Train Epoch: 79 	 Loss: 14.318575
Train Epoch: 80 	 Loss: 13.911139
Train Epoch: 81 	 Loss: 13.971478
Train Epoch: 82 	 Loss: 13.867769
Train Epoch: 83 	 Loss: 14.039988
Train Epoch: 84 	 Loss: 14.006018
Train Epoch: 85 	 Loss: 13.910235
Train Epoch: 86 	 Loss: 13.720984
Train Epoch: 87 	 Loss: 14.233448
Train Epoch: 88 	 Loss: 13.772362
Train Epoch: 89 	 Loss: 13.778128
Train Epoch: 90 	 Loss: 13.789639
Train Epoch: 91 	 Loss: 13.776481
Train Epoch: 92 	 Loss: 13.808828
Train Epoch: 93 	 Loss: 13.921385
Train Epoch: 94 	 Loss: 13.952930
Train Epoch: 95 	 Loss: 14.171740
Train Epoch: 96 	 Loss: 13.862162
Train Epoch: 97 	 Loss: 13.896088
Train Epoch: 98 	 Loss: 13.783510
Train Epoch: 99 	 Loss: 13.902412
Train Epoch: 100 	 Loss: 13.935317
Train Epoch: 101 	 Loss: 13.666887
Train Epoch: 102 	 Loss: 14.901609
Train Epoch: 103 	 Loss: 14.506814
Train Epoch: 104 	 Loss: 14.010250
Train Epoch: 105 	 Loss: 13.875229
Train Epoch: 106 	 Loss: 14.729938
Train Epoch: 107 	 Loss: 14.039511
Train Epoch: 108 	 Loss: 13.969374
Train Epoch: 109 	 Loss: 13.986312
Train Epoch: 110 	 Loss: 13.984973
Train Epoch: 111 	 Loss: 13.939598
Train Epoch: 112 	 Loss: 14.625357
Train Epoch: 113 	 Loss: 13.451426
Train Epoch: 114 	 Loss: 14.038408
Train Epoch: 115 	 Loss: 14.084286
Train Epoch: 116 	 Loss: 13.996114
Train Epoch: 117 	 Loss: 14.300394
Train Epoch: 118 	 Loss: 14.061009
Train Epoch: 119 	 Loss: 13.861574
Train Epoch: 120 	 Loss: 13.786732
Train Epoch: 121 	 Loss: 13.957220
Train Epoch: 122 	 Loss: 14.345427
Train Epoch: 123 	 Loss: 13.712309
Train Epoch: 124 	 Loss: 13.820822
Train Epoch: 125 	 Loss: 14.102214
Train Epoch: 126 	 Loss: 13.983274
Train Epoch: 127 	 Loss: 14.279014
Train Epoch: 128 	 Loss: 13.860010
Train Epoch: 129 	 Loss: 13.975587
Train Epoch: 130 	 Loss: 14.016582
Train Epoch: 131 	 Loss: 13.817389
Train Epoch: 132 	 Loss: 13.890659
Train Epoch: 133 	 Loss: 13.797974
Train Epoch: 134 	 Loss: 13.974565
Train Epoch: 135 	 Loss: 13.763498
Train Epoch: 136 	 Loss: 13.845060
Train Epoch: 137 	 Loss: 13.990059
Train Epoch: 138 	 Loss: 13.881598
Train Epoch: 139 	 Loss: 13.707359
Train Epoch: 140 	 Loss: 14.152955
Train Epoch: 141 	 Loss: 14.481544
Train Epoch: 142 	 Loss: 13.756216
Train Epoch: 143 	 Loss: 13.668306
Train Epoch: 144 	 Loss: 13.728386
Train Epoch: 145 	 Loss: 13.952650
Train Epoch: 146 	 Loss: 13.948996
Train Epoch: 147 	 Loss: 14.088568
Train Epoch: 148 	 Loss: 14.156712
Train Epoch: 149 	 Loss: 13.965971
Train Epoch: 150 	 Loss: 13.751786
Train Epoch: 151 	 Loss: 13.809214
Train Epoch: 152 	 Loss: 13.968688
Train Epoch: 153 	 Loss: 14.058979
Train Epoch: 154 	 Loss: 13.836711
Train Epoch: 155 	 Loss: 14.621721
Train Epoch: 156 	 Loss: 13.975516
Train Epoch: 157 	 Loss: 14.371630
Train Epoch: 158 	 Loss: 13.726013
Train Epoch: 159 	 Loss: 13.773563
Train Epoch: 160 	 Loss: 14.076272
Train Epoch: 161 	 Loss: 14.148812
Train Epoch: 162 	 Loss: 13.813918
Train Epoch: 163 	 Loss: 14.373385
Train Epoch: 164 	 Loss: 14.029419
Train Epoch: 165 	 Loss: 14.030525
Train Epoch: 166 	 Loss: 13.816540
Train Epoch: 167 	 Loss: 13.971676
Train Epoch: 168 	 Loss: 13.839289
Train Epoch: 169 	 Loss: 13.779819
Train Epoch: 170 	 Loss: 14.048401
Train Epoch: 171 	 Loss: 13.955142
Train Epoch: 172 	 Loss: 14.014181
Train Epoch: 173 	 Loss: 13.905479
Train Epoch: 174 	 Loss: 13.992012
Train Epoch: 175 	 Loss: 14.012827
Train Epoch: 176 	 Loss: 14.537919
Train Epoch: 177 	 Loss: 13.815104
Train Epoch: 178 	 Loss: 13.878582
Train Epoch: 179 	 Loss: 13.944046
Train Epoch: 180 	 Loss: 14.205747
Train Epoch: 181 	 Loss: 13.817019
Train Epoch: 182 	 Loss: 13.710149
Train Epoch: 183 	 Loss: 13.899758
Train Epoch: 184 	 Loss: 14.058124
Train Epoch: 185 	 Loss: 14.158688
Train Epoch: 186 	 Loss: 13.817142
Train Epoch: 187 	 Loss: 14.020174
Train Epoch: 188 	 Loss: 13.950052
Train Epoch: 189 	 Loss: 13.867403
Train Epoch: 190 	 Loss: 13.900784
Train Epoch: 191 	 Loss: 13.997257
Train Epoch: 192 	 Loss: 13.788044
Train Epoch: 193 	 Loss: 13.823165
Train Epoch: 194 	 Loss: 13.806505
Train Epoch: 195 	 Loss: 14.476736
Train Epoch: 196 	 Loss: 13.808527
Train Epoch: 197 	 Loss: 13.810074
Train Epoch: 198 	 Loss: 14.119020
Train Epoch: 199 	 Loss: 13.841723

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.569

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.210

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.858

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.756

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.929

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.468

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.905

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.937

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.935

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.388

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.780

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.945
------
f1 mean across methods is 0.723


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33707792e-01 4.42259740e-01 1.85194805e-02 5.28571429e-03
 2.27272727e-04] 

Train Epoch: 0 	 Loss: 18.234545
Train Epoch: 1 	 Loss: 18.650553
Train Epoch: 2 	 Loss: 14.537226
Train Epoch: 3 	 Loss: 13.004017
Train Epoch: 4 	 Loss: 13.500891
Train Epoch: 5 	 Loss: 12.955412
Train Epoch: 6 	 Loss: 13.384580
Train Epoch: 7 	 Loss: 13.209884
Train Epoch: 8 	 Loss: 12.954762
Train Epoch: 9 	 Loss: 12.793179
Train Epoch: 10 	 Loss: 12.958421
Train Epoch: 11 	 Loss: 12.530553
Train Epoch: 12 	 Loss: 13.023188
Train Epoch: 13 	 Loss: 12.928905
Train Epoch: 14 	 Loss: 12.904327
Train Epoch: 15 	 Loss: 13.611513
Train Epoch: 16 	 Loss: 12.416544
Train Epoch: 17 	 Loss: 12.498268
Train Epoch: 18 	 Loss: 13.020226
Train Epoch: 19 	 Loss: 12.841825
Train Epoch: 20 	 Loss: 12.245964
Train Epoch: 21 	 Loss: 12.178180
Train Epoch: 22 	 Loss: 12.565777
Train Epoch: 23 	 Loss: 11.939644
Train Epoch: 24 	 Loss: 11.980310
Train Epoch: 25 	 Loss: 11.376188
Train Epoch: 26 	 Loss: 11.953701
Train Epoch: 27 	 Loss: 12.299664
Train Epoch: 28 	 Loss: 11.512813
Train Epoch: 29 	 Loss: 11.667096
Train Epoch: 30 	 Loss: 11.724510
Train Epoch: 31 	 Loss: 11.424048
Train Epoch: 32 	 Loss: 11.796643
Train Epoch: 33 	 Loss: 11.575393
Train Epoch: 34 	 Loss: 11.905626
Train Epoch: 35 	 Loss: 12.032253
Train Epoch: 36 	 Loss: 11.289482
Train Epoch: 37 	 Loss: 11.514833
Train Epoch: 38 	 Loss: 11.872030
Train Epoch: 39 	 Loss: 11.282553
Train Epoch: 40 	 Loss: 11.345083
Train Epoch: 41 	 Loss: 11.508694
Train Epoch: 42 	 Loss: 13.158865
Train Epoch: 43 	 Loss: 11.288963
Train Epoch: 44 	 Loss: 11.452570
Train Epoch: 45 	 Loss: 11.545163
Train Epoch: 46 	 Loss: 11.268883
Train Epoch: 47 	 Loss: 11.557859
Train Epoch: 48 	 Loss: 11.648699
Train Epoch: 49 	 Loss: 11.536196
Train Epoch: 50 	 Loss: 11.781906
Train Epoch: 51 	 Loss: 11.494812
Train Epoch: 52 	 Loss: 11.368266
Train Epoch: 53 	 Loss: 11.981750
Train Epoch: 54 	 Loss: 11.425323
Train Epoch: 55 	 Loss: 12.121381
Train Epoch: 56 	 Loss: 11.397579
Train Epoch: 57 	 Loss: 11.575896
Train Epoch: 58 	 Loss: 11.424646
Train Epoch: 59 	 Loss: 11.688932
Train Epoch: 60 	 Loss: 11.574446
Train Epoch: 61 	 Loss: 11.736324
Train Epoch: 62 	 Loss: 11.325942
Train Epoch: 63 	 Loss: 11.393311
Train Epoch: 64 	 Loss: 11.346411
Train Epoch: 65 	 Loss: 11.783178
Train Epoch: 66 	 Loss: 11.698048
Train Epoch: 67 	 Loss: 11.715548
Train Epoch: 68 	 Loss: 12.091202
Train Epoch: 69 	 Loss: 11.706656
Train Epoch: 70 	 Loss: 11.278410
Train Epoch: 71 	 Loss: 11.971750
Train Epoch: 72 	 Loss: 11.648420
Train Epoch: 73 	 Loss: 11.320516
Train Epoch: 74 	 Loss: 11.409266
Train Epoch: 75 	 Loss: 11.464905
Train Epoch: 76 	 Loss: 11.282894
Train Epoch: 77 	 Loss: 11.425642
Train Epoch: 78 	 Loss: 11.570362
Train Epoch: 79 	 Loss: 11.386793
Train Epoch: 80 	 Loss: 12.121202
Train Epoch: 81 	 Loss: 12.043671
Train Epoch: 82 	 Loss: 11.398043
Train Epoch: 83 	 Loss: 11.516041
Train Epoch: 84 	 Loss: 11.518214
Train Epoch: 85 	 Loss: 11.545077
Train Epoch: 86 	 Loss: 11.416073
Train Epoch: 87 	 Loss: 11.271276
Train Epoch: 88 	 Loss: 11.429650
Train Epoch: 89 	 Loss: 11.761801
Train Epoch: 90 	 Loss: 11.476793
Train Epoch: 91 	 Loss: 11.313851
Train Epoch: 92 	 Loss: 11.251440
Train Epoch: 93 	 Loss: 11.502556
Train Epoch: 94 	 Loss: 11.632740
Train Epoch: 95 	 Loss: 11.645491
Train Epoch: 96 	 Loss: 11.243946
Train Epoch: 97 	 Loss: 11.407148
Train Epoch: 98 	 Loss: 11.223944
Train Epoch: 99 	 Loss: 11.299843
Train Epoch: 100 	 Loss: 11.359015
Train Epoch: 101 	 Loss: 11.416668
Train Epoch: 102 	 Loss: 11.382234
Train Epoch: 103 	 Loss: 11.269985
Train Epoch: 104 	 Loss: 11.402049
Train Epoch: 105 	 Loss: 11.550957
Train Epoch: 106 	 Loss: 11.376095
Train Epoch: 107 	 Loss: 11.450956
Train Epoch: 108 	 Loss: 11.395441
Train Epoch: 109 	 Loss: 11.609165
Train Epoch: 110 	 Loss: 11.417168
Train Epoch: 111 	 Loss: 11.723946
Train Epoch: 112 	 Loss: 11.196780
Train Epoch: 113 	 Loss: 11.297253
Train Epoch: 114 	 Loss: 11.549458
Train Epoch: 115 	 Loss: 11.214910
Train Epoch: 116 	 Loss: 11.346935
Train Epoch: 117 	 Loss: 11.259709
Train Epoch: 118 	 Loss: 11.264227
Train Epoch: 119 	 Loss: 11.548682
Train Epoch: 120 	 Loss: 11.283024
Train Epoch: 121 	 Loss: 11.320123
Train Epoch: 122 	 Loss: 11.211782
Train Epoch: 123 	 Loss: 11.486587
Train Epoch: 124 	 Loss: 11.390289
Train Epoch: 125 	 Loss: 11.964609
Train Epoch: 126 	 Loss: 11.290888
Train Epoch: 127 	 Loss: 11.353572
Train Epoch: 128 	 Loss: 11.231694
Train Epoch: 129 	 Loss: 11.311646
Train Epoch: 130 	 Loss: 11.312943
Train Epoch: 131 	 Loss: 11.467593
Train Epoch: 132 	 Loss: 11.226091
Train Epoch: 133 	 Loss: 11.273592
Train Epoch: 134 	 Loss: 11.303312
Train Epoch: 135 	 Loss: 11.144140
Train Epoch: 136 	 Loss: 11.554139
Train Epoch: 137 	 Loss: 11.359835
Train Epoch: 138 	 Loss: 11.373698
Train Epoch: 139 	 Loss: 11.464947
Train Epoch: 140 	 Loss: 11.690763
Train Epoch: 141 	 Loss: 11.683231
Train Epoch: 142 	 Loss: 11.271681
Train Epoch: 143 	 Loss: 11.351358
Train Epoch: 144 	 Loss: 11.290716
Train Epoch: 145 	 Loss: 11.629250
Train Epoch: 146 	 Loss: 11.276136
Train Epoch: 147 	 Loss: 11.175828
Train Epoch: 148 	 Loss: 11.305015
Train Epoch: 149 	 Loss: 11.908166
Train Epoch: 150 	 Loss: 11.276987
Train Epoch: 151 	 Loss: 11.373903
Train Epoch: 152 	 Loss: 11.605585
Train Epoch: 153 	 Loss: 11.177408
Train Epoch: 154 	 Loss: 11.438583
Train Epoch: 155 	 Loss: 11.313159
Train Epoch: 156 	 Loss: 11.230621
Train Epoch: 157 	 Loss: 11.444018
Train Epoch: 158 	 Loss: 11.314579
Train Epoch: 159 	 Loss: 11.397602
Train Epoch: 160 	 Loss: 11.238972
Train Epoch: 161 	 Loss: 11.245088
Train Epoch: 162 	 Loss: 11.445688
Train Epoch: 163 	 Loss: 11.363067
Train Epoch: 164 	 Loss: 11.250198
Train Epoch: 165 	 Loss: 11.184904
Train Epoch: 166 	 Loss: 11.381907
Train Epoch: 167 	 Loss: 11.421217
Train Epoch: 168 	 Loss: 11.231630
Train Epoch: 169 	 Loss: 11.351604
Train Epoch: 170 	 Loss: 11.396786
Train Epoch: 171 	 Loss: 11.261429
Train Epoch: 172 	 Loss: 11.371478
Train Epoch: 173 	 Loss: 11.309137
Train Epoch: 174 	 Loss: 11.345734
Train Epoch: 175 	 Loss: 11.423595
Train Epoch: 176 	 Loss: 11.539772
Train Epoch: 177 	 Loss: 11.399897
Train Epoch: 178 	 Loss: 11.360507
Train Epoch: 179 	 Loss: 11.171254
Train Epoch: 180 	 Loss: 11.546230
Train Epoch: 181 	 Loss: 11.519346
Train Epoch: 182 	 Loss: 11.172832
Train Epoch: 183 	 Loss: 11.168941
Train Epoch: 184 	 Loss: 11.364405
Train Epoch: 185 	 Loss: 11.932080
Train Epoch: 186 	 Loss: 11.182388
Train Epoch: 187 	 Loss: 11.247349
Train Epoch: 188 	 Loss: 11.237782
Train Epoch: 189 	 Loss: 11.274968
Train Epoch: 190 	 Loss: 11.171828
Train Epoch: 191 	 Loss: 11.232206
Train Epoch: 192 	 Loss: 11.332511
Train Epoch: 193 	 Loss: 11.316716
Train Epoch: 194 	 Loss: 11.537724
Train Epoch: 195 	 Loss: 11.265101
Train Epoch: 196 	 Loss: 11.396106
Train Epoch: 197 	 Loss: 11.174943
Train Epoch: 198 	 Loss: 11.373723
Train Epoch: 199 	 Loss: 11.206333

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.878

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.595

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.912

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.867

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.889

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.911

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.933

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.382

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.829

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.495

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.859

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.839
------
f1 mean across methods is 0.782


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33701299e-01 4.42097403e-01 1.89155844e-02 5.08441558e-03
 2.01298701e-04] 

Train Epoch: 0 	 Loss: 21.786774
Train Epoch: 1 	 Loss: 18.627422
Train Epoch: 2 	 Loss: 16.743744
Train Epoch: 3 	 Loss: 16.316515
Train Epoch: 4 	 Loss: 15.396826
Train Epoch: 5 	 Loss: 14.984030
Train Epoch: 6 	 Loss: 15.407667
Train Epoch: 7 	 Loss: 14.991727
Train Epoch: 8 	 Loss: 14.776114
Train Epoch: 9 	 Loss: 14.133210
Train Epoch: 10 	 Loss: 14.298326
Train Epoch: 11 	 Loss: 14.039670
Train Epoch: 12 	 Loss: 14.731670
Train Epoch: 13 	 Loss: 14.559414
Train Epoch: 14 	 Loss: 14.419649
Train Epoch: 15 	 Loss: 13.868860
Train Epoch: 16 	 Loss: 14.072586
Train Epoch: 17 	 Loss: 14.280092
Train Epoch: 18 	 Loss: 13.556355
Train Epoch: 19 	 Loss: 14.612060
Train Epoch: 20 	 Loss: 15.969168
Train Epoch: 21 	 Loss: 13.794125
Train Epoch: 22 	 Loss: 13.653793
Train Epoch: 23 	 Loss: 13.766358
Train Epoch: 24 	 Loss: 14.289174
Train Epoch: 25 	 Loss: 13.802540
Train Epoch: 26 	 Loss: 13.509522
Train Epoch: 27 	 Loss: 13.562761
Train Epoch: 28 	 Loss: 14.479782
Train Epoch: 29 	 Loss: 13.647228
Train Epoch: 30 	 Loss: 13.665039
Train Epoch: 31 	 Loss: 13.809931
Train Epoch: 32 	 Loss: 13.522644
Train Epoch: 33 	 Loss: 13.651927
Train Epoch: 34 	 Loss: 13.890202
Train Epoch: 35 	 Loss: 13.735720
Train Epoch: 36 	 Loss: 13.851686
Train Epoch: 37 	 Loss: 15.153040
Train Epoch: 38 	 Loss: 13.562780
Train Epoch: 39 	 Loss: 13.659681
Train Epoch: 40 	 Loss: 13.955044
Train Epoch: 41 	 Loss: 13.534006
Train Epoch: 42 	 Loss: 13.315561
Train Epoch: 43 	 Loss: 13.716532
Train Epoch: 44 	 Loss: 13.201325
Train Epoch: 45 	 Loss: 13.475816
Train Epoch: 46 	 Loss: 13.622612
Train Epoch: 47 	 Loss: 13.371662
Train Epoch: 48 	 Loss: 13.258067
Train Epoch: 49 	 Loss: 13.250248
Train Epoch: 50 	 Loss: 13.823414
Train Epoch: 51 	 Loss: 13.789492
Train Epoch: 52 	 Loss: 13.421219
Train Epoch: 53 	 Loss: 13.405495
Train Epoch: 54 	 Loss: 13.157404
Train Epoch: 55 	 Loss: 14.091356
Train Epoch: 56 	 Loss: 13.468897
Train Epoch: 57 	 Loss: 13.745411
Train Epoch: 58 	 Loss: 13.081848
Train Epoch: 59 	 Loss: 13.257832
Train Epoch: 60 	 Loss: 13.207131
Train Epoch: 61 	 Loss: 13.227884
Train Epoch: 62 	 Loss: 13.361306
Train Epoch: 63 	 Loss: 13.233591
Train Epoch: 64 	 Loss: 13.728545
Train Epoch: 65 	 Loss: 13.369479
Train Epoch: 66 	 Loss: 13.251202
Train Epoch: 67 	 Loss: 13.717654
Train Epoch: 68 	 Loss: 13.150476
Train Epoch: 69 	 Loss: 13.069717
Train Epoch: 70 	 Loss: 13.558764
Train Epoch: 71 	 Loss: 13.107660
Train Epoch: 72 	 Loss: 13.261225
Train Epoch: 73 	 Loss: 13.773571
Train Epoch: 74 	 Loss: 13.555055
Train Epoch: 75 	 Loss: 13.318727
Train Epoch: 76 	 Loss: 13.331489
Train Epoch: 77 	 Loss: 13.202050
Train Epoch: 78 	 Loss: 13.142163
Train Epoch: 79 	 Loss: 13.345259
Train Epoch: 80 	 Loss: 13.238535
Train Epoch: 81 	 Loss: 15.099610
Train Epoch: 82 	 Loss: 13.037744
Train Epoch: 83 	 Loss: 13.351294
Train Epoch: 84 	 Loss: 13.297451
Train Epoch: 85 	 Loss: 13.055531
Train Epoch: 86 	 Loss: 13.793932
Train Epoch: 87 	 Loss: 13.194698
Train Epoch: 88 	 Loss: 13.383104
Train Epoch: 89 	 Loss: 13.356694
Train Epoch: 90 	 Loss: 13.363068
Train Epoch: 91 	 Loss: 13.152466
Train Epoch: 92 	 Loss: 12.887329
Train Epoch: 93 	 Loss: 13.003706
Train Epoch: 94 	 Loss: 13.028143
Train Epoch: 95 	 Loss: 13.486205
Train Epoch: 96 	 Loss: 12.886981
Train Epoch: 97 	 Loss: 13.097914
Train Epoch: 98 	 Loss: 12.987274
Train Epoch: 99 	 Loss: 13.039114
Train Epoch: 100 	 Loss: 12.825470
Train Epoch: 101 	 Loss: 12.950690
Train Epoch: 102 	 Loss: 13.220684
Train Epoch: 103 	 Loss: 13.190182
Train Epoch: 104 	 Loss: 12.918615
Train Epoch: 105 	 Loss: 12.949376
Train Epoch: 106 	 Loss: 13.882833
Train Epoch: 107 	 Loss: 13.109385
Train Epoch: 108 	 Loss: 12.966163
Train Epoch: 109 	 Loss: 14.298946
Train Epoch: 110 	 Loss: 13.136738
Train Epoch: 111 	 Loss: 12.965981
Train Epoch: 112 	 Loss: 13.116589
Train Epoch: 113 	 Loss: 12.854921
Train Epoch: 114 	 Loss: 13.022509
Train Epoch: 115 	 Loss: 12.851499
Train Epoch: 116 	 Loss: 13.419390
Train Epoch: 117 	 Loss: 13.061255
Train Epoch: 118 	 Loss: 13.176597
Train Epoch: 119 	 Loss: 12.789553
Train Epoch: 120 	 Loss: 13.122126
Train Epoch: 121 	 Loss: 13.556741
Train Epoch: 122 	 Loss: 13.678137
Train Epoch: 123 	 Loss: 12.982150
Train Epoch: 124 	 Loss: 12.830835
Train Epoch: 125 	 Loss: 12.859930
Train Epoch: 126 	 Loss: 12.988853
Train Epoch: 127 	 Loss: 13.227024
Train Epoch: 128 	 Loss: 12.893867
Train Epoch: 129 	 Loss: 13.026508
Train Epoch: 130 	 Loss: 12.971172
Train Epoch: 131 	 Loss: 13.135384
Train Epoch: 132 	 Loss: 13.129177
Train Epoch: 133 	 Loss: 13.201180
Train Epoch: 134 	 Loss: 12.973994
Train Epoch: 135 	 Loss: 13.104723
Train Epoch: 136 	 Loss: 12.971284
Train Epoch: 137 	 Loss: 13.586319
Train Epoch: 138 	 Loss: 13.043466
Train Epoch: 139 	 Loss: 13.127058
Train Epoch: 140 	 Loss: 12.990631
Train Epoch: 141 	 Loss: 12.911593
Train Epoch: 142 	 Loss: 13.346801
Train Epoch: 143 	 Loss: 12.889215
Train Epoch: 144 	 Loss: 13.386187
Train Epoch: 145 	 Loss: 12.926292
Train Epoch: 146 	 Loss: 13.344669
Train Epoch: 147 	 Loss: 13.134047
Train Epoch: 148 	 Loss: 13.189623
Train Epoch: 149 	 Loss: 13.931295
Train Epoch: 150 	 Loss: 12.947018
Train Epoch: 151 	 Loss: 12.895119
Train Epoch: 152 	 Loss: 13.003316
Train Epoch: 153 	 Loss: 13.521733
Train Epoch: 154 	 Loss: 13.014704
Train Epoch: 155 	 Loss: 13.064753
Train Epoch: 156 	 Loss: 13.142706
Train Epoch: 157 	 Loss: 13.071563
Train Epoch: 158 	 Loss: 12.878338
Train Epoch: 159 	 Loss: 13.332806
Train Epoch: 160 	 Loss: 12.807262
Train Epoch: 161 	 Loss: 13.334843
Train Epoch: 162 	 Loss: 13.450165
Train Epoch: 163 	 Loss: 12.825949
Train Epoch: 164 	 Loss: 13.228098
Train Epoch: 165 	 Loss: 13.473164
Train Epoch: 166 	 Loss: 12.907276
Train Epoch: 167 	 Loss: 12.847454
Train Epoch: 168 	 Loss: 12.996513
Train Epoch: 169 	 Loss: 13.220095
Train Epoch: 170 	 Loss: 13.050838
Train Epoch: 171 	 Loss: 13.056054
Train Epoch: 172 	 Loss: 13.007405
Train Epoch: 173 	 Loss: 13.064095
Train Epoch: 174 	 Loss: 13.033485
Train Epoch: 175 	 Loss: 12.898511
Train Epoch: 176 	 Loss: 12.943465
Train Epoch: 177 	 Loss: 12.960632
Train Epoch: 178 	 Loss: 13.006696
Train Epoch: 179 	 Loss: 13.300566
Train Epoch: 180 	 Loss: 12.929155
Train Epoch: 181 	 Loss: 13.264143
Train Epoch: 182 	 Loss: 12.794151
Train Epoch: 183 	 Loss: 13.089935
Train Epoch: 184 	 Loss: 13.000135
Train Epoch: 185 	 Loss: 12.858295
Train Epoch: 186 	 Loss: 13.126850
Train Epoch: 187 	 Loss: 12.897837
Train Epoch: 188 	 Loss: 12.959692
Train Epoch: 189 	 Loss: 13.285870
Train Epoch: 190 	 Loss: 13.558007
Train Epoch: 191 	 Loss: 12.846977
Train Epoch: 192 	 Loss: 13.205296
Train Epoch: 193 	 Loss: 13.151832
Train Epoch: 194 	 Loss: 13.397198
Train Epoch: 195 	 Loss: 13.357595
Train Epoch: 196 	 Loss: 13.096102
Train Epoch: 197 	 Loss: 12.909011
Train Epoch: 198 	 Loss: 13.549910
Train Epoch: 199 	 Loss: 12.875563

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.889

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.545

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.932

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.867

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.872

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.931

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.939

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.875

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.901

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.904

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.876

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.910
------
f1 mean across methods is 0.870


 ---------------------------------------- 


For each of the methods
Average F1:  [0.81736725 0.48363049 0.909471   0.84679654 0.90499289 0.77858204
 0.92584383 0.80583709 0.8965537  0.71976394 0.85066893 0.90804451]
Std F1:  [0.12436155 0.13856085 0.02659482 0.04529152 0.02329288 0.18545288
 0.01223845 0.21317722 0.03589844 0.2294625  0.03735376 0.03659058]
Average over repetitions across all methods
Average f1 score:  0.820629350165218
Std F1:  0.05878067158367358

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 200, 'order_hermite': 100, 'subsampled_rate': 0.35} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.70986458e-01 4.06980280e-01 1.71018658e-02 4.73461697e-03
 1.96778791e-04] 

Train Epoch: 0 	 Loss: 20.030584
Train Epoch: 1 	 Loss: 19.893988
Train Epoch: 2 	 Loss: 19.004150
Train Epoch: 3 	 Loss: 15.980694
Train Epoch: 4 	 Loss: 15.348709
Train Epoch: 5 	 Loss: 15.590622
Train Epoch: 6 	 Loss: 15.855729
Train Epoch: 7 	 Loss: 15.240814
Train Epoch: 8 	 Loss: 15.341101
Train Epoch: 9 	 Loss: 15.624678
Train Epoch: 10 	 Loss: 15.254223
Train Epoch: 11 	 Loss: 15.222782
Train Epoch: 12 	 Loss: 15.548669
Train Epoch: 13 	 Loss: 15.836507
Train Epoch: 14 	 Loss: 14.980354
Train Epoch: 15 	 Loss: 15.669281
Train Epoch: 16 	 Loss: 15.109228
Train Epoch: 17 	 Loss: 14.857071
Train Epoch: 18 	 Loss: 14.562827
Train Epoch: 19 	 Loss: 14.783987
Train Epoch: 20 	 Loss: 14.948625
Train Epoch: 21 	 Loss: 15.148211
Train Epoch: 22 	 Loss: 15.572378
Train Epoch: 23 	 Loss: 14.819555
Train Epoch: 24 	 Loss: 15.481506
Train Epoch: 25 	 Loss: 14.655470
Train Epoch: 26 	 Loss: 14.564882
Train Epoch: 27 	 Loss: 14.380342
Train Epoch: 28 	 Loss: 14.982740
Train Epoch: 29 	 Loss: 15.179776
Train Epoch: 30 	 Loss: 15.185860
Train Epoch: 31 	 Loss: 15.422939
Train Epoch: 32 	 Loss: 14.271315
Train Epoch: 33 	 Loss: 14.834009
Train Epoch: 34 	 Loss: 15.164497
Train Epoch: 35 	 Loss: 15.336018
Train Epoch: 36 	 Loss: 14.752234
Train Epoch: 37 	 Loss: 14.923124
Train Epoch: 38 	 Loss: 14.712130
Train Epoch: 39 	 Loss: 14.840073
Train Epoch: 40 	 Loss: 15.153225
Train Epoch: 41 	 Loss: 14.486633
Train Epoch: 42 	 Loss: 15.034632
Train Epoch: 43 	 Loss: 14.074167
Train Epoch: 44 	 Loss: 13.973759
Train Epoch: 45 	 Loss: 14.498434
Train Epoch: 46 	 Loss: 13.981247
Train Epoch: 47 	 Loss: 13.754285
Train Epoch: 48 	 Loss: 13.967546
Train Epoch: 49 	 Loss: 13.805850
Train Epoch: 50 	 Loss: 14.030494
Train Epoch: 51 	 Loss: 14.176370
Train Epoch: 52 	 Loss: 13.902947
Train Epoch: 53 	 Loss: 14.172916
Train Epoch: 54 	 Loss: 14.010216
Train Epoch: 55 	 Loss: 13.973234
Train Epoch: 56 	 Loss: 13.854569
Train Epoch: 57 	 Loss: 14.611177
Train Epoch: 58 	 Loss: 13.866762
Train Epoch: 59 	 Loss: 14.058940
Train Epoch: 60 	 Loss: 13.873318
Train Epoch: 61 	 Loss: 13.741663
Train Epoch: 62 	 Loss: 13.990527
Train Epoch: 63 	 Loss: 14.883337
Train Epoch: 64 	 Loss: 14.314613
Train Epoch: 65 	 Loss: 13.830280
Train Epoch: 66 	 Loss: 13.692940
Train Epoch: 67 	 Loss: 13.791306
Train Epoch: 68 	 Loss: 13.970233
Train Epoch: 69 	 Loss: 13.869835
Train Epoch: 70 	 Loss: 14.124797
Train Epoch: 71 	 Loss: 13.725031
Train Epoch: 72 	 Loss: 13.874792
Train Epoch: 73 	 Loss: 13.679057
Train Epoch: 74 	 Loss: 13.711931
Train Epoch: 75 	 Loss: 14.603963
Train Epoch: 76 	 Loss: 13.938004
Train Epoch: 77 	 Loss: 14.061898
Train Epoch: 78 	 Loss: 14.101967
Train Epoch: 79 	 Loss: 13.880070
Train Epoch: 80 	 Loss: 13.722627
Train Epoch: 81 	 Loss: 14.118702
Train Epoch: 82 	 Loss: 13.885513
Train Epoch: 83 	 Loss: 14.228155
Train Epoch: 84 	 Loss: 13.638849
Train Epoch: 85 	 Loss: 14.059348
Train Epoch: 86 	 Loss: 15.110006
Train Epoch: 87 	 Loss: 13.929327
Train Epoch: 88 	 Loss: 14.328740
Train Epoch: 89 	 Loss: 13.981848
Train Epoch: 90 	 Loss: 14.205206
Train Epoch: 91 	 Loss: 13.791162
Train Epoch: 92 	 Loss: 13.826437
Train Epoch: 93 	 Loss: 13.794832
Train Epoch: 94 	 Loss: 13.897799
Train Epoch: 95 	 Loss: 13.725241
Train Epoch: 96 	 Loss: 13.963419
Train Epoch: 97 	 Loss: 13.914778
Train Epoch: 98 	 Loss: 13.895804
Train Epoch: 99 	 Loss: 14.294569
Train Epoch: 100 	 Loss: 13.842993
Train Epoch: 101 	 Loss: 14.019186
Train Epoch: 102 	 Loss: 14.121093
Train Epoch: 103 	 Loss: 14.015106
Train Epoch: 104 	 Loss: 13.880116
Train Epoch: 105 	 Loss: 13.941935
Train Epoch: 106 	 Loss: 14.076672
Train Epoch: 107 	 Loss: 13.867428
Train Epoch: 108 	 Loss: 14.421646
Train Epoch: 109 	 Loss: 13.688396
Train Epoch: 110 	 Loss: 13.982184
Train Epoch: 111 	 Loss: 13.794924
Train Epoch: 112 	 Loss: 13.633001
Train Epoch: 113 	 Loss: 13.716760
Train Epoch: 114 	 Loss: 13.722693
Train Epoch: 115 	 Loss: 13.773309
Train Epoch: 116 	 Loss: 14.240031
Train Epoch: 117 	 Loss: 13.812052
Train Epoch: 118 	 Loss: 14.263090
Train Epoch: 119 	 Loss: 13.877815
Train Epoch: 120 	 Loss: 13.707822
Train Epoch: 121 	 Loss: 14.127309
Train Epoch: 122 	 Loss: 13.951940
Train Epoch: 123 	 Loss: 14.049316
Train Epoch: 124 	 Loss: 13.921104
Train Epoch: 125 	 Loss: 13.966249
Train Epoch: 126 	 Loss: 13.851391
Train Epoch: 127 	 Loss: 13.821476
Train Epoch: 128 	 Loss: 13.805485
Train Epoch: 129 	 Loss: 13.684343
Train Epoch: 130 	 Loss: 14.022160
Train Epoch: 131 	 Loss: 14.003297
Train Epoch: 132 	 Loss: 14.378338
Train Epoch: 133 	 Loss: 13.819788
Train Epoch: 134 	 Loss: 13.991282
Train Epoch: 135 	 Loss: 13.739104
Train Epoch: 136 	 Loss: 13.653658
Train Epoch: 137 	 Loss: 14.048603
Train Epoch: 138 	 Loss: 13.725366
Train Epoch: 139 	 Loss: 13.668280
Train Epoch: 140 	 Loss: 13.963463
Train Epoch: 141 	 Loss: 13.949898
Train Epoch: 142 	 Loss: 13.883108
Train Epoch: 143 	 Loss: 13.746979
Train Epoch: 144 	 Loss: 13.728534
Train Epoch: 145 	 Loss: 14.075970
Train Epoch: 146 	 Loss: 13.706059
Train Epoch: 147 	 Loss: 14.037552
Train Epoch: 148 	 Loss: 13.579193
Train Epoch: 149 	 Loss: 13.681344
Train Epoch: 150 	 Loss: 13.923398
Train Epoch: 151 	 Loss: 14.277537
Train Epoch: 152 	 Loss: 13.756763
Train Epoch: 153 	 Loss: 13.933681
Train Epoch: 154 	 Loss: 14.302356
Train Epoch: 155 	 Loss: 13.755057
Train Epoch: 156 	 Loss: 14.597105
Train Epoch: 157 	 Loss: 14.150107
Train Epoch: 158 	 Loss: 13.739048
Train Epoch: 159 	 Loss: 13.767212
Train Epoch: 160 	 Loss: 13.644917
Train Epoch: 161 	 Loss: 13.889162
Train Epoch: 162 	 Loss: 14.126471
Train Epoch: 163 	 Loss: 14.144114
Train Epoch: 164 	 Loss: 13.917564
Train Epoch: 165 	 Loss: 13.802416
Train Epoch: 166 	 Loss: 13.941380
Train Epoch: 167 	 Loss: 13.609743
Train Epoch: 168 	 Loss: 13.869259
Train Epoch: 169 	 Loss: 13.719789
Train Epoch: 170 	 Loss: 13.922462
Train Epoch: 171 	 Loss: 13.666075
Train Epoch: 172 	 Loss: 13.774902
Train Epoch: 173 	 Loss: 14.030252
Train Epoch: 174 	 Loss: 14.013319
Train Epoch: 175 	 Loss: 13.792457
Train Epoch: 176 	 Loss: 13.579004
Train Epoch: 177 	 Loss: 13.821117
Train Epoch: 178 	 Loss: 13.609465
Train Epoch: 179 	 Loss: 13.783457
Train Epoch: 180 	 Loss: 13.626783
Train Epoch: 181 	 Loss: 13.711850
Train Epoch: 182 	 Loss: 14.220860
Train Epoch: 183 	 Loss: 13.718956
Train Epoch: 184 	 Loss: 13.823406
Train Epoch: 185 	 Loss: 14.122421
Train Epoch: 186 	 Loss: 13.943619
Train Epoch: 187 	 Loss: 14.207918
Train Epoch: 188 	 Loss: 13.883809
Train Epoch: 189 	 Loss: 13.753754
Train Epoch: 190 	 Loss: 13.824049
Train Epoch: 191 	 Loss: 13.935827
Train Epoch: 192 	 Loss: 13.995626
Train Epoch: 193 	 Loss: 14.066880
Train Epoch: 194 	 Loss: 13.598407
Train Epoch: 195 	 Loss: 13.594233
Train Epoch: 196 	 Loss: 13.680660
Train Epoch: 197 	 Loss: 13.853691
Train Epoch: 198 	 Loss: 13.679670
Train Epoch: 199 	 Loss: 13.916683

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.902

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.546

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.805

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.894

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.922

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.689

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.938

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.911

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.916

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.904

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.891

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.915
------
f1 mean across methods is 0.853


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.72673985e-01 4.04982678e-01 1.74298305e-02 4.69883901e-03
 2.14667772e-04] 

Train Epoch: 0 	 Loss: 18.932775
Train Epoch: 1 	 Loss: 18.041607
Train Epoch: 2 	 Loss: 17.032185
Train Epoch: 3 	 Loss: 15.621761
Train Epoch: 4 	 Loss: 14.393703
Train Epoch: 5 	 Loss: 14.012380
Train Epoch: 6 	 Loss: 14.142497
Train Epoch: 7 	 Loss: 13.779909
Train Epoch: 8 	 Loss: 13.932857
Train Epoch: 9 	 Loss: 14.239111
Train Epoch: 10 	 Loss: 14.053961
Train Epoch: 11 	 Loss: 13.724211
Train Epoch: 12 	 Loss: 13.583220
Train Epoch: 13 	 Loss: 13.656777
Train Epoch: 14 	 Loss: 13.479738
Train Epoch: 15 	 Loss: 13.309752
Train Epoch: 16 	 Loss: 13.107227
Train Epoch: 17 	 Loss: 13.288900
Train Epoch: 18 	 Loss: 14.189667
Train Epoch: 19 	 Loss: 13.815919
Train Epoch: 20 	 Loss: 13.517496
Train Epoch: 21 	 Loss: 13.093399
Train Epoch: 22 	 Loss: 14.360947
Train Epoch: 23 	 Loss: 13.323238
Train Epoch: 24 	 Loss: 13.063881
Train Epoch: 25 	 Loss: 13.048508
Train Epoch: 26 	 Loss: 13.198034
Train Epoch: 27 	 Loss: 13.260447
Train Epoch: 28 	 Loss: 13.225391
Train Epoch: 29 	 Loss: 13.312576
Train Epoch: 30 	 Loss: 13.134602
Train Epoch: 31 	 Loss: 12.868462
Train Epoch: 32 	 Loss: 13.004212
Train Epoch: 33 	 Loss: 12.929049
Train Epoch: 34 	 Loss: 13.637222
Train Epoch: 35 	 Loss: 13.103299
Train Epoch: 36 	 Loss: 13.152139
Train Epoch: 37 	 Loss: 13.643625
Train Epoch: 38 	 Loss: 12.969123
Train Epoch: 39 	 Loss: 13.967321
Train Epoch: 40 	 Loss: 13.091429
Train Epoch: 41 	 Loss: 13.782413
Train Epoch: 42 	 Loss: 13.310947
Train Epoch: 43 	 Loss: 13.490957
Train Epoch: 44 	 Loss: 13.358786
Train Epoch: 45 	 Loss: 12.979925
Train Epoch: 46 	 Loss: 12.925914
Train Epoch: 47 	 Loss: 13.062607
Train Epoch: 48 	 Loss: 13.499121
Train Epoch: 49 	 Loss: 13.266171
Train Epoch: 50 	 Loss: 13.189255
Train Epoch: 51 	 Loss: 13.182615
Train Epoch: 52 	 Loss: 13.186745
Train Epoch: 53 	 Loss: 13.552651
Train Epoch: 54 	 Loss: 12.999172
Train Epoch: 55 	 Loss: 13.112293
Train Epoch: 56 	 Loss: 13.092522
Train Epoch: 57 	 Loss: 13.266644
Train Epoch: 58 	 Loss: 13.208038
Train Epoch: 59 	 Loss: 12.968946
Train Epoch: 60 	 Loss: 13.055330
Train Epoch: 61 	 Loss: 12.952160
Train Epoch: 62 	 Loss: 12.856053
Train Epoch: 63 	 Loss: 13.027794
Train Epoch: 64 	 Loss: 13.129761
Train Epoch: 65 	 Loss: 12.968454
Train Epoch: 66 	 Loss: 13.130971
Train Epoch: 67 	 Loss: 13.355693
Train Epoch: 68 	 Loss: 13.102197
Train Epoch: 69 	 Loss: 12.877313
Train Epoch: 70 	 Loss: 12.963291
Train Epoch: 71 	 Loss: 12.937139
Train Epoch: 72 	 Loss: 13.427933
Train Epoch: 73 	 Loss: 13.367823
Train Epoch: 74 	 Loss: 13.004135
Train Epoch: 75 	 Loss: 13.272047
Train Epoch: 76 	 Loss: 12.960482
Train Epoch: 77 	 Loss: 12.772598
Train Epoch: 78 	 Loss: 12.835249
Train Epoch: 79 	 Loss: 13.073526
Train Epoch: 80 	 Loss: 12.877285
Train Epoch: 81 	 Loss: 12.988479
Train Epoch: 82 	 Loss: 13.605308
Train Epoch: 83 	 Loss: 12.847192
Train Epoch: 84 	 Loss: 12.861270
Train Epoch: 85 	 Loss: 13.047685
Train Epoch: 86 	 Loss: 12.937841
Train Epoch: 87 	 Loss: 12.939476
Train Epoch: 88 	 Loss: 13.469599
Train Epoch: 89 	 Loss: 13.352055
Train Epoch: 90 	 Loss: 13.257343
Train Epoch: 91 	 Loss: 13.009609
Train Epoch: 92 	 Loss: 12.802910
Train Epoch: 93 	 Loss: 12.947694
Train Epoch: 94 	 Loss: 13.187611
Train Epoch: 95 	 Loss: 12.989126
Train Epoch: 96 	 Loss: 13.303510
Train Epoch: 97 	 Loss: 12.917682
Train Epoch: 98 	 Loss: 12.999051
Train Epoch: 99 	 Loss: 13.127510
Train Epoch: 100 	 Loss: 13.247450
Train Epoch: 101 	 Loss: 13.178118
Train Epoch: 102 	 Loss: 12.991159
Train Epoch: 103 	 Loss: 13.493246
Train Epoch: 104 	 Loss: 13.024935
Train Epoch: 105 	 Loss: 13.548001
Train Epoch: 106 	 Loss: 13.101044
Train Epoch: 107 	 Loss: 13.651415
Train Epoch: 108 	 Loss: 13.309418
Train Epoch: 109 	 Loss: 13.514263
Train Epoch: 110 	 Loss: 13.945393
Train Epoch: 111 	 Loss: 13.413475
Train Epoch: 112 	 Loss: 13.343511
Train Epoch: 113 	 Loss: 13.453362
Train Epoch: 114 	 Loss: 13.679872
Train Epoch: 115 	 Loss: 13.286320
Train Epoch: 116 	 Loss: 13.561803
Train Epoch: 117 	 Loss: 13.319224
Train Epoch: 118 	 Loss: 13.266691
Train Epoch: 119 	 Loss: 13.397396
Train Epoch: 120 	 Loss: 13.653360
Train Epoch: 121 	 Loss: 13.713789
Train Epoch: 122 	 Loss: 13.432905
Train Epoch: 123 	 Loss: 13.388649
Train Epoch: 124 	 Loss: 13.536449
Train Epoch: 125 	 Loss: 13.432730
Train Epoch: 126 	 Loss: 13.253519
Train Epoch: 127 	 Loss: 13.360775
Train Epoch: 128 	 Loss: 13.443554
Train Epoch: 129 	 Loss: 13.262128
Train Epoch: 130 	 Loss: 13.264271
Train Epoch: 131 	 Loss: 13.760802
Train Epoch: 132 	 Loss: 13.195871
Train Epoch: 133 	 Loss: 13.641400
Train Epoch: 134 	 Loss: 13.284563
Train Epoch: 135 	 Loss: 13.392196
Train Epoch: 136 	 Loss: 13.273304
Train Epoch: 137 	 Loss: 13.509674
Train Epoch: 138 	 Loss: 13.510380
Train Epoch: 139 	 Loss: 13.226209
Train Epoch: 140 	 Loss: 13.824932
Train Epoch: 141 	 Loss: 13.242042
Train Epoch: 142 	 Loss: 13.920362
Train Epoch: 143 	 Loss: 13.864883
Train Epoch: 144 	 Loss: 13.250246
Train Epoch: 145 	 Loss: 13.351054
Train Epoch: 146 	 Loss: 13.224916
Train Epoch: 147 	 Loss: 13.797693
Train Epoch: 148 	 Loss: 13.213442
Train Epoch: 149 	 Loss: 13.235105
Train Epoch: 150 	 Loss: 13.249985
Train Epoch: 151 	 Loss: 13.589277
Train Epoch: 152 	 Loss: 12.784931
Train Epoch: 153 	 Loss: 12.821146
Train Epoch: 154 	 Loss: 12.991156
Train Epoch: 155 	 Loss: 13.992324
Train Epoch: 156 	 Loss: 12.874964
Train Epoch: 157 	 Loss: 13.655034
Train Epoch: 158 	 Loss: 13.698641
Train Epoch: 159 	 Loss: 13.378788
Train Epoch: 160 	 Loss: 13.788750
Train Epoch: 161 	 Loss: 13.518505
Train Epoch: 162 	 Loss: 13.271269
Train Epoch: 163 	 Loss: 13.248972
Train Epoch: 164 	 Loss: 13.225540
Train Epoch: 165 	 Loss: 13.264687
Train Epoch: 166 	 Loss: 13.306220
Train Epoch: 167 	 Loss: 13.216122
Train Epoch: 168 	 Loss: 13.542847
Train Epoch: 169 	 Loss: 13.917389
Train Epoch: 170 	 Loss: 13.481218
Train Epoch: 171 	 Loss: 13.495037
Train Epoch: 172 	 Loss: 13.396883
Train Epoch: 173 	 Loss: 13.168172
Train Epoch: 174 	 Loss: 13.275831
Train Epoch: 175 	 Loss: 13.651630
Train Epoch: 176 	 Loss: 13.395870
Train Epoch: 177 	 Loss: 13.176682
Train Epoch: 178 	 Loss: 13.210320
Train Epoch: 179 	 Loss: 13.614252
Train Epoch: 180 	 Loss: 13.327578
Train Epoch: 181 	 Loss: 13.525690
Train Epoch: 182 	 Loss: 13.274498
Train Epoch: 183 	 Loss: 13.347912
Train Epoch: 184 	 Loss: 13.522308
Train Epoch: 185 	 Loss: 13.387379
Train Epoch: 186 	 Loss: 13.723251
Train Epoch: 187 	 Loss: 13.239783
Train Epoch: 188 	 Loss: 13.513741
Train Epoch: 189 	 Loss: 13.357527
Train Epoch: 190 	 Loss: 13.493142
Train Epoch: 191 	 Loss: 13.543020
Train Epoch: 192 	 Loss: 13.188707
Train Epoch: 193 	 Loss: 13.198465
Train Epoch: 194 	 Loss: 13.480534
Train Epoch: 195 	 Loss: 13.409462
Train Epoch: 196 	 Loss: 13.331225
Train Epoch: 197 	 Loss: 13.477101
Train Epoch: 198 	 Loss: 13.565064
Train Epoch: 199 	 Loss: 13.247486

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.931

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.538

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.855

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.880

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.901

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.792

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.962

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.892

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.927

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.911

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.872

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.962
------
f1 mean across methods is 0.868


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.71177274e-01 4.06825243e-01 1.70601249e-02 4.71076499e-03
 2.26593759e-04] 

Train Epoch: 0 	 Loss: 21.365555
Train Epoch: 1 	 Loss: 19.054451
Train Epoch: 2 	 Loss: 18.347198
Train Epoch: 3 	 Loss: 17.716358
Train Epoch: 4 	 Loss: 15.825588
Train Epoch: 5 	 Loss: 15.072290
Train Epoch: 6 	 Loss: 15.097336
Train Epoch: 7 	 Loss: 15.302519
Train Epoch: 8 	 Loss: 15.424503
Train Epoch: 9 	 Loss: 14.857800
Train Epoch: 10 	 Loss: 14.928899
Train Epoch: 11 	 Loss: 15.443651
Train Epoch: 12 	 Loss: 14.565646
Train Epoch: 13 	 Loss: 14.593132
Train Epoch: 14 	 Loss: 14.922801
Train Epoch: 15 	 Loss: 14.899615
Train Epoch: 16 	 Loss: 14.360813
Train Epoch: 17 	 Loss: 15.142174
Train Epoch: 18 	 Loss: 14.811523
Train Epoch: 19 	 Loss: 14.483870
Train Epoch: 20 	 Loss: 14.472553
Train Epoch: 21 	 Loss: 14.611155
Train Epoch: 22 	 Loss: 15.421339
Train Epoch: 23 	 Loss: 14.596073
Train Epoch: 24 	 Loss: 14.668614
Train Epoch: 25 	 Loss: 14.507536
Train Epoch: 26 	 Loss: 14.356364
Train Epoch: 27 	 Loss: 14.537514
Train Epoch: 28 	 Loss: 14.259633
Train Epoch: 29 	 Loss: 14.582770
Train Epoch: 30 	 Loss: 14.506699
Train Epoch: 31 	 Loss: 14.242128
Train Epoch: 32 	 Loss: 14.193867
Train Epoch: 33 	 Loss: 14.246912
Train Epoch: 34 	 Loss: 14.009820
Train Epoch: 35 	 Loss: 13.997671
Train Epoch: 36 	 Loss: 14.084041
Train Epoch: 37 	 Loss: 14.717222
Train Epoch: 38 	 Loss: 14.029413
Train Epoch: 39 	 Loss: 14.020327
Train Epoch: 40 	 Loss: 13.985500
Train Epoch: 41 	 Loss: 13.937799
Train Epoch: 42 	 Loss: 13.986433
Train Epoch: 43 	 Loss: 14.023742
Train Epoch: 44 	 Loss: 13.875406
Train Epoch: 45 	 Loss: 13.867035
Train Epoch: 46 	 Loss: 14.085938
Train Epoch: 47 	 Loss: 14.926105
Train Epoch: 48 	 Loss: 15.001280
Train Epoch: 49 	 Loss: 14.321779
Train Epoch: 50 	 Loss: 14.361679
Train Epoch: 51 	 Loss: 14.304224
Train Epoch: 52 	 Loss: 14.749290
Train Epoch: 53 	 Loss: 14.141260
Train Epoch: 54 	 Loss: 14.081455
Train Epoch: 55 	 Loss: 14.370773
Train Epoch: 56 	 Loss: 14.088195
Train Epoch: 57 	 Loss: 14.547422
Train Epoch: 58 	 Loss: 14.012997
Train Epoch: 59 	 Loss: 14.105120
Train Epoch: 60 	 Loss: 14.062639
Train Epoch: 61 	 Loss: 14.163265
Train Epoch: 62 	 Loss: 14.065622
Train Epoch: 63 	 Loss: 14.353186
Train Epoch: 64 	 Loss: 14.057398
Train Epoch: 65 	 Loss: 14.748661
Train Epoch: 66 	 Loss: 13.992850
Train Epoch: 67 	 Loss: 14.184376
Train Epoch: 68 	 Loss: 14.099255
Train Epoch: 69 	 Loss: 14.049560
Train Epoch: 70 	 Loss: 14.149682
Train Epoch: 71 	 Loss: 14.013340
Train Epoch: 72 	 Loss: 14.127145
Train Epoch: 73 	 Loss: 14.027065
Train Epoch: 74 	 Loss: 14.010850
Train Epoch: 75 	 Loss: 14.203756
Train Epoch: 76 	 Loss: 14.353453
Train Epoch: 77 	 Loss: 14.087622
Train Epoch: 78 	 Loss: 13.964603
Train Epoch: 79 	 Loss: 14.188881
Train Epoch: 80 	 Loss: 14.221721
Train Epoch: 81 	 Loss: 14.393057
Train Epoch: 82 	 Loss: 14.278271
Train Epoch: 83 	 Loss: 14.038061
Train Epoch: 84 	 Loss: 14.107652
Train Epoch: 85 	 Loss: 14.101743
Train Epoch: 86 	 Loss: 13.961326
Train Epoch: 87 	 Loss: 14.191031
Train Epoch: 88 	 Loss: 14.362177
Train Epoch: 89 	 Loss: 14.034325
Train Epoch: 90 	 Loss: 14.206524
Train Epoch: 91 	 Loss: 14.207433
Train Epoch: 92 	 Loss: 14.183825
Train Epoch: 93 	 Loss: 14.279675
Train Epoch: 94 	 Loss: 14.111748
Train Epoch: 95 	 Loss: 13.990485
Train Epoch: 96 	 Loss: 14.063522
Train Epoch: 97 	 Loss: 14.207195
Train Epoch: 98 	 Loss: 14.099598
Train Epoch: 99 	 Loss: 14.187956
Train Epoch: 100 	 Loss: 14.113338
Train Epoch: 101 	 Loss: 14.121192
Train Epoch: 102 	 Loss: 14.262278
Train Epoch: 103 	 Loss: 13.966667
Train Epoch: 104 	 Loss: 14.061697
Train Epoch: 105 	 Loss: 14.404249
Train Epoch: 106 	 Loss: 14.078701
Train Epoch: 107 	 Loss: 14.165390
Train Epoch: 108 	 Loss: 14.021605
Train Epoch: 109 	 Loss: 14.103830
Train Epoch: 110 	 Loss: 14.040503
Train Epoch: 111 	 Loss: 14.104573
Train Epoch: 112 	 Loss: 13.936333
Train Epoch: 113 	 Loss: 14.305429
Train Epoch: 114 	 Loss: 14.021719
Train Epoch: 115 	 Loss: 14.014475
Train Epoch: 116 	 Loss: 14.119028
Train Epoch: 117 	 Loss: 14.104083
Train Epoch: 118 	 Loss: 13.980676
Train Epoch: 119 	 Loss: 14.042361
Train Epoch: 120 	 Loss: 14.073075
Train Epoch: 121 	 Loss: 14.018877
Train Epoch: 122 	 Loss: 14.072088
Train Epoch: 123 	 Loss: 14.283818
Train Epoch: 124 	 Loss: 13.983982
Train Epoch: 125 	 Loss: 14.025364
Train Epoch: 126 	 Loss: 14.148295
Train Epoch: 127 	 Loss: 13.907329
Train Epoch: 128 	 Loss: 14.003656
Train Epoch: 129 	 Loss: 13.893882
Train Epoch: 130 	 Loss: 14.374358
Train Epoch: 131 	 Loss: 14.688047
Train Epoch: 132 	 Loss: 14.156473
Train Epoch: 133 	 Loss: 14.215633
Train Epoch: 134 	 Loss: 14.077831
Train Epoch: 135 	 Loss: 13.962045
Train Epoch: 136 	 Loss: 14.054839
Train Epoch: 137 	 Loss: 14.655383
Train Epoch: 138 	 Loss: 13.950260
Train Epoch: 139 	 Loss: 14.109590
Train Epoch: 140 	 Loss: 14.019897
Train Epoch: 141 	 Loss: 13.897656
Train Epoch: 142 	 Loss: 13.993102
Train Epoch: 143 	 Loss: 14.180153
Train Epoch: 144 	 Loss: 14.696526
Train Epoch: 145 	 Loss: 13.994928
Train Epoch: 146 	 Loss: 14.416895
Train Epoch: 147 	 Loss: 14.200544
Train Epoch: 148 	 Loss: 13.951502
Train Epoch: 149 	 Loss: 14.000360
Train Epoch: 150 	 Loss: 14.022484
Train Epoch: 151 	 Loss: 14.131526
Train Epoch: 152 	 Loss: 14.124115
Train Epoch: 153 	 Loss: 14.047998
Train Epoch: 154 	 Loss: 14.551694
Train Epoch: 155 	 Loss: 13.933784
Train Epoch: 156 	 Loss: 14.488114
Train Epoch: 157 	 Loss: 13.933480
Train Epoch: 158 	 Loss: 14.471021
Train Epoch: 159 	 Loss: 14.744396
Train Epoch: 160 	 Loss: 14.011827
Train Epoch: 161 	 Loss: 13.894850
Train Epoch: 162 	 Loss: 13.997223
Train Epoch: 163 	 Loss: 14.102589
Train Epoch: 164 	 Loss: 13.950857
Train Epoch: 165 	 Loss: 14.086836
Train Epoch: 166 	 Loss: 13.861768
Train Epoch: 167 	 Loss: 14.032003
Train Epoch: 168 	 Loss: 14.019838
Train Epoch: 169 	 Loss: 14.123549
Train Epoch: 170 	 Loss: 14.289634
Train Epoch: 171 	 Loss: 13.855141
Train Epoch: 172 	 Loss: 14.258033
Train Epoch: 173 	 Loss: 14.424008
Train Epoch: 174 	 Loss: 13.963160
Train Epoch: 175 	 Loss: 13.985025
Train Epoch: 176 	 Loss: 13.878382
Train Epoch: 177 	 Loss: 14.330299
Train Epoch: 178 	 Loss: 13.925651
Train Epoch: 179 	 Loss: 14.176157
Train Epoch: 180 	 Loss: 13.891783
Train Epoch: 181 	 Loss: 14.103543
Train Epoch: 182 	 Loss: 13.936680
Train Epoch: 183 	 Loss: 14.021931
Train Epoch: 184 	 Loss: 14.055825
Train Epoch: 185 	 Loss: 13.891308
Train Epoch: 186 	 Loss: 13.960084
Train Epoch: 187 	 Loss: 14.208075
Train Epoch: 188 	 Loss: 13.889469
Train Epoch: 189 	 Loss: 13.972872
Train Epoch: 190 	 Loss: 13.911319
Train Epoch: 191 	 Loss: 14.057951
Train Epoch: 192 	 Loss: 14.017769
Train Epoch: 193 	 Loss: 14.061159
Train Epoch: 194 	 Loss: 15.003733
Train Epoch: 195 	 Loss: 13.945243
Train Epoch: 196 	 Loss: 13.860634
Train Epoch: 197 	 Loss: 14.220585
Train Epoch: 198 	 Loss: 14.312550
Train Epoch: 199 	 Loss: 13.951925

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.926

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.544

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.856

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.876

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.963

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.928

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.913

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.917

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.936

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.911

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.826

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.873
------
f1 mean across methods is 0.872


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.71827240e-01 4.06103720e-01 1.70004949e-02 4.84791385e-03
 2.20630765e-04] 

Train Epoch: 0 	 Loss: 20.530582
Train Epoch: 1 	 Loss: 19.788260
Train Epoch: 2 	 Loss: 18.615849
Train Epoch: 3 	 Loss: 17.293964
Train Epoch: 4 	 Loss: 15.802657
Train Epoch: 5 	 Loss: 15.913797
Train Epoch: 6 	 Loss: 15.607684
Train Epoch: 7 	 Loss: 15.976445
Train Epoch: 8 	 Loss: 15.649936
Train Epoch: 9 	 Loss: 15.674830
Train Epoch: 10 	 Loss: 16.161804
Train Epoch: 11 	 Loss: 15.790419
Train Epoch: 12 	 Loss: 15.828769
Train Epoch: 13 	 Loss: 16.022991
Train Epoch: 14 	 Loss: 15.113861
Train Epoch: 15 	 Loss: 15.207510
Train Epoch: 16 	 Loss: 15.692574
Train Epoch: 17 	 Loss: 15.422225
Train Epoch: 18 	 Loss: 15.080997
Train Epoch: 19 	 Loss: 15.458027
Train Epoch: 20 	 Loss: 15.411776
Train Epoch: 21 	 Loss: 15.379232
Train Epoch: 22 	 Loss: 15.292713
Train Epoch: 23 	 Loss: 15.205937
Train Epoch: 24 	 Loss: 15.058626
Train Epoch: 25 	 Loss: 15.199560
Train Epoch: 26 	 Loss: 15.551155
Train Epoch: 27 	 Loss: 15.506985
Train Epoch: 28 	 Loss: 15.404764
Train Epoch: 29 	 Loss: 15.213799
Train Epoch: 30 	 Loss: 15.240898
Train Epoch: 31 	 Loss: 15.172143
Train Epoch: 32 	 Loss: 15.799940
Train Epoch: 33 	 Loss: 15.243871
Train Epoch: 34 	 Loss: 15.303545
Train Epoch: 35 	 Loss: 15.397427
Train Epoch: 36 	 Loss: 15.244635
Train Epoch: 37 	 Loss: 15.215130
Train Epoch: 38 	 Loss: 15.171092
Train Epoch: 39 	 Loss: 15.558362
Train Epoch: 40 	 Loss: 15.264491
Train Epoch: 41 	 Loss: 15.186133
Train Epoch: 42 	 Loss: 15.282694
Train Epoch: 43 	 Loss: 15.032996
Train Epoch: 44 	 Loss: 15.420076
Train Epoch: 45 	 Loss: 15.092267
Train Epoch: 46 	 Loss: 15.120851
Train Epoch: 47 	 Loss: 15.055584
Train Epoch: 48 	 Loss: 15.052992
Train Epoch: 49 	 Loss: 14.909975
Train Epoch: 50 	 Loss: 15.275848
Train Epoch: 51 	 Loss: 14.972053
Train Epoch: 52 	 Loss: 15.024270
Train Epoch: 53 	 Loss: 14.902171
Train Epoch: 54 	 Loss: 14.947933
Train Epoch: 55 	 Loss: 15.318523
Train Epoch: 56 	 Loss: 14.992546
Train Epoch: 57 	 Loss: 16.226742
Train Epoch: 58 	 Loss: 15.098114
Train Epoch: 59 	 Loss: 15.490094
Train Epoch: 60 	 Loss: 15.132019
Train Epoch: 61 	 Loss: 15.017242
Train Epoch: 62 	 Loss: 15.035601
Train Epoch: 63 	 Loss: 15.341597
Train Epoch: 64 	 Loss: 15.270885
Train Epoch: 65 	 Loss: 14.914665
Train Epoch: 66 	 Loss: 15.149222
Train Epoch: 67 	 Loss: 15.313279
Train Epoch: 68 	 Loss: 14.970108
Train Epoch: 69 	 Loss: 15.017875
Train Epoch: 70 	 Loss: 15.124859
Train Epoch: 71 	 Loss: 15.097382
Train Epoch: 72 	 Loss: 14.958231
Train Epoch: 73 	 Loss: 15.045912
Train Epoch: 74 	 Loss: 15.036710
Train Epoch: 75 	 Loss: 15.232725
Train Epoch: 76 	 Loss: 15.321241
Train Epoch: 77 	 Loss: 15.074220
Train Epoch: 78 	 Loss: 14.972731
Train Epoch: 79 	 Loss: 15.007571
Train Epoch: 80 	 Loss: 15.525855
Train Epoch: 81 	 Loss: 14.969077
Train Epoch: 82 	 Loss: 15.070413
Train Epoch: 83 	 Loss: 14.948217
Train Epoch: 84 	 Loss: 15.166793
Train Epoch: 85 	 Loss: 14.948753
Train Epoch: 86 	 Loss: 15.062531
Train Epoch: 87 	 Loss: 15.105022
Train Epoch: 88 	 Loss: 14.932243
Train Epoch: 89 	 Loss: 14.925522
Train Epoch: 90 	 Loss: 14.968740
Train Epoch: 91 	 Loss: 15.441483
Train Epoch: 92 	 Loss: 14.966214
Train Epoch: 93 	 Loss: 15.078138
Train Epoch: 94 	 Loss: 14.949102
Train Epoch: 95 	 Loss: 14.970249
Train Epoch: 96 	 Loss: 15.359614
Train Epoch: 97 	 Loss: 15.135992
Train Epoch: 98 	 Loss: 14.866884
Train Epoch: 99 	 Loss: 14.861103
Train Epoch: 100 	 Loss: 15.185081
Train Epoch: 101 	 Loss: 14.765951
Train Epoch: 102 	 Loss: 15.101996
Train Epoch: 103 	 Loss: 15.180641
Train Epoch: 104 	 Loss: 15.344895
Train Epoch: 105 	 Loss: 15.086952
Train Epoch: 106 	 Loss: 14.844261
Train Epoch: 107 	 Loss: 15.226343
Train Epoch: 108 	 Loss: 15.125921
Train Epoch: 109 	 Loss: 15.193254
Train Epoch: 110 	 Loss: 14.908736
Train Epoch: 111 	 Loss: 14.819918
Train Epoch: 112 	 Loss: 14.711263
Train Epoch: 113 	 Loss: 15.289675
Train Epoch: 114 	 Loss: 14.677060
Train Epoch: 115 	 Loss: 14.798458
Train Epoch: 116 	 Loss: 14.872225
Train Epoch: 117 	 Loss: 14.695766
Train Epoch: 118 	 Loss: 14.832991
Train Epoch: 119 	 Loss: 14.766575
Train Epoch: 120 	 Loss: 14.557232
Train Epoch: 121 	 Loss: 14.782550
Train Epoch: 122 	 Loss: 15.318239
Train Epoch: 123 	 Loss: 14.610553
Train Epoch: 124 	 Loss: 14.865298
Train Epoch: 125 	 Loss: 14.805540
Train Epoch: 126 	 Loss: 14.832591
Train Epoch: 127 	 Loss: 14.582494
Train Epoch: 128 	 Loss: 14.716543
Train Epoch: 129 	 Loss: 14.856895
Train Epoch: 130 	 Loss: 14.694074
Train Epoch: 131 	 Loss: 14.605378
Train Epoch: 132 	 Loss: 14.742300
Train Epoch: 133 	 Loss: 14.711430
Train Epoch: 134 	 Loss: 14.711765
Train Epoch: 135 	 Loss: 14.822460
Train Epoch: 136 	 Loss: 14.675850
Train Epoch: 137 	 Loss: 14.754027
Train Epoch: 138 	 Loss: 14.908325
Train Epoch: 139 	 Loss: 14.718883
Train Epoch: 140 	 Loss: 14.672054
Train Epoch: 141 	 Loss: 14.743082
Train Epoch: 142 	 Loss: 14.695608
Train Epoch: 143 	 Loss: 14.720574
Train Epoch: 144 	 Loss: 14.654439
Train Epoch: 145 	 Loss: 14.719351
Train Epoch: 146 	 Loss: 14.666281
Train Epoch: 147 	 Loss: 14.901243
Train Epoch: 148 	 Loss: 15.130682
Train Epoch: 149 	 Loss: 14.724441
Train Epoch: 150 	 Loss: 14.652052
Train Epoch: 151 	 Loss: 14.652369
Train Epoch: 152 	 Loss: 14.689682
Train Epoch: 153 	 Loss: 14.832339
Train Epoch: 154 	 Loss: 14.809381
Train Epoch: 155 	 Loss: 14.874161
Train Epoch: 156 	 Loss: 14.817899
Train Epoch: 157 	 Loss: 15.251190
Train Epoch: 158 	 Loss: 14.759097
Train Epoch: 159 	 Loss: 14.593482
Train Epoch: 160 	 Loss: 14.591863
Train Epoch: 161 	 Loss: 14.651969
Train Epoch: 162 	 Loss: 14.729704
Train Epoch: 163 	 Loss: 14.734519
Train Epoch: 164 	 Loss: 14.811136
Train Epoch: 165 	 Loss: 14.743530
Train Epoch: 166 	 Loss: 15.115413
Train Epoch: 167 	 Loss: 14.765429
Train Epoch: 168 	 Loss: 14.801692
Train Epoch: 169 	 Loss: 14.708573
Train Epoch: 170 	 Loss: 14.811174
Train Epoch: 171 	 Loss: 14.696334
Train Epoch: 172 	 Loss: 14.604140
Train Epoch: 173 	 Loss: 15.818474
Train Epoch: 174 	 Loss: 14.716706
Train Epoch: 175 	 Loss: 14.679222
Train Epoch: 176 	 Loss: 14.999426
Train Epoch: 177 	 Loss: 15.031691
Train Epoch: 178 	 Loss: 14.641621
Train Epoch: 179 	 Loss: 14.663696
Train Epoch: 180 	 Loss: 14.647154
Train Epoch: 181 	 Loss: 14.995510
Train Epoch: 182 	 Loss: 14.968369
Train Epoch: 183 	 Loss: 14.726921
Train Epoch: 184 	 Loss: 14.618118
Train Epoch: 185 	 Loss: 14.645042
Train Epoch: 186 	 Loss: 14.932622
Train Epoch: 187 	 Loss: 14.715384
Train Epoch: 188 	 Loss: 14.688421
Train Epoch: 189 	 Loss: 15.086656
Train Epoch: 190 	 Loss: 14.830051
Train Epoch: 191 	 Loss: 14.750868
Train Epoch: 192 	 Loss: 14.717446
Train Epoch: 193 	 Loss: 14.698097
Train Epoch: 194 	 Loss: 15.017406
Train Epoch: 195 	 Loss: 14.827100
Train Epoch: 196 	 Loss: 14.802766
Train Epoch: 197 	 Loss: 14.729427
Train Epoch: 198 	 Loss: 14.699383
Train Epoch: 199 	 Loss: 14.660112

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.922

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.616

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.928

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.889

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.936

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.927

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.928

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.919

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.931

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.920

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.894

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.923
------
f1 mean across methods is 0.895


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 200
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.35
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=200_undersam_rate=0.35_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.72214835e-01 4.05346420e-01 1.74357935e-02 4.77635792e-03
 2.26593759e-04] 

Train Epoch: 0 	 Loss: 20.744898
Train Epoch: 1 	 Loss: 19.036657
Train Epoch: 2 	 Loss: 17.530613
Train Epoch: 3 	 Loss: 15.498290
Train Epoch: 4 	 Loss: 15.409888
Train Epoch: 5 	 Loss: 15.428305
Train Epoch: 6 	 Loss: 15.014761
Train Epoch: 7 	 Loss: 15.538243
Train Epoch: 8 	 Loss: 15.642530
Train Epoch: 9 	 Loss: 14.632871
Train Epoch: 10 	 Loss: 15.181849
Train Epoch: 11 	 Loss: 14.846539
Train Epoch: 12 	 Loss: 15.084074
Train Epoch: 13 	 Loss: 15.158180
Train Epoch: 14 	 Loss: 15.055910
Train Epoch: 15 	 Loss: 15.950056
Train Epoch: 16 	 Loss: 14.755182
Train Epoch: 17 	 Loss: 14.795451
Train Epoch: 18 	 Loss: 15.058833
Train Epoch: 19 	 Loss: 14.568853
Train Epoch: 20 	 Loss: 14.951772
Train Epoch: 21 	 Loss: 14.973666
Train Epoch: 22 	 Loss: 15.134608
Train Epoch: 23 	 Loss: 14.506934
Train Epoch: 24 	 Loss: 14.781660
Train Epoch: 25 	 Loss: 14.896180
Train Epoch: 26 	 Loss: 13.996582
Train Epoch: 27 	 Loss: 14.956184
Train Epoch: 28 	 Loss: 14.624336
Train Epoch: 29 	 Loss: 13.878693
Train Epoch: 30 	 Loss: 14.484542
Train Epoch: 31 	 Loss: 15.078803
Train Epoch: 32 	 Loss: 14.736877
Train Epoch: 33 	 Loss: 15.324893
Train Epoch: 34 	 Loss: 14.427126
Train Epoch: 35 	 Loss: 14.738239
Train Epoch: 36 	 Loss: 15.185484
Train Epoch: 37 	 Loss: 14.764756
Train Epoch: 38 	 Loss: 15.486373
Train Epoch: 39 	 Loss: 14.453798
Train Epoch: 40 	 Loss: 14.227190
Train Epoch: 41 	 Loss: 13.975235
Train Epoch: 42 	 Loss: 14.433557
Train Epoch: 43 	 Loss: 14.454723
Train Epoch: 44 	 Loss: 14.295990
Train Epoch: 45 	 Loss: 15.333370
Train Epoch: 46 	 Loss: 15.656125
Train Epoch: 47 	 Loss: 14.292118
Train Epoch: 48 	 Loss: 14.539593
Train Epoch: 49 	 Loss: 13.898601
Train Epoch: 50 	 Loss: 13.968166
Train Epoch: 51 	 Loss: 15.427683
Train Epoch: 52 	 Loss: 14.044149
Train Epoch: 53 	 Loss: 14.075256
Train Epoch: 54 	 Loss: 14.204966
Train Epoch: 55 	 Loss: 14.458616
Train Epoch: 56 	 Loss: 14.180717
Train Epoch: 57 	 Loss: 14.485140
Train Epoch: 58 	 Loss: 15.021317
Train Epoch: 59 	 Loss: 15.051423
Train Epoch: 60 	 Loss: 14.309690
Train Epoch: 61 	 Loss: 14.940046
Train Epoch: 62 	 Loss: 14.124626
Train Epoch: 63 	 Loss: 13.933364
Train Epoch: 64 	 Loss: 15.262458
Train Epoch: 65 	 Loss: 14.071338
Train Epoch: 66 	 Loss: 14.159302
Train Epoch: 67 	 Loss: 14.158216
Train Epoch: 68 	 Loss: 14.338580
Train Epoch: 69 	 Loss: 14.325208
Train Epoch: 70 	 Loss: 15.104736
Train Epoch: 71 	 Loss: 15.035327
Train Epoch: 72 	 Loss: 14.299240
Train Epoch: 73 	 Loss: 14.244763
Train Epoch: 74 	 Loss: 15.280848
Train Epoch: 75 	 Loss: 14.261805
Train Epoch: 76 	 Loss: 14.725405
Train Epoch: 77 	 Loss: 14.214088
Train Epoch: 78 	 Loss: 13.898727
Train Epoch: 79 	 Loss: 13.939221
Train Epoch: 80 	 Loss: 14.198459
Train Epoch: 81 	 Loss: 14.182533
Train Epoch: 82 	 Loss: 14.084976
Train Epoch: 83 	 Loss: 14.287973
Train Epoch: 84 	 Loss: 14.281700
Train Epoch: 85 	 Loss: 14.172598
Train Epoch: 86 	 Loss: 14.157321
Train Epoch: 87 	 Loss: 14.061914
Train Epoch: 88 	 Loss: 13.924841
Train Epoch: 89 	 Loss: 13.925928
Train Epoch: 90 	 Loss: 14.062384
Train Epoch: 91 	 Loss: 13.803923
Train Epoch: 92 	 Loss: 14.160720
Train Epoch: 93 	 Loss: 13.834564
Train Epoch: 94 	 Loss: 13.982334
Train Epoch: 95 	 Loss: 14.311457
Train Epoch: 96 	 Loss: 13.990929
Train Epoch: 97 	 Loss: 14.331103
Train Epoch: 98 	 Loss: 14.274405
Train Epoch: 99 	 Loss: 13.947232
Train Epoch: 100 	 Loss: 14.049343
Train Epoch: 101 	 Loss: 13.884634
Train Epoch: 102 	 Loss: 14.017033
Train Epoch: 103 	 Loss: 14.237420
Train Epoch: 104 	 Loss: 13.906158
Train Epoch: 105 	 Loss: 13.865571
Train Epoch: 106 	 Loss: 14.027121
Train Epoch: 107 	 Loss: 14.025146
Train Epoch: 108 	 Loss: 14.260366
Train Epoch: 109 	 Loss: 14.148581
Train Epoch: 110 	 Loss: 14.080402
Train Epoch: 111 	 Loss: 14.489050
Train Epoch: 112 	 Loss: 14.125016
Train Epoch: 113 	 Loss: 14.209949
Train Epoch: 114 	 Loss: 14.692784
Train Epoch: 115 	 Loss: 14.352649
Train Epoch: 116 	 Loss: 14.120117
Train Epoch: 117 	 Loss: 14.675446
Train Epoch: 118 	 Loss: 13.805063
Train Epoch: 119 	 Loss: 14.618005
Train Epoch: 120 	 Loss: 13.912215
Train Epoch: 121 	 Loss: 14.159048
Train Epoch: 122 	 Loss: 14.079185
Train Epoch: 123 	 Loss: 14.193392
Train Epoch: 124 	 Loss: 13.627451
Train Epoch: 125 	 Loss: 14.046440
Train Epoch: 126 	 Loss: 13.753044
Train Epoch: 127 	 Loss: 13.705625
Train Epoch: 128 	 Loss: 13.781115
Train Epoch: 129 	 Loss: 13.529457
Train Epoch: 130 	 Loss: 13.626893
Train Epoch: 131 	 Loss: 13.806520
Train Epoch: 132 	 Loss: 13.848294
Train Epoch: 133 	 Loss: 14.045944
Train Epoch: 134 	 Loss: 13.768641
Train Epoch: 135 	 Loss: 13.449854
Train Epoch: 136 	 Loss: 14.010665
Train Epoch: 137 	 Loss: 14.118705
Train Epoch: 138 	 Loss: 13.887600
Train Epoch: 139 	 Loss: 13.608000
Train Epoch: 140 	 Loss: 14.064644
Train Epoch: 141 	 Loss: 14.030014
Train Epoch: 142 	 Loss: 13.796934
Train Epoch: 143 	 Loss: 13.869329
Train Epoch: 144 	 Loss: 13.438468
Train Epoch: 145 	 Loss: 13.554390
Train Epoch: 146 	 Loss: 13.765467
Train Epoch: 147 	 Loss: 13.749025
Train Epoch: 148 	 Loss: 13.770557
Train Epoch: 149 	 Loss: 13.598562
Train Epoch: 150 	 Loss: 13.735950
Train Epoch: 151 	 Loss: 13.587278
Train Epoch: 152 	 Loss: 13.550746
Train Epoch: 153 	 Loss: 13.813974
Train Epoch: 154 	 Loss: 13.809509
Train Epoch: 155 	 Loss: 13.740498
Train Epoch: 156 	 Loss: 13.694412
Train Epoch: 157 	 Loss: 13.603394
Train Epoch: 158 	 Loss: 13.884315
Train Epoch: 159 	 Loss: 13.660182
Train Epoch: 160 	 Loss: 13.981421
Train Epoch: 161 	 Loss: 13.765598
Train Epoch: 162 	 Loss: 13.853284
Train Epoch: 163 	 Loss: 13.563597
Train Epoch: 164 	 Loss: 13.915031
Train Epoch: 165 	 Loss: 13.565681
Train Epoch: 166 	 Loss: 13.837785
Train Epoch: 167 	 Loss: 13.977694
Train Epoch: 168 	 Loss: 13.447920
Train Epoch: 169 	 Loss: 14.077004
Train Epoch: 170 	 Loss: 13.550876
Train Epoch: 171 	 Loss: 13.670496
Train Epoch: 172 	 Loss: 13.675297
Train Epoch: 173 	 Loss: 13.921943
Train Epoch: 174 	 Loss: 13.552899
Train Epoch: 175 	 Loss: 13.830513
Train Epoch: 176 	 Loss: 13.522425
Train Epoch: 177 	 Loss: 13.542419
Train Epoch: 178 	 Loss: 14.764603
Train Epoch: 179 	 Loss: 13.395341
Train Epoch: 180 	 Loss: 13.821709
Train Epoch: 181 	 Loss: 13.506846
Train Epoch: 182 	 Loss: 13.965265
Train Epoch: 183 	 Loss: 13.528170
Train Epoch: 184 	 Loss: 13.650824
Train Epoch: 185 	 Loss: 13.591614
Train Epoch: 186 	 Loss: 13.454500
Train Epoch: 187 	 Loss: 13.705523
Train Epoch: 188 	 Loss: 13.514182
Train Epoch: 189 	 Loss: 13.576565
Train Epoch: 190 	 Loss: 13.600548
Train Epoch: 191 	 Loss: 13.583921
Train Epoch: 192 	 Loss: 13.553310
Train Epoch: 193 	 Loss: 13.556234
Train Epoch: 194 	 Loss: 13.885861
Train Epoch: 195 	 Loss: 14.039778
Train Epoch: 196 	 Loss: 13.500389
Train Epoch: 197 	 Loss: 13.594393
Train Epoch: 198 	 Loss: 14.875080
Train Epoch: 199 	 Loss: 13.856334

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.916

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.622

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.919

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.876

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.920

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.910

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.941

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.911

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.923

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.902

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.859

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.921
------
f1 mean across methods is 0.885


 ---------------------------------------- 


For each of the methods
Average F1:  [0.91931724 0.57301372 0.87264096 0.88295833 0.92843018 0.84903571
 0.93632338 0.91009815 0.92651472 0.90976061 0.86841872 0.91876604]
Std F1:  [0.01006931 0.03744901 0.04567094 0.00722545 0.02083385 0.09477865
 0.01581347 0.00947959 0.00682955 0.00627032 0.02504726 0.02853485]
Average over repetitions across all methods
Average f1 score:  0.8746064800046648
Std F1:  0.014294777096669917

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 400, 'order_hermite': 100, 'subsampled_rate': 0.25} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 400
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.87498040e-01 4.86150907e-01 2.04208185e-02 5.70927597e-03
 2.20958246e-04] 

Train Epoch: 0 	 Loss: 17.737362
Train Epoch: 1 	 Loss: 17.945063
Train Epoch: 2 	 Loss: 16.445370
Train Epoch: 3 	 Loss: 15.162182
Train Epoch: 4 	 Loss: 14.039949
Train Epoch: 5 	 Loss: 14.247274
Train Epoch: 6 	 Loss: 13.647343
Train Epoch: 7 	 Loss: 13.832807
Train Epoch: 8 	 Loss: 13.362582
Train Epoch: 9 	 Loss: 13.339866
Train Epoch: 10 	 Loss: 13.998705
Train Epoch: 11 	 Loss: 13.172113
Train Epoch: 12 	 Loss: 12.939022
Train Epoch: 13 	 Loss: 12.919922
Train Epoch: 14 	 Loss: 12.919301
Train Epoch: 15 	 Loss: 12.592813
Train Epoch: 16 	 Loss: 12.829882
Train Epoch: 17 	 Loss: 13.262189
Train Epoch: 18 	 Loss: 12.847476
Train Epoch: 19 	 Loss: 12.754618
Train Epoch: 20 	 Loss: 14.166563
Train Epoch: 21 	 Loss: 12.792448
Train Epoch: 22 	 Loss: 12.867750
Train Epoch: 23 	 Loss: 13.536143
Train Epoch: 24 	 Loss: 13.357288
Train Epoch: 25 	 Loss: 12.796778
Train Epoch: 26 	 Loss: 12.591001
Train Epoch: 27 	 Loss: 12.485989
Train Epoch: 28 	 Loss: 13.117361
Train Epoch: 29 	 Loss: 12.985290
Train Epoch: 30 	 Loss: 12.478205
Train Epoch: 31 	 Loss: 12.720708
Train Epoch: 32 	 Loss: 12.308969
Train Epoch: 33 	 Loss: 13.374571
Train Epoch: 34 	 Loss: 12.459048
Train Epoch: 35 	 Loss: 12.328646
Train Epoch: 36 	 Loss: 12.066224
Train Epoch: 37 	 Loss: 12.808018
Train Epoch: 38 	 Loss: 12.667778
Train Epoch: 39 	 Loss: 12.371983
Train Epoch: 40 	 Loss: 12.598587
Train Epoch: 41 	 Loss: 12.664036
Train Epoch: 42 	 Loss: 12.511983
Train Epoch: 43 	 Loss: 12.310659
Train Epoch: 44 	 Loss: 12.726738
Train Epoch: 45 	 Loss: 12.309564
Train Epoch: 46 	 Loss: 13.049590
Train Epoch: 47 	 Loss: 12.337273
Train Epoch: 48 	 Loss: 12.586590
Train Epoch: 49 	 Loss: 12.078577
Train Epoch: 50 	 Loss: 12.417296
Train Epoch: 51 	 Loss: 12.115498
Train Epoch: 52 	 Loss: 12.255963
Train Epoch: 53 	 Loss: 12.211741
Train Epoch: 54 	 Loss: 12.158474
Train Epoch: 55 	 Loss: 12.126038
Train Epoch: 56 	 Loss: 12.202429
Train Epoch: 57 	 Loss: 12.206515
Train Epoch: 58 	 Loss: 12.198179
Train Epoch: 59 	 Loss: 13.305700
Train Epoch: 60 	 Loss: 12.648502
Train Epoch: 61 	 Loss: 12.485783
Train Epoch: 62 	 Loss: 12.004207
Train Epoch: 63 	 Loss: 12.175410
Train Epoch: 64 	 Loss: 12.013895
Train Epoch: 65 	 Loss: 13.386803
Train Epoch: 66 	 Loss: 12.343159
Train Epoch: 67 	 Loss: 12.146932
Train Epoch: 68 	 Loss: 12.064672
Train Epoch: 69 	 Loss: 12.134915
Train Epoch: 70 	 Loss: 12.147707
Train Epoch: 71 	 Loss: 12.180624
Train Epoch: 72 	 Loss: 12.065690
Train Epoch: 73 	 Loss: 11.970022
Train Epoch: 74 	 Loss: 12.092830
Train Epoch: 75 	 Loss: 12.966806
Train Epoch: 76 	 Loss: 12.305370
Train Epoch: 77 	 Loss: 12.288342
Train Epoch: 78 	 Loss: 12.818512
Train Epoch: 79 	 Loss: 12.317587
Train Epoch: 80 	 Loss: 12.177641
Train Epoch: 81 	 Loss: 11.990667
Train Epoch: 82 	 Loss: 12.308113
Train Epoch: 83 	 Loss: 12.219685
Train Epoch: 84 	 Loss: 11.940657
Train Epoch: 85 	 Loss: 12.234973
Train Epoch: 86 	 Loss: 12.035643
Train Epoch: 87 	 Loss: 12.331810
Train Epoch: 88 	 Loss: 12.056469
Train Epoch: 89 	 Loss: 12.093348
Train Epoch: 90 	 Loss: 11.973351
Train Epoch: 91 	 Loss: 12.508033
Train Epoch: 92 	 Loss: 11.893221
Train Epoch: 93 	 Loss: 12.527834
Train Epoch: 94 	 Loss: 12.431372
Train Epoch: 95 	 Loss: 12.520015
Train Epoch: 96 	 Loss: 12.063894
Train Epoch: 97 	 Loss: 12.223160
Train Epoch: 98 	 Loss: 12.025742
Train Epoch: 99 	 Loss: 12.117012
Train Epoch: 100 	 Loss: 12.074857
Train Epoch: 101 	 Loss: 12.291002
Train Epoch: 102 	 Loss: 12.202925
Train Epoch: 103 	 Loss: 12.128438
Train Epoch: 104 	 Loss: 12.094209
Train Epoch: 105 	 Loss: 12.010029
Train Epoch: 106 	 Loss: 11.931852
Train Epoch: 107 	 Loss: 12.216541
Train Epoch: 108 	 Loss: 12.095836
Train Epoch: 109 	 Loss: 12.144690
Train Epoch: 110 	 Loss: 12.032450
Train Epoch: 111 	 Loss: 11.942755
Train Epoch: 112 	 Loss: 12.213165
Train Epoch: 113 	 Loss: 11.981525
Train Epoch: 114 	 Loss: 12.255250
Train Epoch: 115 	 Loss: 12.344917
Train Epoch: 116 	 Loss: 12.328959
Train Epoch: 117 	 Loss: 12.345531
Train Epoch: 118 	 Loss: 12.397188
Train Epoch: 119 	 Loss: 12.400126
Train Epoch: 120 	 Loss: 12.011583
Train Epoch: 121 	 Loss: 12.418036
Train Epoch: 122 	 Loss: 12.174908
Train Epoch: 123 	 Loss: 12.132449
Train Epoch: 124 	 Loss: 12.202509
Train Epoch: 125 	 Loss: 12.132332
Train Epoch: 126 	 Loss: 11.994426
Train Epoch: 127 	 Loss: 12.082033
Train Epoch: 128 	 Loss: 12.024447
Train Epoch: 129 	 Loss: 11.991394
Train Epoch: 130 	 Loss: 11.951166
Train Epoch: 131 	 Loss: 12.726315
Train Epoch: 132 	 Loss: 12.288570
Train Epoch: 133 	 Loss: 11.926167
Train Epoch: 134 	 Loss: 12.353206
Train Epoch: 135 	 Loss: 12.527697
Train Epoch: 136 	 Loss: 12.044032
Train Epoch: 137 	 Loss: 11.755116
Train Epoch: 138 	 Loss: 12.116476
Train Epoch: 139 	 Loss: 12.204332
Train Epoch: 140 	 Loss: 12.168596
Train Epoch: 141 	 Loss: 12.123337
Train Epoch: 142 	 Loss: 11.888526
Train Epoch: 143 	 Loss: 12.059179
Train Epoch: 144 	 Loss: 11.923810
Train Epoch: 145 	 Loss: 12.035076
Train Epoch: 146 	 Loss: 11.948473
Train Epoch: 147 	 Loss: 11.936658
Train Epoch: 148 	 Loss: 12.293528
Train Epoch: 149 	 Loss: 12.043421
Train Epoch: 150 	 Loss: 12.463173
Train Epoch: 151 	 Loss: 12.134193
Train Epoch: 152 	 Loss: 12.287492
Train Epoch: 153 	 Loss: 12.013377
Train Epoch: 154 	 Loss: 11.954681
Train Epoch: 155 	 Loss: 11.932350
Train Epoch: 156 	 Loss: 11.952996
Train Epoch: 157 	 Loss: 11.874614
Train Epoch: 158 	 Loss: 11.944097
Train Epoch: 159 	 Loss: 12.133493
Train Epoch: 160 	 Loss: 12.169521
Train Epoch: 161 	 Loss: 11.968460
Train Epoch: 162 	 Loss: 12.259909
Train Epoch: 163 	 Loss: 12.204641
Train Epoch: 164 	 Loss: 12.067858
Train Epoch: 165 	 Loss: 12.289988
Train Epoch: 166 	 Loss: 12.103746
Train Epoch: 167 	 Loss: 12.177595
Train Epoch: 168 	 Loss: 12.585685
Train Epoch: 169 	 Loss: 12.160559
Train Epoch: 170 	 Loss: 11.829620
Train Epoch: 171 	 Loss: 12.136564
Train Epoch: 172 	 Loss: 12.009234
Train Epoch: 173 	 Loss: 12.227413
Train Epoch: 174 	 Loss: 11.854533
Train Epoch: 175 	 Loss: 11.934760
Train Epoch: 176 	 Loss: 11.839584
Train Epoch: 177 	 Loss: 12.096161
Train Epoch: 178 	 Loss: 11.828569
Train Epoch: 179 	 Loss: 11.963352
Train Epoch: 180 	 Loss: 11.988207
Train Epoch: 181 	 Loss: 13.213192
Train Epoch: 182 	 Loss: 11.864714
Train Epoch: 183 	 Loss: 12.022074
Train Epoch: 184 	 Loss: 12.113488
Train Epoch: 185 	 Loss: 11.974881
Train Epoch: 186 	 Loss: 12.082911
Train Epoch: 187 	 Loss: 11.845198
Train Epoch: 188 	 Loss: 12.262294
Train Epoch: 189 	 Loss: 12.089853
Train Epoch: 190 	 Loss: 11.924213
Train Epoch: 191 	 Loss: 13.031326
Train Epoch: 192 	 Loss: 12.399064
Train Epoch: 193 	 Loss: 11.878341
Train Epoch: 194 	 Loss: 12.328841
Train Epoch: 195 	 Loss: 12.482582
Train Epoch: 196 	 Loss: 12.385817
Train Epoch: 197 	 Loss: 12.010613
Train Epoch: 198 	 Loss: 12.139465
Train Epoch: 199 	 Loss: 11.992531
Train Epoch: 200 	 Loss: 11.790581
Train Epoch: 201 	 Loss: 12.053795
Train Epoch: 202 	 Loss: 12.059474
Train Epoch: 203 	 Loss: 11.830486
Train Epoch: 204 	 Loss: 11.967776
Train Epoch: 205 	 Loss: 12.036312
Train Epoch: 206 	 Loss: 11.923831
Train Epoch: 207 	 Loss: 12.849292
Train Epoch: 208 	 Loss: 12.121494
Train Epoch: 209 	 Loss: 12.056726
Train Epoch: 210 	 Loss: 12.140592
Train Epoch: 211 	 Loss: 12.044342
Train Epoch: 212 	 Loss: 11.808044
Train Epoch: 213 	 Loss: 11.849391
Train Epoch: 214 	 Loss: 11.938770
Train Epoch: 215 	 Loss: 12.031960
Train Epoch: 216 	 Loss: 11.938090
Train Epoch: 217 	 Loss: 11.909043
Train Epoch: 218 	 Loss: 12.040944
Train Epoch: 219 	 Loss: 12.014961
Train Epoch: 220 	 Loss: 12.306215
Train Epoch: 221 	 Loss: 12.029111
Train Epoch: 222 	 Loss: 11.821357
Train Epoch: 223 	 Loss: 12.293887
Train Epoch: 224 	 Loss: 11.845900
Train Epoch: 225 	 Loss: 12.179757
Train Epoch: 226 	 Loss: 11.971443
Train Epoch: 227 	 Loss: 11.859180
Train Epoch: 228 	 Loss: 12.246218
Train Epoch: 229 	 Loss: 12.120846
Train Epoch: 230 	 Loss: 12.206897
Train Epoch: 231 	 Loss: 12.224148
Train Epoch: 232 	 Loss: 12.334351
Train Epoch: 233 	 Loss: 11.855852
Train Epoch: 234 	 Loss: 12.114950
Train Epoch: 235 	 Loss: 12.437311
Train Epoch: 236 	 Loss: 12.138443
Train Epoch: 237 	 Loss: 11.965052
Train Epoch: 238 	 Loss: 12.280433
Train Epoch: 239 	 Loss: 11.838902
Train Epoch: 240 	 Loss: 11.927986
Train Epoch: 241 	 Loss: 12.026058
Train Epoch: 242 	 Loss: 11.969666
Train Epoch: 243 	 Loss: 12.007925
Train Epoch: 244 	 Loss: 11.935453
Train Epoch: 245 	 Loss: 12.142311
Train Epoch: 246 	 Loss: 12.094631
Train Epoch: 247 	 Loss: 12.158595
Train Epoch: 248 	 Loss: 12.080793
Train Epoch: 249 	 Loss: 11.835061
Train Epoch: 250 	 Loss: 12.028920
Train Epoch: 251 	 Loss: 12.322130
Train Epoch: 252 	 Loss: 11.846329
Train Epoch: 253 	 Loss: 11.935973
Train Epoch: 254 	 Loss: 12.424208
Train Epoch: 255 	 Loss: 11.945330
Train Epoch: 256 	 Loss: 11.898962
Train Epoch: 257 	 Loss: 12.107512
Train Epoch: 258 	 Loss: 11.903709
Train Epoch: 259 	 Loss: 12.378632
Train Epoch: 260 	 Loss: 11.870448
Train Epoch: 261 	 Loss: 11.906634
Train Epoch: 262 	 Loss: 11.808187
Train Epoch: 263 	 Loss: 12.207477
Train Epoch: 264 	 Loss: 12.225263
Train Epoch: 265 	 Loss: 12.129524
Train Epoch: 266 	 Loss: 11.985914
Train Epoch: 267 	 Loss: 12.139120
Train Epoch: 268 	 Loss: 11.853476
Train Epoch: 269 	 Loss: 12.401639
Train Epoch: 270 	 Loss: 11.789133
Train Epoch: 271 	 Loss: 11.929443
Train Epoch: 272 	 Loss: 12.133809
Train Epoch: 273 	 Loss: 12.076221
Train Epoch: 274 	 Loss: 12.384447
Train Epoch: 275 	 Loss: 11.918458
Train Epoch: 276 	 Loss: 11.862966
Train Epoch: 277 	 Loss: 12.282559
Train Epoch: 278 	 Loss: 11.847386
Train Epoch: 279 	 Loss: 12.083057
Train Epoch: 280 	 Loss: 12.323763
Train Epoch: 281 	 Loss: 12.105309
Train Epoch: 282 	 Loss: 11.923135
Train Epoch: 283 	 Loss: 12.029061
Train Epoch: 284 	 Loss: 11.825436
Train Epoch: 285 	 Loss: 12.136166
Train Epoch: 286 	 Loss: 11.866211
Train Epoch: 287 	 Loss: 11.886972
Train Epoch: 288 	 Loss: 11.890970
Train Epoch: 289 	 Loss: 11.968443
Train Epoch: 290 	 Loss: 12.131603
Train Epoch: 291 	 Loss: 12.095898
Train Epoch: 292 	 Loss: 12.297065
Train Epoch: 293 	 Loss: 11.965763
Train Epoch: 294 	 Loss: 11.952055
Train Epoch: 295 	 Loss: 11.938468
Train Epoch: 296 	 Loss: 11.939989
Train Epoch: 297 	 Loss: 12.189819
Train Epoch: 298 	 Loss: 12.133691
Train Epoch: 299 	 Loss: 11.785631
Train Epoch: 300 	 Loss: 11.778078
Train Epoch: 301 	 Loss: 11.863337
Train Epoch: 302 	 Loss: 12.372108
Train Epoch: 303 	 Loss: 13.197191
Train Epoch: 304 	 Loss: 12.129419
Train Epoch: 305 	 Loss: 12.157096
Train Epoch: 306 	 Loss: 12.067646
Train Epoch: 307 	 Loss: 12.089537
Train Epoch: 308 	 Loss: 11.878412
Train Epoch: 309 	 Loss: 12.092867
Train Epoch: 310 	 Loss: 11.925774
Train Epoch: 311 	 Loss: 11.918031
Train Epoch: 312 	 Loss: 11.884370
Train Epoch: 313 	 Loss: 12.069671
Train Epoch: 314 	 Loss: 12.123686
Train Epoch: 315 	 Loss: 12.331126
Train Epoch: 316 	 Loss: 12.152442
Train Epoch: 317 	 Loss: 11.886080
Train Epoch: 318 	 Loss: 12.377367
Train Epoch: 319 	 Loss: 12.374586
Train Epoch: 320 	 Loss: 12.481047
Train Epoch: 321 	 Loss: 11.814945
Train Epoch: 322 	 Loss: 11.881020
Train Epoch: 323 	 Loss: 11.820895
Train Epoch: 324 	 Loss: 12.020954
Train Epoch: 325 	 Loss: 11.905993
Train Epoch: 326 	 Loss: 11.835626
Train Epoch: 327 	 Loss: 11.995568
Train Epoch: 328 	 Loss: 11.980115
Train Epoch: 329 	 Loss: 12.158053
Train Epoch: 330 	 Loss: 12.571424
Train Epoch: 331 	 Loss: 12.449636
Train Epoch: 332 	 Loss: 12.258673
Train Epoch: 333 	 Loss: 12.137177
Train Epoch: 334 	 Loss: 11.864893
Train Epoch: 335 	 Loss: 12.372717
Train Epoch: 336 	 Loss: 12.250247
Train Epoch: 337 	 Loss: 12.447935
Train Epoch: 338 	 Loss: 12.088114
Train Epoch: 339 	 Loss: 11.954035
Train Epoch: 340 	 Loss: 12.175156
Train Epoch: 341 	 Loss: 12.232546
Train Epoch: 342 	 Loss: 12.341727
Train Epoch: 343 	 Loss: 11.896921
Train Epoch: 344 	 Loss: 11.833164
Train Epoch: 345 	 Loss: 11.786810
Train Epoch: 346 	 Loss: 12.084064
Train Epoch: 347 	 Loss: 12.085197
Train Epoch: 348 	 Loss: 11.835155
Train Epoch: 349 	 Loss: 11.885815
Train Epoch: 350 	 Loss: 11.789645
Train Epoch: 351 	 Loss: 11.981995
Train Epoch: 352 	 Loss: 12.582192
Train Epoch: 353 	 Loss: 11.877301
Train Epoch: 354 	 Loss: 11.916800
Train Epoch: 355 	 Loss: 11.949710
Train Epoch: 356 	 Loss: 11.992500
Train Epoch: 357 	 Loss: 12.330376
Train Epoch: 358 	 Loss: 11.907433
Train Epoch: 359 	 Loss: 12.106135
Train Epoch: 360 	 Loss: 11.898880
Train Epoch: 361 	 Loss: 11.814708
Train Epoch: 362 	 Loss: 12.651199
Train Epoch: 363 	 Loss: 11.877415
Train Epoch: 364 	 Loss: 11.821364
Train Epoch: 365 	 Loss: 12.078770
Train Epoch: 366 	 Loss: 12.195906
Train Epoch: 367 	 Loss: 11.873041
Train Epoch: 368 	 Loss: 11.810028
Train Epoch: 369 	 Loss: 12.485413
Train Epoch: 370 	 Loss: 12.047119
Train Epoch: 371 	 Loss: 12.356398
Train Epoch: 372 	 Loss: 12.677011
Train Epoch: 373 	 Loss: 12.931683
Train Epoch: 374 	 Loss: 12.617555
Train Epoch: 375 	 Loss: 12.543038
Train Epoch: 376 	 Loss: 12.683359
Train Epoch: 377 	 Loss: 12.292358
Train Epoch: 378 	 Loss: 12.423316
Train Epoch: 379 	 Loss: 12.379043
Train Epoch: 380 	 Loss: 12.365031
Train Epoch: 381 	 Loss: 13.263868
Train Epoch: 382 	 Loss: 12.382614
Train Epoch: 383 	 Loss: 12.511049
Train Epoch: 384 	 Loss: 12.632327
Train Epoch: 385 	 Loss: 12.375717
Train Epoch: 386 	 Loss: 12.288759
Train Epoch: 387 	 Loss: 12.473006
Train Epoch: 388 	 Loss: 12.254950
Train Epoch: 389 	 Loss: 12.355049
Train Epoch: 390 	 Loss: 12.634748
Train Epoch: 391 	 Loss: 12.590371
Train Epoch: 392 	 Loss: 12.389016
Train Epoch: 393 	 Loss: 12.282412
Train Epoch: 394 	 Loss: 12.375643
Train Epoch: 395 	 Loss: 12.391245
Train Epoch: 396 	 Loss: 12.380369
Train Epoch: 397 	 Loss: 13.435133
Train Epoch: 398 	 Loss: 12.892547
Train Epoch: 399 	 Loss: 12.768723

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.882

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.640

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.912

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.876

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.870

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.298

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.904

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.877

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.899

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.885

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.863

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.901
------
f1 mean across methods is 0.817


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 400
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.88460277e-01 4.84981967e-01 2.07059260e-02 5.57384995e-03
 2.77979729e-04] 

Train Epoch: 0 	 Loss: 18.824202
Train Epoch: 1 	 Loss: 17.779896
Train Epoch: 2 	 Loss: 16.136246
Train Epoch: 3 	 Loss: 14.885567
Train Epoch: 4 	 Loss: 14.597428
Train Epoch: 5 	 Loss: 14.175176
Train Epoch: 6 	 Loss: 13.761894
Train Epoch: 7 	 Loss: 13.899776
Train Epoch: 8 	 Loss: 13.614605
Train Epoch: 9 	 Loss: 13.896452
Train Epoch: 10 	 Loss: 13.439377
Train Epoch: 11 	 Loss: 13.583275
Train Epoch: 12 	 Loss: 14.187655
Train Epoch: 13 	 Loss: 13.428287
Train Epoch: 14 	 Loss: 13.704464
Train Epoch: 15 	 Loss: 13.564213
Train Epoch: 16 	 Loss: 13.274059
Train Epoch: 17 	 Loss: 13.588781
Train Epoch: 18 	 Loss: 13.474252
Train Epoch: 19 	 Loss: 13.265942
Train Epoch: 20 	 Loss: 13.485503
Train Epoch: 21 	 Loss: 13.352444
Train Epoch: 22 	 Loss: 13.703955
Train Epoch: 23 	 Loss: 13.658596
Train Epoch: 24 	 Loss: 13.422300
Train Epoch: 25 	 Loss: 13.571175
Train Epoch: 26 	 Loss: 13.372771
Train Epoch: 27 	 Loss: 13.381610
Train Epoch: 28 	 Loss: 13.240341
Train Epoch: 29 	 Loss: 13.240440
Train Epoch: 30 	 Loss: 13.275658
Train Epoch: 31 	 Loss: 13.356132
Train Epoch: 32 	 Loss: 13.297145
Train Epoch: 33 	 Loss: 13.336887
Train Epoch: 34 	 Loss: 13.176908
Train Epoch: 35 	 Loss: 13.662167
Train Epoch: 36 	 Loss: 13.522073
Train Epoch: 37 	 Loss: 13.521326
Train Epoch: 38 	 Loss: 13.148903
Train Epoch: 39 	 Loss: 13.274250
Train Epoch: 40 	 Loss: 13.418147
Train Epoch: 41 	 Loss: 14.011224
Train Epoch: 42 	 Loss: 13.311273
Train Epoch: 43 	 Loss: 13.443177
Train Epoch: 44 	 Loss: 13.508139
Train Epoch: 45 	 Loss: 13.554741
Train Epoch: 46 	 Loss: 13.362629
Train Epoch: 47 	 Loss: 13.269212
Train Epoch: 48 	 Loss: 13.291419
Train Epoch: 49 	 Loss: 13.217803
Train Epoch: 50 	 Loss: 13.352777
Train Epoch: 51 	 Loss: 14.352934
Train Epoch: 52 	 Loss: 13.426423
Train Epoch: 53 	 Loss: 13.413052
Train Epoch: 54 	 Loss: 13.267849
Train Epoch: 55 	 Loss: 13.143746
Train Epoch: 56 	 Loss: 13.291634
Train Epoch: 57 	 Loss: 13.234351
Train Epoch: 58 	 Loss: 13.300463
Train Epoch: 59 	 Loss: 13.230921
Train Epoch: 60 	 Loss: 13.180066
Train Epoch: 61 	 Loss: 13.239577
Train Epoch: 62 	 Loss: 13.556633
Train Epoch: 63 	 Loss: 13.068089
Train Epoch: 64 	 Loss: 13.347183
Train Epoch: 65 	 Loss: 13.677377
Train Epoch: 66 	 Loss: 13.221146
Train Epoch: 67 	 Loss: 13.361928
Train Epoch: 68 	 Loss: 13.343252
Train Epoch: 69 	 Loss: 13.622006
Train Epoch: 70 	 Loss: 13.486138
Train Epoch: 71 	 Loss: 13.194500
Train Epoch: 72 	 Loss: 13.186457
Train Epoch: 73 	 Loss: 13.175321
Train Epoch: 74 	 Loss: 13.394556
Train Epoch: 75 	 Loss: 13.254551
Train Epoch: 76 	 Loss: 13.067031
Train Epoch: 77 	 Loss: 13.571513
Train Epoch: 78 	 Loss: 13.356611
Train Epoch: 79 	 Loss: 13.229170
Train Epoch: 80 	 Loss: 13.221100
Train Epoch: 81 	 Loss: 13.061024
Train Epoch: 82 	 Loss: 13.131823
Train Epoch: 83 	 Loss: 13.191670
Train Epoch: 84 	 Loss: 13.095011
Train Epoch: 85 	 Loss: 12.977149
Train Epoch: 86 	 Loss: 13.456166
Train Epoch: 87 	 Loss: 13.429373
Train Epoch: 88 	 Loss: 12.996689
Train Epoch: 89 	 Loss: 13.557207
Train Epoch: 90 	 Loss: 13.407630
Train Epoch: 91 	 Loss: 13.114738
Train Epoch: 92 	 Loss: 13.114542
Train Epoch: 93 	 Loss: 13.043066
Train Epoch: 94 	 Loss: 13.082356
Train Epoch: 95 	 Loss: 13.160553
Train Epoch: 96 	 Loss: 13.764344
Train Epoch: 97 	 Loss: 13.010613
Train Epoch: 98 	 Loss: 13.244015
Train Epoch: 99 	 Loss: 13.357695
Train Epoch: 100 	 Loss: 13.209939
Train Epoch: 101 	 Loss: 13.218252
Train Epoch: 102 	 Loss: 13.207603
Train Epoch: 103 	 Loss: 13.377155
Train Epoch: 104 	 Loss: 13.341654
Train Epoch: 105 	 Loss: 13.402700
Train Epoch: 106 	 Loss: 12.997871
Train Epoch: 107 	 Loss: 13.270588
Train Epoch: 108 	 Loss: 13.145416
Train Epoch: 109 	 Loss: 13.032539
Train Epoch: 110 	 Loss: 14.083731
Train Epoch: 111 	 Loss: 13.263904
Train Epoch: 112 	 Loss: 12.979080
Train Epoch: 113 	 Loss: 13.038219
Train Epoch: 114 	 Loss: 13.272465
Train Epoch: 115 	 Loss: 13.125050
Train Epoch: 116 	 Loss: 13.124089
Train Epoch: 117 	 Loss: 13.423447
Train Epoch: 118 	 Loss: 13.322706
Train Epoch: 119 	 Loss: 13.566444
Train Epoch: 120 	 Loss: 13.271089
Train Epoch: 121 	 Loss: 13.043976
Train Epoch: 122 	 Loss: 13.147970
Train Epoch: 123 	 Loss: 13.039720
Train Epoch: 124 	 Loss: 13.591166
Train Epoch: 125 	 Loss: 13.161570
Train Epoch: 126 	 Loss: 13.092183
Train Epoch: 127 	 Loss: 13.094470
Train Epoch: 128 	 Loss: 13.232285
Train Epoch: 129 	 Loss: 13.008592
Train Epoch: 130 	 Loss: 13.087788
Train Epoch: 131 	 Loss: 13.233133
Train Epoch: 132 	 Loss: 13.679506
Train Epoch: 133 	 Loss: 13.097586
Train Epoch: 134 	 Loss: 13.089523
Train Epoch: 135 	 Loss: 13.254982
Train Epoch: 136 	 Loss: 13.567348
Train Epoch: 137 	 Loss: 13.516627
Train Epoch: 138 	 Loss: 13.098345
Train Epoch: 139 	 Loss: 12.971849
Train Epoch: 140 	 Loss: 13.077088
Train Epoch: 141 	 Loss: 13.211160
Train Epoch: 142 	 Loss: 13.672960
Train Epoch: 143 	 Loss: 13.284538
Train Epoch: 144 	 Loss: 13.115407
Train Epoch: 145 	 Loss: 13.359295
Train Epoch: 146 	 Loss: 13.265671
Train Epoch: 147 	 Loss: 12.949152
Train Epoch: 148 	 Loss: 13.056164
Train Epoch: 149 	 Loss: 13.065396
Train Epoch: 150 	 Loss: 12.928226
Train Epoch: 151 	 Loss: 12.884741
Train Epoch: 152 	 Loss: 13.207228
Train Epoch: 153 	 Loss: 13.029572
Train Epoch: 154 	 Loss: 13.077592
Train Epoch: 155 	 Loss: 13.144875
Train Epoch: 156 	 Loss: 13.028008
Train Epoch: 157 	 Loss: 12.988871
Train Epoch: 158 	 Loss: 12.999910
Train Epoch: 159 	 Loss: 13.215203
Train Epoch: 160 	 Loss: 12.989429
Train Epoch: 161 	 Loss: 12.984179
Train Epoch: 162 	 Loss: 13.701859
Train Epoch: 163 	 Loss: 12.945757
Train Epoch: 164 	 Loss: 13.007912
Train Epoch: 165 	 Loss: 12.913613
Train Epoch: 166 	 Loss: 12.944824
Train Epoch: 167 	 Loss: 12.999997
Train Epoch: 168 	 Loss: 12.977982
Train Epoch: 169 	 Loss: 13.270675
Train Epoch: 170 	 Loss: 13.014898
Train Epoch: 171 	 Loss: 13.375547
Train Epoch: 172 	 Loss: 12.940939
Train Epoch: 173 	 Loss: 12.895920
Train Epoch: 174 	 Loss: 12.867764
Train Epoch: 175 	 Loss: 12.944405
Train Epoch: 176 	 Loss: 12.999882
Train Epoch: 177 	 Loss: 13.089135
Train Epoch: 178 	 Loss: 12.917786
Train Epoch: 179 	 Loss: 13.013380
Train Epoch: 180 	 Loss: 13.133718
Train Epoch: 181 	 Loss: 12.874996
Train Epoch: 182 	 Loss: 13.106639
Train Epoch: 183 	 Loss: 13.023865
Train Epoch: 184 	 Loss: 12.954994
Train Epoch: 185 	 Loss: 13.012310
Train Epoch: 186 	 Loss: 13.159905
Train Epoch: 187 	 Loss: 13.008953
Train Epoch: 188 	 Loss: 13.021539
Train Epoch: 189 	 Loss: 12.995112
Train Epoch: 190 	 Loss: 12.912171
Train Epoch: 191 	 Loss: 14.004375
Train Epoch: 192 	 Loss: 12.955197
Train Epoch: 193 	 Loss: 12.980265
Train Epoch: 194 	 Loss: 13.108013
Train Epoch: 195 	 Loss: 13.215611
Train Epoch: 196 	 Loss: 12.945474
Train Epoch: 197 	 Loss: 12.924575
Train Epoch: 198 	 Loss: 12.971766
Train Epoch: 199 	 Loss: 13.246162
Train Epoch: 200 	 Loss: 12.937000
Train Epoch: 201 	 Loss: 13.062646
Train Epoch: 202 	 Loss: 13.102255
Train Epoch: 203 	 Loss: 13.029521
Train Epoch: 204 	 Loss: 12.896006
Train Epoch: 205 	 Loss: 13.219151
Train Epoch: 206 	 Loss: 13.598793
Train Epoch: 207 	 Loss: 13.015232
Train Epoch: 208 	 Loss: 12.908733
Train Epoch: 209 	 Loss: 12.824988
Train Epoch: 210 	 Loss: 13.124893
Train Epoch: 211 	 Loss: 12.979944
Train Epoch: 212 	 Loss: 12.945139
Train Epoch: 213 	 Loss: 12.850551
Train Epoch: 214 	 Loss: 12.967993
Train Epoch: 215 	 Loss: 13.130651
Train Epoch: 216 	 Loss: 12.907324
Train Epoch: 217 	 Loss: 13.158621
Train Epoch: 218 	 Loss: 13.811947
Train Epoch: 219 	 Loss: 13.684277
Train Epoch: 220 	 Loss: 12.976213
Train Epoch: 221 	 Loss: 13.035666
Train Epoch: 222 	 Loss: 12.952601
Train Epoch: 223 	 Loss: 12.979454
Train Epoch: 224 	 Loss: 12.937426
Train Epoch: 225 	 Loss: 12.926057
Train Epoch: 226 	 Loss: 13.063753
Train Epoch: 227 	 Loss: 13.490206
Train Epoch: 228 	 Loss: 13.001823
Train Epoch: 229 	 Loss: 12.918998
Train Epoch: 230 	 Loss: 13.160060
Train Epoch: 231 	 Loss: 13.160572
Train Epoch: 232 	 Loss: 13.152895
Train Epoch: 233 	 Loss: 13.305840
Train Epoch: 234 	 Loss: 13.042692
Train Epoch: 235 	 Loss: 13.191956
Train Epoch: 236 	 Loss: 13.043777
Train Epoch: 237 	 Loss: 13.041571
Train Epoch: 238 	 Loss: 12.914298
Train Epoch: 239 	 Loss: 13.078051
Train Epoch: 240 	 Loss: 13.207907
Train Epoch: 241 	 Loss: 13.112032
Train Epoch: 242 	 Loss: 12.890258
Train Epoch: 243 	 Loss: 13.327595
Train Epoch: 244 	 Loss: 13.380082
Train Epoch: 245 	 Loss: 13.408970
Train Epoch: 246 	 Loss: 13.129757
Train Epoch: 247 	 Loss: 13.055332
Train Epoch: 248 	 Loss: 13.097036
Train Epoch: 249 	 Loss: 12.958630
Train Epoch: 250 	 Loss: 12.904622
Train Epoch: 251 	 Loss: 12.870529
Train Epoch: 252 	 Loss: 12.932334
Train Epoch: 253 	 Loss: 13.102551
Train Epoch: 254 	 Loss: 13.434052
Train Epoch: 255 	 Loss: 13.037090
Train Epoch: 256 	 Loss: 13.233022
Train Epoch: 257 	 Loss: 13.010733
Train Epoch: 258 	 Loss: 13.259687
Train Epoch: 259 	 Loss: 13.196930
Train Epoch: 260 	 Loss: 12.993532
Train Epoch: 261 	 Loss: 13.081841
Train Epoch: 262 	 Loss: 12.931845
Train Epoch: 263 	 Loss: 13.037348
Train Epoch: 264 	 Loss: 12.897291
Train Epoch: 265 	 Loss: 13.155881
Train Epoch: 266 	 Loss: 12.954664
Train Epoch: 267 	 Loss: 12.988332
Train Epoch: 268 	 Loss: 13.024916
Train Epoch: 269 	 Loss: 12.854571
Train Epoch: 270 	 Loss: 12.886963
Train Epoch: 271 	 Loss: 12.993275
Train Epoch: 272 	 Loss: 13.138639
Train Epoch: 273 	 Loss: 13.343997
Train Epoch: 274 	 Loss: 12.895932
Train Epoch: 275 	 Loss: 13.085157
Train Epoch: 276 	 Loss: 13.300371
Train Epoch: 277 	 Loss: 12.855299
Train Epoch: 278 	 Loss: 12.808146
Train Epoch: 279 	 Loss: 12.908032
Train Epoch: 280 	 Loss: 12.847147
Train Epoch: 281 	 Loss: 12.923541
Train Epoch: 282 	 Loss: 12.909152
Train Epoch: 283 	 Loss: 13.024771
Train Epoch: 284 	 Loss: 13.449614
Train Epoch: 285 	 Loss: 13.007856
Train Epoch: 286 	 Loss: 13.268032
Train Epoch: 287 	 Loss: 13.070581
Train Epoch: 288 	 Loss: 12.930279
Train Epoch: 289 	 Loss: 12.871037
Train Epoch: 290 	 Loss: 13.090306
Train Epoch: 291 	 Loss: 13.106174
Train Epoch: 292 	 Loss: 13.225365
Train Epoch: 293 	 Loss: 13.037345
Train Epoch: 294 	 Loss: 13.049304
Train Epoch: 295 	 Loss: 12.921885
Train Epoch: 296 	 Loss: 12.979375
Train Epoch: 297 	 Loss: 13.050158
Train Epoch: 298 	 Loss: 13.037326
Train Epoch: 299 	 Loss: 13.346788
Train Epoch: 300 	 Loss: 12.930397
Train Epoch: 301 	 Loss: 13.252665
Train Epoch: 302 	 Loss: 13.202856
Train Epoch: 303 	 Loss: 12.819296
Train Epoch: 304 	 Loss: 13.137321
Train Epoch: 305 	 Loss: 12.903327
Train Epoch: 306 	 Loss: 12.991117
Train Epoch: 307 	 Loss: 13.059687
Train Epoch: 308 	 Loss: 12.940975
Train Epoch: 309 	 Loss: 12.903864
Train Epoch: 310 	 Loss: 12.994020
Train Epoch: 311 	 Loss: 12.908014
Train Epoch: 312 	 Loss: 12.978071
Train Epoch: 313 	 Loss: 13.044587
Train Epoch: 314 	 Loss: 12.856415
Train Epoch: 315 	 Loss: 13.415525
Train Epoch: 316 	 Loss: 13.307912
Train Epoch: 317 	 Loss: 13.005180
Train Epoch: 318 	 Loss: 13.080274
Train Epoch: 319 	 Loss: 13.421719
Train Epoch: 320 	 Loss: 12.851698
Train Epoch: 321 	 Loss: 13.250360
Train Epoch: 322 	 Loss: 12.925854
Train Epoch: 323 	 Loss: 12.855228
Train Epoch: 324 	 Loss: 12.999554
Train Epoch: 325 	 Loss: 12.884514
Train Epoch: 326 	 Loss: 12.870260
Train Epoch: 327 	 Loss: 12.888517
Train Epoch: 328 	 Loss: 12.903621
Train Epoch: 329 	 Loss: 12.936485
Train Epoch: 330 	 Loss: 13.267643
Train Epoch: 331 	 Loss: 12.974575
Train Epoch: 332 	 Loss: 12.895099
Train Epoch: 333 	 Loss: 12.859556
Train Epoch: 334 	 Loss: 13.177268
Train Epoch: 335 	 Loss: 13.101342
Train Epoch: 336 	 Loss: 13.012700
Train Epoch: 337 	 Loss: 13.010233
Train Epoch: 338 	 Loss: 13.028189
Train Epoch: 339 	 Loss: 13.395866
Train Epoch: 340 	 Loss: 13.004652
Train Epoch: 341 	 Loss: 13.024439
Train Epoch: 342 	 Loss: 12.872778
Train Epoch: 343 	 Loss: 12.923405
Train Epoch: 344 	 Loss: 13.702152
Train Epoch: 345 	 Loss: 13.195026
Train Epoch: 346 	 Loss: 12.822128
Train Epoch: 347 	 Loss: 12.842707
Train Epoch: 348 	 Loss: 13.914330
Train Epoch: 349 	 Loss: 12.898298
Train Epoch: 350 	 Loss: 12.808475
Train Epoch: 351 	 Loss: 13.082788
Train Epoch: 352 	 Loss: 12.892002
Train Epoch: 353 	 Loss: 13.115241
Train Epoch: 354 	 Loss: 12.973535
Train Epoch: 355 	 Loss: 12.883230
Train Epoch: 356 	 Loss: 12.856117
Train Epoch: 357 	 Loss: 12.982180
Train Epoch: 358 	 Loss: 13.142845
Train Epoch: 359 	 Loss: 12.844211
Train Epoch: 360 	 Loss: 13.089007
Train Epoch: 361 	 Loss: 12.869338
Train Epoch: 362 	 Loss: 13.300928
Train Epoch: 363 	 Loss: 12.913305
Train Epoch: 364 	 Loss: 13.164940
Train Epoch: 365 	 Loss: 13.009008
Train Epoch: 366 	 Loss: 12.993442
Train Epoch: 367 	 Loss: 12.974200
Train Epoch: 368 	 Loss: 12.991074
Train Epoch: 369 	 Loss: 12.849527
Train Epoch: 370 	 Loss: 12.942843
Train Epoch: 371 	 Loss: 12.863054
Train Epoch: 372 	 Loss: 13.420561
Train Epoch: 373 	 Loss: 12.951999
Train Epoch: 374 	 Loss: 12.921128
Train Epoch: 375 	 Loss: 12.808731
Train Epoch: 376 	 Loss: 12.905014
Train Epoch: 377 	 Loss: 12.943946
Train Epoch: 378 	 Loss: 12.975998
Train Epoch: 379 	 Loss: 13.298756
Train Epoch: 380 	 Loss: 13.803092
Train Epoch: 381 	 Loss: 13.400744
Train Epoch: 382 	 Loss: 13.151474
Train Epoch: 383 	 Loss: 13.229158
Train Epoch: 384 	 Loss: 12.928246
Train Epoch: 385 	 Loss: 13.195343
Train Epoch: 386 	 Loss: 12.804646
Train Epoch: 387 	 Loss: 13.073390
Train Epoch: 388 	 Loss: 13.459750
Train Epoch: 389 	 Loss: 13.528597
Train Epoch: 390 	 Loss: 13.324270
Train Epoch: 391 	 Loss: 13.077314
Train Epoch: 392 	 Loss: 13.275866
Train Epoch: 393 	 Loss: 13.136023
Train Epoch: 394 	 Loss: 12.896893
Train Epoch: 395 	 Loss: 12.992783
Train Epoch: 396 	 Loss: 13.782444
Train Epoch: 397 	 Loss: 12.971109
Train Epoch: 398 	 Loss: 12.936554
Train Epoch: 399 	 Loss: 13.109211

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.868

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.648

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.905

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.862

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.933

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.870

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.948

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.906

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.913

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.913

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.864

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.913
------
f1 mean across methods is 0.879


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 400
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.87391125e-01 4.86136652e-01 2.05491169e-02 5.67363754e-03
 2.49468987e-04] 

Train Epoch: 0 	 Loss: 17.699314
Train Epoch: 1 	 Loss: 16.403065
Train Epoch: 2 	 Loss: 17.004028
Train Epoch: 3 	 Loss: 15.036882
Train Epoch: 4 	 Loss: 13.916759
Train Epoch: 5 	 Loss: 14.029137
Train Epoch: 6 	 Loss: 13.971611
Train Epoch: 7 	 Loss: 13.909195
Train Epoch: 8 	 Loss: 13.572208
Train Epoch: 9 	 Loss: 13.689835
Train Epoch: 10 	 Loss: 13.044112
Train Epoch: 11 	 Loss: 13.078121
Train Epoch: 12 	 Loss: 13.516640
Train Epoch: 13 	 Loss: 13.235537
Train Epoch: 14 	 Loss: 12.817610
Train Epoch: 15 	 Loss: 13.007608
Train Epoch: 16 	 Loss: 13.058898
Train Epoch: 17 	 Loss: 12.705680
Train Epoch: 18 	 Loss: 12.443771
Train Epoch: 19 	 Loss: 12.803807
Train Epoch: 20 	 Loss: 12.523604
Train Epoch: 21 	 Loss: 13.087405
Train Epoch: 22 	 Loss: 12.577766
Train Epoch: 23 	 Loss: 13.081493
Train Epoch: 24 	 Loss: 13.059303
Train Epoch: 25 	 Loss: 12.536841
Train Epoch: 26 	 Loss: 12.616076
Train Epoch: 27 	 Loss: 12.964177
Train Epoch: 28 	 Loss: 12.189398
Train Epoch: 29 	 Loss: 13.451958
Train Epoch: 30 	 Loss: 11.804640
Train Epoch: 31 	 Loss: 12.568617
Train Epoch: 32 	 Loss: 11.959229
Train Epoch: 33 	 Loss: 12.253209
Train Epoch: 34 	 Loss: 12.182190
Train Epoch: 35 	 Loss: 11.976773
Train Epoch: 36 	 Loss: 12.479153
Train Epoch: 37 	 Loss: 12.308719
Train Epoch: 38 	 Loss: 12.096148
Train Epoch: 39 	 Loss: 12.138680
Train Epoch: 40 	 Loss: 11.967888
Train Epoch: 41 	 Loss: 12.112600
Train Epoch: 42 	 Loss: 12.319421
Train Epoch: 43 	 Loss: 11.966547
Train Epoch: 44 	 Loss: 12.172399
Train Epoch: 45 	 Loss: 12.183634
Train Epoch: 46 	 Loss: 12.016048
Train Epoch: 47 	 Loss: 12.518628
Train Epoch: 48 	 Loss: 12.117100
Train Epoch: 49 	 Loss: 12.067854
Train Epoch: 50 	 Loss: 12.163749
Train Epoch: 51 	 Loss: 11.908029
Train Epoch: 52 	 Loss: 11.931936
Train Epoch: 53 	 Loss: 12.333681
Train Epoch: 54 	 Loss: 12.766284
Train Epoch: 55 	 Loss: 13.203197
Train Epoch: 56 	 Loss: 12.096610
Train Epoch: 57 	 Loss: 12.462311
Train Epoch: 58 	 Loss: 11.898658
Train Epoch: 59 	 Loss: 12.538534
Train Epoch: 60 	 Loss: 12.875687
Train Epoch: 61 	 Loss: 12.190812
Train Epoch: 62 	 Loss: 12.964420
Train Epoch: 63 	 Loss: 12.376091
Train Epoch: 64 	 Loss: 11.804167
Train Epoch: 65 	 Loss: 12.215618
Train Epoch: 66 	 Loss: 12.019173
Train Epoch: 67 	 Loss: 11.888619
Train Epoch: 68 	 Loss: 12.189005
Train Epoch: 69 	 Loss: 12.110441
Train Epoch: 70 	 Loss: 12.742352
Train Epoch: 71 	 Loss: 12.444479
Train Epoch: 72 	 Loss: 12.148685
Train Epoch: 73 	 Loss: 12.481874
Train Epoch: 74 	 Loss: 13.313790
Train Epoch: 75 	 Loss: 12.659622
Train Epoch: 76 	 Loss: 11.752933
Train Epoch: 77 	 Loss: 11.745324
Train Epoch: 78 	 Loss: 12.676130
Train Epoch: 79 	 Loss: 12.534130
Train Epoch: 80 	 Loss: 11.978786
Train Epoch: 81 	 Loss: 12.745448
Train Epoch: 82 	 Loss: 12.465776
Train Epoch: 83 	 Loss: 12.538202
Train Epoch: 84 	 Loss: 11.233946
Train Epoch: 85 	 Loss: 12.908813
Train Epoch: 86 	 Loss: 12.291089
Train Epoch: 87 	 Loss: 13.034468
Train Epoch: 88 	 Loss: 12.603220
Train Epoch: 89 	 Loss: 12.967349
Train Epoch: 90 	 Loss: 12.423617
Train Epoch: 91 	 Loss: 12.889342
Train Epoch: 92 	 Loss: 12.572891
Train Epoch: 93 	 Loss: 11.818972
Train Epoch: 94 	 Loss: 12.529546
Train Epoch: 95 	 Loss: 12.568357
Train Epoch: 96 	 Loss: 12.139261
Train Epoch: 97 	 Loss: 13.276420
Train Epoch: 98 	 Loss: 13.120236
Train Epoch: 99 	 Loss: 11.825583
Train Epoch: 100 	 Loss: 11.794747
Train Epoch: 101 	 Loss: 11.481564
Train Epoch: 102 	 Loss: 12.348476
Train Epoch: 103 	 Loss: 12.518938
Train Epoch: 104 	 Loss: 12.727959
Train Epoch: 105 	 Loss: 12.847586
Train Epoch: 106 	 Loss: 12.417027
Train Epoch: 107 	 Loss: 12.672899
Train Epoch: 108 	 Loss: 12.928959
Train Epoch: 109 	 Loss: 11.642687
Train Epoch: 110 	 Loss: 12.440424
Train Epoch: 111 	 Loss: 12.728763
Train Epoch: 112 	 Loss: 11.200473
Train Epoch: 113 	 Loss: 12.843780
Train Epoch: 114 	 Loss: 12.733202
Train Epoch: 115 	 Loss: 11.557232
Train Epoch: 116 	 Loss: 12.392197
Train Epoch: 117 	 Loss: 12.491571
Train Epoch: 118 	 Loss: 12.399285
Train Epoch: 119 	 Loss: 12.504552
Train Epoch: 120 	 Loss: 11.592295
Train Epoch: 121 	 Loss: 12.320513
Train Epoch: 122 	 Loss: 12.195133
Train Epoch: 123 	 Loss: 12.730736
Train Epoch: 124 	 Loss: 13.044389
Train Epoch: 125 	 Loss: 12.594945
Train Epoch: 126 	 Loss: 12.190779
Train Epoch: 127 	 Loss: 12.346406
Train Epoch: 128 	 Loss: 12.358468
Train Epoch: 129 	 Loss: 13.534234
Train Epoch: 130 	 Loss: 12.401493
Train Epoch: 131 	 Loss: 12.089882
Train Epoch: 132 	 Loss: 12.399291
Train Epoch: 133 	 Loss: 11.812259
Train Epoch: 134 	 Loss: 12.070616
Train Epoch: 135 	 Loss: 11.854448
Train Epoch: 136 	 Loss: 12.426023
Train Epoch: 137 	 Loss: 12.696524
Train Epoch: 138 	 Loss: 12.776442
Train Epoch: 139 	 Loss: 12.075830
Train Epoch: 140 	 Loss: 12.153000
Train Epoch: 141 	 Loss: 11.366759
Train Epoch: 142 	 Loss: 13.622991
Train Epoch: 143 	 Loss: 14.010742
Train Epoch: 144 	 Loss: 12.214729
Train Epoch: 145 	 Loss: 12.493831
Train Epoch: 146 	 Loss: 11.995234
Train Epoch: 147 	 Loss: 11.613713
Train Epoch: 148 	 Loss: 12.144982
Train Epoch: 149 	 Loss: 12.838500
Train Epoch: 150 	 Loss: 12.621741
Train Epoch: 151 	 Loss: 11.758659
Train Epoch: 152 	 Loss: 12.082884
Train Epoch: 153 	 Loss: 12.193848
Train Epoch: 154 	 Loss: 12.337175
Train Epoch: 155 	 Loss: 12.082950
Train Epoch: 156 	 Loss: 12.040045
Train Epoch: 157 	 Loss: 12.262455
Train Epoch: 158 	 Loss: 12.431978
Train Epoch: 159 	 Loss: 12.581242
Train Epoch: 160 	 Loss: 12.319914
Train Epoch: 161 	 Loss: 11.667067
Train Epoch: 162 	 Loss: 12.265789
Train Epoch: 163 	 Loss: 12.666673
Train Epoch: 164 	 Loss: 12.229078
Train Epoch: 165 	 Loss: 12.607938
Train Epoch: 166 	 Loss: 12.612021
Train Epoch: 167 	 Loss: 12.401034
Train Epoch: 168 	 Loss: 11.983193
Train Epoch: 169 	 Loss: 12.479449
Train Epoch: 170 	 Loss: 12.081579
Train Epoch: 171 	 Loss: 12.533728
Train Epoch: 172 	 Loss: 13.258778
Train Epoch: 173 	 Loss: 11.989618
Train Epoch: 174 	 Loss: 12.379608
Train Epoch: 175 	 Loss: 13.278028
Train Epoch: 176 	 Loss: 12.273972
Train Epoch: 177 	 Loss: 12.123131
Train Epoch: 178 	 Loss: 12.047480
Train Epoch: 179 	 Loss: 12.034995
Train Epoch: 180 	 Loss: 12.241798
Train Epoch: 181 	 Loss: 11.811880
Train Epoch: 182 	 Loss: 12.259015
Train Epoch: 183 	 Loss: 11.994278
Train Epoch: 184 	 Loss: 12.074611
Train Epoch: 185 	 Loss: 12.008369
Train Epoch: 186 	 Loss: 12.557369
Train Epoch: 187 	 Loss: 11.837369
Train Epoch: 188 	 Loss: 12.209431
Train Epoch: 189 	 Loss: 12.107134
Train Epoch: 190 	 Loss: 12.040471
Train Epoch: 191 	 Loss: 12.423519
Train Epoch: 192 	 Loss: 12.274117
Train Epoch: 193 	 Loss: 11.859401
Train Epoch: 194 	 Loss: 11.985016
Train Epoch: 195 	 Loss: 12.076738
Train Epoch: 196 	 Loss: 12.727094
Train Epoch: 197 	 Loss: 12.665892
Train Epoch: 198 	 Loss: 11.869610
Train Epoch: 199 	 Loss: 11.988943
Train Epoch: 200 	 Loss: 12.266144
Train Epoch: 201 	 Loss: 12.041789
Train Epoch: 202 	 Loss: 12.185404
Train Epoch: 203 	 Loss: 11.876474
Train Epoch: 204 	 Loss: 12.039588
Train Epoch: 205 	 Loss: 11.898560
Train Epoch: 206 	 Loss: 12.471607
Train Epoch: 207 	 Loss: 11.969481
Train Epoch: 208 	 Loss: 12.532432
Train Epoch: 209 	 Loss: 12.014068
Train Epoch: 210 	 Loss: 11.305639
Train Epoch: 211 	 Loss: 11.912959
Train Epoch: 212 	 Loss: 11.962589
Train Epoch: 213 	 Loss: 12.053585
Train Epoch: 214 	 Loss: 11.974205
Train Epoch: 215 	 Loss: 11.813202
Train Epoch: 216 	 Loss: 11.858833
Train Epoch: 217 	 Loss: 11.574582
Train Epoch: 218 	 Loss: 11.965715
Train Epoch: 219 	 Loss: 12.114486
Train Epoch: 220 	 Loss: 12.141573
Train Epoch: 221 	 Loss: 12.230490
Train Epoch: 222 	 Loss: 12.085596
Train Epoch: 223 	 Loss: 11.508333
Train Epoch: 224 	 Loss: 12.130533
Train Epoch: 225 	 Loss: 11.894621
Train Epoch: 226 	 Loss: 11.908846
Train Epoch: 227 	 Loss: 12.082222
Train Epoch: 228 	 Loss: 12.292999
Train Epoch: 229 	 Loss: 11.963461
Train Epoch: 230 	 Loss: 12.260821
Train Epoch: 231 	 Loss: 11.863447
Train Epoch: 232 	 Loss: 12.089346
Train Epoch: 233 	 Loss: 12.068116
Train Epoch: 234 	 Loss: 12.048626
Train Epoch: 235 	 Loss: 12.469372
Train Epoch: 236 	 Loss: 11.886308
Train Epoch: 237 	 Loss: 11.784296
Train Epoch: 238 	 Loss: 12.086324
Train Epoch: 239 	 Loss: 12.104945
Train Epoch: 240 	 Loss: 11.859677
Train Epoch: 241 	 Loss: 11.939003
Train Epoch: 242 	 Loss: 12.472725
Train Epoch: 243 	 Loss: 12.043877
Train Epoch: 244 	 Loss: 12.088902
Train Epoch: 245 	 Loss: 12.347185
Train Epoch: 246 	 Loss: 11.752251
Train Epoch: 247 	 Loss: 11.859238
Train Epoch: 248 	 Loss: 11.933729
Train Epoch: 249 	 Loss: 12.794575
Train Epoch: 250 	 Loss: 12.228812
Train Epoch: 251 	 Loss: 11.728206
Train Epoch: 252 	 Loss: 12.126999
Train Epoch: 253 	 Loss: 12.894432
Train Epoch: 254 	 Loss: 12.183342
Train Epoch: 255 	 Loss: 12.142059
Train Epoch: 256 	 Loss: 12.430220
Train Epoch: 257 	 Loss: 11.724398
Train Epoch: 258 	 Loss: 12.116538
Train Epoch: 259 	 Loss: 11.991644
Train Epoch: 260 	 Loss: 11.938204
Train Epoch: 261 	 Loss: 12.039789
Train Epoch: 262 	 Loss: 13.158213
Train Epoch: 263 	 Loss: 12.261827
Train Epoch: 264 	 Loss: 12.961658
Train Epoch: 265 	 Loss: 11.896149
Train Epoch: 266 	 Loss: 12.008076
Train Epoch: 267 	 Loss: 11.820053
Train Epoch: 268 	 Loss: 11.395110
Train Epoch: 269 	 Loss: 11.449384
Train Epoch: 270 	 Loss: 12.247274
Train Epoch: 271 	 Loss: 12.067346
Train Epoch: 272 	 Loss: 11.999412
Train Epoch: 273 	 Loss: 12.299717
Train Epoch: 274 	 Loss: 11.964415
Train Epoch: 275 	 Loss: 11.823438
Train Epoch: 276 	 Loss: 11.602882
Train Epoch: 277 	 Loss: 11.809030
Train Epoch: 278 	 Loss: 11.818826
Train Epoch: 279 	 Loss: 11.883756
Train Epoch: 280 	 Loss: 11.371510
Train Epoch: 281 	 Loss: 11.999610
Train Epoch: 282 	 Loss: 12.459988
Train Epoch: 283 	 Loss: 12.200253
Train Epoch: 284 	 Loss: 11.882509
Train Epoch: 285 	 Loss: 12.169842
Train Epoch: 286 	 Loss: 11.820197
Train Epoch: 287 	 Loss: 11.728215
Train Epoch: 288 	 Loss: 12.337529
Train Epoch: 289 	 Loss: 11.527843
Train Epoch: 290 	 Loss: 12.036887
Train Epoch: 291 	 Loss: 11.826712
Train Epoch: 292 	 Loss: 11.935884
Train Epoch: 293 	 Loss: 11.991175
Train Epoch: 294 	 Loss: 12.212802
Train Epoch: 295 	 Loss: 12.047394
Train Epoch: 296 	 Loss: 12.271721
Train Epoch: 297 	 Loss: 11.946443
Train Epoch: 298 	 Loss: 11.922463
Train Epoch: 299 	 Loss: 11.961554
Train Epoch: 300 	 Loss: 11.969210
Train Epoch: 301 	 Loss: 12.041708
Train Epoch: 302 	 Loss: 11.953193
Train Epoch: 303 	 Loss: 11.790920
Train Epoch: 304 	 Loss: 11.426163
Train Epoch: 305 	 Loss: 12.464273
Train Epoch: 306 	 Loss: 12.045278
Train Epoch: 307 	 Loss: 12.407400
Train Epoch: 308 	 Loss: 11.807606
Train Epoch: 309 	 Loss: 12.776202
Train Epoch: 310 	 Loss: 12.051949
Train Epoch: 311 	 Loss: 12.019898
Train Epoch: 312 	 Loss: 12.427565
Train Epoch: 313 	 Loss: 12.315577
Train Epoch: 314 	 Loss: 11.869752
Train Epoch: 315 	 Loss: 11.437438
Train Epoch: 316 	 Loss: 11.807150
Train Epoch: 317 	 Loss: 11.833549
Train Epoch: 318 	 Loss: 12.553216
Train Epoch: 319 	 Loss: 12.444983
Train Epoch: 320 	 Loss: 12.590974
Train Epoch: 321 	 Loss: 12.580723
Train Epoch: 322 	 Loss: 12.484793
Train Epoch: 323 	 Loss: 12.474829
Train Epoch: 324 	 Loss: 12.507892
Train Epoch: 325 	 Loss: 12.700038
Train Epoch: 326 	 Loss: 11.655217
Train Epoch: 327 	 Loss: 11.655363
Train Epoch: 328 	 Loss: 12.690386
Train Epoch: 329 	 Loss: 12.668956
Train Epoch: 330 	 Loss: 12.712969
Train Epoch: 331 	 Loss: 12.310555
Train Epoch: 332 	 Loss: 12.546019
Train Epoch: 333 	 Loss: 12.923325
Train Epoch: 334 	 Loss: 12.587912
Train Epoch: 335 	 Loss: 11.301969
Train Epoch: 336 	 Loss: 12.713620
Train Epoch: 337 	 Loss: 13.043909
Train Epoch: 338 	 Loss: 12.445890
Train Epoch: 339 	 Loss: 12.502516
Train Epoch: 340 	 Loss: 12.209478
Train Epoch: 341 	 Loss: 12.992237
Train Epoch: 342 	 Loss: 12.210417
Train Epoch: 343 	 Loss: 12.447603
Train Epoch: 344 	 Loss: 12.331894
Train Epoch: 345 	 Loss: 12.342997
Train Epoch: 346 	 Loss: 11.881261
Train Epoch: 347 	 Loss: 12.498961
Train Epoch: 348 	 Loss: 12.881304
Train Epoch: 349 	 Loss: 12.452927
Train Epoch: 350 	 Loss: 12.405897
Train Epoch: 351 	 Loss: 11.327751
Train Epoch: 352 	 Loss: 11.297106
Train Epoch: 353 	 Loss: 11.209379
Train Epoch: 354 	 Loss: 12.264151
Train Epoch: 355 	 Loss: 12.110215
Train Epoch: 356 	 Loss: 10.944940
Train Epoch: 357 	 Loss: 12.137491
Train Epoch: 358 	 Loss: 11.871248
Train Epoch: 359 	 Loss: 12.081516
Train Epoch: 360 	 Loss: 12.711952
Train Epoch: 361 	 Loss: 12.046687
Train Epoch: 362 	 Loss: 11.883364
Train Epoch: 363 	 Loss: 11.884491
Train Epoch: 364 	 Loss: 11.135944
Train Epoch: 365 	 Loss: 12.505472
Train Epoch: 366 	 Loss: 11.903406
Train Epoch: 367 	 Loss: 12.640408
Train Epoch: 368 	 Loss: 12.365904
Train Epoch: 369 	 Loss: 11.337459
Train Epoch: 370 	 Loss: 11.012862
Train Epoch: 371 	 Loss: 11.801354
Train Epoch: 372 	 Loss: 12.076479
Train Epoch: 373 	 Loss: 11.418119
Train Epoch: 374 	 Loss: 12.064977
Train Epoch: 375 	 Loss: 12.221765
Train Epoch: 376 	 Loss: 11.939217
Train Epoch: 377 	 Loss: 12.412559
Train Epoch: 378 	 Loss: 11.989952
Train Epoch: 379 	 Loss: 12.321625
Train Epoch: 380 	 Loss: 11.918907
Train Epoch: 381 	 Loss: 12.232760
Train Epoch: 382 	 Loss: 11.356867
Train Epoch: 383 	 Loss: 12.321775
Train Epoch: 384 	 Loss: 12.458429
Train Epoch: 385 	 Loss: 11.789608
Train Epoch: 386 	 Loss: 11.782284
Train Epoch: 387 	 Loss: 11.962339
Train Epoch: 388 	 Loss: 11.930682
Train Epoch: 389 	 Loss: 12.451591
Train Epoch: 390 	 Loss: 12.071623
Train Epoch: 391 	 Loss: 11.858716
Train Epoch: 392 	 Loss: 12.547954
Train Epoch: 393 	 Loss: 11.947002
Train Epoch: 394 	 Loss: 11.406288
Train Epoch: 395 	 Loss: 12.017248
Train Epoch: 396 	 Loss: 11.859095
Train Epoch: 397 	 Loss: 11.472522
Train Epoch: 398 	 Loss: 11.891541
Train Epoch: 399 	 Loss: 11.260076

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.872

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.516

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.822

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.861

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.898

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.933

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.936

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.907

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.911

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.916

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.877

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.921
------
f1 mean across methods is 0.864


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 400
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.89322727e-01 4.84254943e-01 2.03637971e-02 5.80193588e-03
 2.56596673e-04] 

Train Epoch: 0 	 Loss: 20.327835
Train Epoch: 1 	 Loss: 18.369965
Train Epoch: 2 	 Loss: 17.386044
Train Epoch: 3 	 Loss: 16.221483
Train Epoch: 4 	 Loss: 15.366977
Train Epoch: 5 	 Loss: 15.004657
Train Epoch: 6 	 Loss: 14.859586
Train Epoch: 7 	 Loss: 14.181628
Train Epoch: 8 	 Loss: 14.305294
Train Epoch: 9 	 Loss: 14.090436
Train Epoch: 10 	 Loss: 14.378041
Train Epoch: 11 	 Loss: 14.605546
Train Epoch: 12 	 Loss: 14.974958
Train Epoch: 13 	 Loss: 14.373510
Train Epoch: 14 	 Loss: 14.358452
Train Epoch: 15 	 Loss: 14.437086
Train Epoch: 16 	 Loss: 14.466822
Train Epoch: 17 	 Loss: 14.666979
Train Epoch: 18 	 Loss: 14.238404
Train Epoch: 19 	 Loss: 14.141953
Train Epoch: 20 	 Loss: 14.365196
Train Epoch: 21 	 Loss: 14.330354
Train Epoch: 22 	 Loss: 14.158079
Train Epoch: 23 	 Loss: 14.008721
Train Epoch: 24 	 Loss: 13.318683
Train Epoch: 25 	 Loss: 13.977546
Train Epoch: 26 	 Loss: 14.406343
Train Epoch: 27 	 Loss: 14.336739
Train Epoch: 28 	 Loss: 13.790548
Train Epoch: 29 	 Loss: 14.054397
Train Epoch: 30 	 Loss: 14.311313
Train Epoch: 31 	 Loss: 13.525921
Train Epoch: 32 	 Loss: 14.115278
Train Epoch: 33 	 Loss: 14.208899
Train Epoch: 34 	 Loss: 14.041897
Train Epoch: 35 	 Loss: 14.117653
Train Epoch: 36 	 Loss: 14.139552
Train Epoch: 37 	 Loss: 14.190953
Train Epoch: 38 	 Loss: 13.839236
Train Epoch: 39 	 Loss: 13.868603
Train Epoch: 40 	 Loss: 13.514694
Train Epoch: 41 	 Loss: 14.010947
Train Epoch: 42 	 Loss: 14.190582
Train Epoch: 43 	 Loss: 14.196292
Train Epoch: 44 	 Loss: 13.305828
Train Epoch: 45 	 Loss: 13.077117
Train Epoch: 46 	 Loss: 13.980614
Train Epoch: 47 	 Loss: 14.095636
Train Epoch: 48 	 Loss: 13.552912
Train Epoch: 49 	 Loss: 13.991071
Train Epoch: 50 	 Loss: 13.643698
Train Epoch: 51 	 Loss: 14.067111
Train Epoch: 52 	 Loss: 13.476824
Train Epoch: 53 	 Loss: 14.181000
Train Epoch: 54 	 Loss: 13.472623
Train Epoch: 55 	 Loss: 13.671597
Train Epoch: 56 	 Loss: 13.803993
Train Epoch: 57 	 Loss: 13.773411
Train Epoch: 58 	 Loss: 13.653528
Train Epoch: 59 	 Loss: 13.425674
Train Epoch: 60 	 Loss: 13.316966
Train Epoch: 61 	 Loss: 13.106074
Train Epoch: 62 	 Loss: 12.942327
Train Epoch: 63 	 Loss: 13.586319
Train Epoch: 64 	 Loss: 13.761037
Train Epoch: 65 	 Loss: 13.748390
Train Epoch: 66 	 Loss: 13.545002
Train Epoch: 67 	 Loss: 13.569926
Train Epoch: 68 	 Loss: 13.789912
Train Epoch: 69 	 Loss: 13.782621
Train Epoch: 70 	 Loss: 13.650413
Train Epoch: 71 	 Loss: 13.734776
Train Epoch: 72 	 Loss: 13.800713
Train Epoch: 73 	 Loss: 13.419513
Train Epoch: 74 	 Loss: 13.418194
Train Epoch: 75 	 Loss: 13.302999
Train Epoch: 76 	 Loss: 13.393217
Train Epoch: 77 	 Loss: 13.477100
Train Epoch: 78 	 Loss: 13.303543
Train Epoch: 79 	 Loss: 12.882349
Train Epoch: 80 	 Loss: 13.610069
Train Epoch: 81 	 Loss: 13.858374
Train Epoch: 82 	 Loss: 13.669034
Train Epoch: 83 	 Loss: 13.375175
Train Epoch: 84 	 Loss: 13.565750
Train Epoch: 85 	 Loss: 13.473600
Train Epoch: 86 	 Loss: 13.603579
Train Epoch: 87 	 Loss: 14.085279
Train Epoch: 88 	 Loss: 14.152282
Train Epoch: 89 	 Loss: 13.473752
Train Epoch: 90 	 Loss: 13.312902
Train Epoch: 91 	 Loss: 13.743073
Train Epoch: 92 	 Loss: 13.290525
Train Epoch: 93 	 Loss: 13.231668
Train Epoch: 94 	 Loss: 13.501938
Train Epoch: 95 	 Loss: 13.378199
Train Epoch: 96 	 Loss: 13.587518
Train Epoch: 97 	 Loss: 13.291513
Train Epoch: 98 	 Loss: 13.403406
Train Epoch: 99 	 Loss: 13.493035
Train Epoch: 100 	 Loss: 13.514587
Train Epoch: 101 	 Loss: 13.188093
Train Epoch: 102 	 Loss: 13.963686
Train Epoch: 103 	 Loss: 13.941802
Train Epoch: 104 	 Loss: 14.242163
Train Epoch: 105 	 Loss: 14.172192
Train Epoch: 106 	 Loss: 14.241731
Train Epoch: 107 	 Loss: 14.819139
Train Epoch: 108 	 Loss: 13.915758
Train Epoch: 109 	 Loss: 13.548180
Train Epoch: 110 	 Loss: 13.749660
Train Epoch: 111 	 Loss: 13.575706
Train Epoch: 112 	 Loss: 13.378893
Train Epoch: 113 	 Loss: 13.559702
Train Epoch: 114 	 Loss: 13.418757
Train Epoch: 115 	 Loss: 13.309223
Train Epoch: 116 	 Loss: 13.402856
Train Epoch: 117 	 Loss: 13.421531
Train Epoch: 118 	 Loss: 13.705410
Train Epoch: 119 	 Loss: 13.882135
Train Epoch: 120 	 Loss: 13.255997
Train Epoch: 121 	 Loss: 13.339066
Train Epoch: 122 	 Loss: 13.324409
Train Epoch: 123 	 Loss: 13.193867
Train Epoch: 124 	 Loss: 13.462265
Train Epoch: 125 	 Loss: 13.935770
Train Epoch: 126 	 Loss: 13.742531
Train Epoch: 127 	 Loss: 13.548001
Train Epoch: 128 	 Loss: 13.371016
Train Epoch: 129 	 Loss: 13.443058
Train Epoch: 130 	 Loss: 13.509233
Train Epoch: 131 	 Loss: 13.325182
Train Epoch: 132 	 Loss: 13.525825
Train Epoch: 133 	 Loss: 13.450531
Train Epoch: 134 	 Loss: 13.502020
Train Epoch: 135 	 Loss: 13.894606
Train Epoch: 136 	 Loss: 12.940605
Train Epoch: 137 	 Loss: 13.362571
Train Epoch: 138 	 Loss: 13.434036
Train Epoch: 139 	 Loss: 13.473813
Train Epoch: 140 	 Loss: 13.488040
Train Epoch: 141 	 Loss: 13.003750
Train Epoch: 142 	 Loss: 13.329882
Train Epoch: 143 	 Loss: 13.478783
Train Epoch: 144 	 Loss: 12.876181
Train Epoch: 145 	 Loss: 13.569395
Train Epoch: 146 	 Loss: 13.531557
Train Epoch: 147 	 Loss: 13.263456
Train Epoch: 148 	 Loss: 13.683035
Train Epoch: 149 	 Loss: 12.745239
Train Epoch: 150 	 Loss: 13.712327
Train Epoch: 151 	 Loss: 13.738522
Train Epoch: 152 	 Loss: 13.699452
Train Epoch: 153 	 Loss: 13.528057
Train Epoch: 154 	 Loss: 13.767962
Train Epoch: 155 	 Loss: 12.922730
Train Epoch: 156 	 Loss: 13.534525
Train Epoch: 157 	 Loss: 13.203671
Train Epoch: 158 	 Loss: 13.488599
Train Epoch: 159 	 Loss: 13.746754
Train Epoch: 160 	 Loss: 13.647467
Train Epoch: 161 	 Loss: 13.258615
Train Epoch: 162 	 Loss: 13.637922
Train Epoch: 163 	 Loss: 13.794573
Train Epoch: 164 	 Loss: 13.850140
Train Epoch: 165 	 Loss: 13.393278
Train Epoch: 166 	 Loss: 13.375395
Train Epoch: 167 	 Loss: 13.479837
Train Epoch: 168 	 Loss: 13.182867
Train Epoch: 169 	 Loss: 13.376581
Train Epoch: 170 	 Loss: 13.638927
Train Epoch: 171 	 Loss: 13.324121
Train Epoch: 172 	 Loss: 13.598104
Train Epoch: 173 	 Loss: 13.635281
Train Epoch: 174 	 Loss: 13.450973
Train Epoch: 175 	 Loss: 13.403127
Train Epoch: 176 	 Loss: 13.673834
Train Epoch: 177 	 Loss: 13.514247
Train Epoch: 178 	 Loss: 13.317275
Train Epoch: 179 	 Loss: 13.543133
Train Epoch: 180 	 Loss: 13.505798
Train Epoch: 181 	 Loss: 13.480413
Train Epoch: 182 	 Loss: 13.357768
Train Epoch: 183 	 Loss: 12.787306
Train Epoch: 184 	 Loss: 13.310398
Train Epoch: 185 	 Loss: 13.292038
Train Epoch: 186 	 Loss: 13.219027
Train Epoch: 187 	 Loss: 14.088778
Train Epoch: 188 	 Loss: 13.161674
Train Epoch: 189 	 Loss: 13.315930
Train Epoch: 190 	 Loss: 13.800765
Train Epoch: 191 	 Loss: 13.425243
Train Epoch: 192 	 Loss: 13.727748
Train Epoch: 193 	 Loss: 13.707951
Train Epoch: 194 	 Loss: 12.736118
Train Epoch: 195 	 Loss: 13.431571
Train Epoch: 196 	 Loss: 13.689347
Train Epoch: 197 	 Loss: 13.403845
Train Epoch: 198 	 Loss: 13.245087
Train Epoch: 199 	 Loss: 13.292705
Train Epoch: 200 	 Loss: 13.285088
Train Epoch: 201 	 Loss: 13.444287
Train Epoch: 202 	 Loss: 13.869116
Train Epoch: 203 	 Loss: 13.397882
Train Epoch: 204 	 Loss: 13.345118
Train Epoch: 205 	 Loss: 13.645317
Train Epoch: 206 	 Loss: 13.177015
Train Epoch: 207 	 Loss: 13.773254
Train Epoch: 208 	 Loss: 13.314092
Train Epoch: 209 	 Loss: 12.726496
Train Epoch: 210 	 Loss: 13.324970
Train Epoch: 211 	 Loss: 12.771919
Train Epoch: 212 	 Loss: 13.305837
Train Epoch: 213 	 Loss: 13.574560
Train Epoch: 214 	 Loss: 14.331221
Train Epoch: 215 	 Loss: 13.194778
Train Epoch: 216 	 Loss: 13.289933
Train Epoch: 217 	 Loss: 12.817841
Train Epoch: 218 	 Loss: 13.392612
Train Epoch: 219 	 Loss: 13.203257
Train Epoch: 220 	 Loss: 13.549139
Train Epoch: 221 	 Loss: 13.446241
Train Epoch: 222 	 Loss: 12.766684
Train Epoch: 223 	 Loss: 13.112660
Train Epoch: 224 	 Loss: 12.666991
Train Epoch: 225 	 Loss: 13.192851
Train Epoch: 226 	 Loss: 14.349586
Train Epoch: 227 	 Loss: 13.326309
Train Epoch: 228 	 Loss: 12.942039
Train Epoch: 229 	 Loss: 13.500198
Train Epoch: 230 	 Loss: 13.595864
Train Epoch: 231 	 Loss: 13.530331
Train Epoch: 232 	 Loss: 13.430475
Train Epoch: 233 	 Loss: 13.405367
Train Epoch: 234 	 Loss: 12.691025
Train Epoch: 235 	 Loss: 13.257773
Train Epoch: 236 	 Loss: 13.372388
Train Epoch: 237 	 Loss: 14.091211
Train Epoch: 238 	 Loss: 13.060781
Train Epoch: 239 	 Loss: 13.500484
Train Epoch: 240 	 Loss: 13.572968
Train Epoch: 241 	 Loss: 13.198846
Train Epoch: 242 	 Loss: 13.284174
Train Epoch: 243 	 Loss: 14.923840
Train Epoch: 244 	 Loss: 13.712606
Train Epoch: 245 	 Loss: 14.503181
Train Epoch: 246 	 Loss: 13.165760
Train Epoch: 247 	 Loss: 14.130295
Train Epoch: 248 	 Loss: 13.656075
Train Epoch: 249 	 Loss: 13.870846
Train Epoch: 250 	 Loss: 13.901668
Train Epoch: 251 	 Loss: 13.708530
Train Epoch: 252 	 Loss: 13.558874
Train Epoch: 253 	 Loss: 13.498236
Train Epoch: 254 	 Loss: 13.456225
Train Epoch: 255 	 Loss: 13.580912
Train Epoch: 256 	 Loss: 12.804926
Train Epoch: 257 	 Loss: 13.360126
Train Epoch: 258 	 Loss: 13.068739
Train Epoch: 259 	 Loss: 13.410250
Train Epoch: 260 	 Loss: 12.939671
Train Epoch: 261 	 Loss: 13.639965
Train Epoch: 262 	 Loss: 13.600307
Train Epoch: 263 	 Loss: 13.959731
Train Epoch: 264 	 Loss: 13.685271
Train Epoch: 265 	 Loss: 13.657623
Train Epoch: 266 	 Loss: 12.644915
Train Epoch: 267 	 Loss: 12.986082
Train Epoch: 268 	 Loss: 13.577084
Train Epoch: 269 	 Loss: 13.646499
Train Epoch: 270 	 Loss: 13.299040
Train Epoch: 271 	 Loss: 13.720741
Train Epoch: 272 	 Loss: 13.193949
Train Epoch: 273 	 Loss: 13.845388
Train Epoch: 274 	 Loss: 12.932467
Train Epoch: 275 	 Loss: 13.357550
Train Epoch: 276 	 Loss: 13.626066
Train Epoch: 277 	 Loss: 13.392025
Train Epoch: 278 	 Loss: 13.176950
Train Epoch: 279 	 Loss: 12.626060
Train Epoch: 280 	 Loss: 13.596745
Train Epoch: 281 	 Loss: 13.147026
Train Epoch: 282 	 Loss: 14.415359
Train Epoch: 283 	 Loss: 13.383842
Train Epoch: 284 	 Loss: 14.114559
Train Epoch: 285 	 Loss: 13.065468
Train Epoch: 286 	 Loss: 13.226755
Train Epoch: 287 	 Loss: 13.399182
Train Epoch: 288 	 Loss: 13.318655
Train Epoch: 289 	 Loss: 13.664594
Train Epoch: 290 	 Loss: 13.102159
Train Epoch: 291 	 Loss: 12.727465
Train Epoch: 292 	 Loss: 13.043482
Train Epoch: 293 	 Loss: 13.244745
Train Epoch: 294 	 Loss: 13.293561
Train Epoch: 295 	 Loss: 13.124950
Train Epoch: 296 	 Loss: 13.237108
Train Epoch: 297 	 Loss: 13.050041
Train Epoch: 298 	 Loss: 13.248828
Train Epoch: 299 	 Loss: 13.263035
Train Epoch: 300 	 Loss: 13.829609
Train Epoch: 301 	 Loss: 14.430974
Train Epoch: 302 	 Loss: 13.069059
Train Epoch: 303 	 Loss: 13.159220
Train Epoch: 304 	 Loss: 13.290404
Train Epoch: 305 	 Loss: 13.514929
Train Epoch: 306 	 Loss: 13.119548
Train Epoch: 307 	 Loss: 13.599604
Train Epoch: 308 	 Loss: 13.015360
Train Epoch: 309 	 Loss: 13.533871
Train Epoch: 310 	 Loss: 12.996015
Train Epoch: 311 	 Loss: 13.345364
Train Epoch: 312 	 Loss: 13.002274
Train Epoch: 313 	 Loss: 13.049219
Train Epoch: 314 	 Loss: 13.044156
Train Epoch: 315 	 Loss: 13.865728
Train Epoch: 316 	 Loss: 13.039987
Train Epoch: 317 	 Loss: 13.278341
Train Epoch: 318 	 Loss: 13.134642
Train Epoch: 319 	 Loss: 13.243843
Train Epoch: 320 	 Loss: 13.423134
Train Epoch: 321 	 Loss: 13.046688
Train Epoch: 322 	 Loss: 13.290130
Train Epoch: 323 	 Loss: 13.080879
Train Epoch: 324 	 Loss: 13.413082
Train Epoch: 325 	 Loss: 12.887567
Train Epoch: 326 	 Loss: 13.736369
Train Epoch: 327 	 Loss: 12.370338
Train Epoch: 328 	 Loss: 13.173964
Train Epoch: 329 	 Loss: 12.955870
Train Epoch: 330 	 Loss: 13.478133
Train Epoch: 331 	 Loss: 13.182015
Train Epoch: 332 	 Loss: 13.266466
Train Epoch: 333 	 Loss: 13.607949
Train Epoch: 334 	 Loss: 13.034945
Train Epoch: 335 	 Loss: 12.898496
Train Epoch: 336 	 Loss: 12.881033
Train Epoch: 337 	 Loss: 13.206976
Train Epoch: 338 	 Loss: 13.141182
Train Epoch: 339 	 Loss: 13.357407
Train Epoch: 340 	 Loss: 12.946120
Train Epoch: 341 	 Loss: 13.113228
Train Epoch: 342 	 Loss: 13.285141
Train Epoch: 343 	 Loss: 12.669506
Train Epoch: 344 	 Loss: 12.934641
Train Epoch: 345 	 Loss: 12.986763
Train Epoch: 346 	 Loss: 12.749534
Train Epoch: 347 	 Loss: 13.104900
Train Epoch: 348 	 Loss: 13.311867
Train Epoch: 349 	 Loss: 13.089212
Train Epoch: 350 	 Loss: 13.211850
Train Epoch: 351 	 Loss: 12.997744
Train Epoch: 352 	 Loss: 12.525684
Train Epoch: 353 	 Loss: 13.033690
Train Epoch: 354 	 Loss: 13.399338
Train Epoch: 355 	 Loss: 13.472315
Train Epoch: 356 	 Loss: 13.073248
Train Epoch: 357 	 Loss: 13.060077
Train Epoch: 358 	 Loss: 13.181204
Train Epoch: 359 	 Loss: 13.295573
Train Epoch: 360 	 Loss: 13.121058
Train Epoch: 361 	 Loss: 13.368313
Train Epoch: 362 	 Loss: 13.194552
Train Epoch: 363 	 Loss: 13.097130
Train Epoch: 364 	 Loss: 13.141133
Train Epoch: 365 	 Loss: 13.016975
Train Epoch: 366 	 Loss: 13.044476
Train Epoch: 367 	 Loss: 13.119603
Train Epoch: 368 	 Loss: 13.080972
Train Epoch: 369 	 Loss: 13.549504
Train Epoch: 370 	 Loss: 13.215013
Train Epoch: 371 	 Loss: 13.243477
Train Epoch: 372 	 Loss: 13.022461
Train Epoch: 373 	 Loss: 13.057812
Train Epoch: 374 	 Loss: 12.983854
Train Epoch: 375 	 Loss: 13.203724
Train Epoch: 376 	 Loss: 13.378312
Train Epoch: 377 	 Loss: 13.130318
Train Epoch: 378 	 Loss: 13.108544
Train Epoch: 379 	 Loss: 13.408895
Train Epoch: 380 	 Loss: 13.605325
Train Epoch: 381 	 Loss: 13.151729
Train Epoch: 382 	 Loss: 13.404158
Train Epoch: 383 	 Loss: 13.110558
Train Epoch: 384 	 Loss: 13.107905
Train Epoch: 385 	 Loss: 13.743841
Train Epoch: 386 	 Loss: 12.387745
Train Epoch: 387 	 Loss: 13.257529
Train Epoch: 388 	 Loss: 13.322794
Train Epoch: 389 	 Loss: 13.045223
Train Epoch: 390 	 Loss: 13.376995
Train Epoch: 391 	 Loss: 13.104308
Train Epoch: 392 	 Loss: 13.247263
Train Epoch: 393 	 Loss: 13.388967
Train Epoch: 394 	 Loss: 12.920794
Train Epoch: 395 	 Loss: 12.984132
Train Epoch: 396 	 Loss: 13.169553
Train Epoch: 397 	 Loss: 12.830476
Train Epoch: 398 	 Loss: 13.033094
Train Epoch: 399 	 Loss: 13.007830

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.890

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.489

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.905

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.776

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.948

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.909

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.894

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.872

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.902

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.873

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.774

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.894
------
f1 mean across methods is 0.844


Repetition:  4
 • seed                     - 4
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 400
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.25
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=4_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.25_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [4.88766768e-01 4.84896435e-01 2.05562446e-02 5.52395615e-03
 2.56596673e-04] 

Train Epoch: 0 	 Loss: 17.726551
Train Epoch: 1 	 Loss: 16.852055
Train Epoch: 2 	 Loss: 15.251091
Train Epoch: 3 	 Loss: 13.787664
Train Epoch: 4 	 Loss: 12.755552
Train Epoch: 5 	 Loss: 12.755211
Train Epoch: 6 	 Loss: 12.442509
Train Epoch: 7 	 Loss: 12.878408
Train Epoch: 8 	 Loss: 12.342591
Train Epoch: 9 	 Loss: 12.228683
Train Epoch: 10 	 Loss: 12.198625
Train Epoch: 11 	 Loss: 12.482255
Train Epoch: 12 	 Loss: 12.263735
Train Epoch: 13 	 Loss: 12.004357
Train Epoch: 14 	 Loss: 12.175179
Train Epoch: 15 	 Loss: 12.026472
Train Epoch: 16 	 Loss: 12.127830
Train Epoch: 17 	 Loss: 11.974459
Train Epoch: 18 	 Loss: 12.941969
Train Epoch: 19 	 Loss: 12.103481
Train Epoch: 20 	 Loss: 12.215652
Train Epoch: 21 	 Loss: 12.201985
Train Epoch: 22 	 Loss: 12.062256
Train Epoch: 23 	 Loss: 11.988396
Train Epoch: 24 	 Loss: 12.316713
Train Epoch: 25 	 Loss: 12.156515
Train Epoch: 26 	 Loss: 12.547850
Train Epoch: 27 	 Loss: 12.399414
Train Epoch: 28 	 Loss: 12.142019
Train Epoch: 29 	 Loss: 12.041842
Train Epoch: 30 	 Loss: 12.006094
Train Epoch: 31 	 Loss: 12.042973
Train Epoch: 32 	 Loss: 12.156116
Train Epoch: 33 	 Loss: 12.004915
Train Epoch: 34 	 Loss: 12.234739
Train Epoch: 35 	 Loss: 12.062786
Train Epoch: 36 	 Loss: 12.503202
Train Epoch: 37 	 Loss: 11.965717
Train Epoch: 38 	 Loss: 12.189357
Train Epoch: 39 	 Loss: 12.047858
Train Epoch: 40 	 Loss: 12.534321
Train Epoch: 41 	 Loss: 12.142183
Train Epoch: 42 	 Loss: 11.997964
Train Epoch: 43 	 Loss: 12.405077
Train Epoch: 44 	 Loss: 12.391116
Train Epoch: 45 	 Loss: 12.009840
Train Epoch: 46 	 Loss: 12.142086
Train Epoch: 47 	 Loss: 12.525125
Train Epoch: 48 	 Loss: 12.808571
Train Epoch: 49 	 Loss: 11.900171
Train Epoch: 50 	 Loss: 12.258106
Train Epoch: 51 	 Loss: 12.184895
Train Epoch: 52 	 Loss: 11.934177
Train Epoch: 53 	 Loss: 12.335939
Train Epoch: 54 	 Loss: 12.079015
Train Epoch: 55 	 Loss: 12.843347
Train Epoch: 56 	 Loss: 12.011471
Train Epoch: 57 	 Loss: 12.040306
Train Epoch: 58 	 Loss: 12.180357
Train Epoch: 59 	 Loss: 11.973038
Train Epoch: 60 	 Loss: 12.384482
Train Epoch: 61 	 Loss: 12.122494
Train Epoch: 62 	 Loss: 12.267460
Train Epoch: 63 	 Loss: 12.345045
Train Epoch: 64 	 Loss: 12.085287
Train Epoch: 65 	 Loss: 12.254643
Train Epoch: 66 	 Loss: 12.702975
Train Epoch: 67 	 Loss: 11.917942
Train Epoch: 68 	 Loss: 12.019677
Train Epoch: 69 	 Loss: 11.893729
Train Epoch: 70 	 Loss: 12.385242
Train Epoch: 71 	 Loss: 12.134167
Train Epoch: 72 	 Loss: 12.072594
Train Epoch: 73 	 Loss: 11.894129
Train Epoch: 74 	 Loss: 12.148825
Train Epoch: 75 	 Loss: 12.012847
Train Epoch: 76 	 Loss: 12.036187
Train Epoch: 77 	 Loss: 12.245411
Train Epoch: 78 	 Loss: 12.092705
Train Epoch: 79 	 Loss: 12.200114
Train Epoch: 80 	 Loss: 11.956991
Train Epoch: 81 	 Loss: 12.164297
Train Epoch: 82 	 Loss: 12.170397
Train Epoch: 83 	 Loss: 12.137056
Train Epoch: 84 	 Loss: 12.002857
Train Epoch: 85 	 Loss: 12.186176
Train Epoch: 86 	 Loss: 12.336901
Train Epoch: 87 	 Loss: 12.341087
Train Epoch: 88 	 Loss: 12.013062
Train Epoch: 89 	 Loss: 11.899192
Train Epoch: 90 	 Loss: 12.101987
Train Epoch: 91 	 Loss: 11.977497
Train Epoch: 92 	 Loss: 11.898674
Train Epoch: 93 	 Loss: 12.369620
Train Epoch: 94 	 Loss: 11.984470
Train Epoch: 95 	 Loss: 12.243538
Train Epoch: 96 	 Loss: 11.913017
Train Epoch: 97 	 Loss: 12.628552
Train Epoch: 98 	 Loss: 11.943989
Train Epoch: 99 	 Loss: 12.815931
Train Epoch: 100 	 Loss: 11.981176
Train Epoch: 101 	 Loss: 11.945082
Train Epoch: 102 	 Loss: 12.144684
Train Epoch: 103 	 Loss: 12.082932
Train Epoch: 104 	 Loss: 11.968569
Train Epoch: 105 	 Loss: 12.029568
Train Epoch: 106 	 Loss: 11.905489
Train Epoch: 107 	 Loss: 11.993640
Train Epoch: 108 	 Loss: 11.980158
Train Epoch: 109 	 Loss: 12.181135
Train Epoch: 110 	 Loss: 12.169563
Train Epoch: 111 	 Loss: 11.998789
Train Epoch: 112 	 Loss: 12.150748
Train Epoch: 113 	 Loss: 12.562806
Train Epoch: 114 	 Loss: 11.970197
Train Epoch: 115 	 Loss: 12.275225
Train Epoch: 116 	 Loss: 12.300756
Train Epoch: 117 	 Loss: 12.547758
Train Epoch: 118 	 Loss: 12.327947
Train Epoch: 119 	 Loss: 12.355270
Train Epoch: 120 	 Loss: 12.465024
Train Epoch: 121 	 Loss: 11.953240
Train Epoch: 122 	 Loss: 12.304928
Train Epoch: 123 	 Loss: 12.057477
Train Epoch: 124 	 Loss: 12.855304
Train Epoch: 125 	 Loss: 11.927992
Train Epoch: 126 	 Loss: 12.063910
Train Epoch: 127 	 Loss: 12.128384
Train Epoch: 128 	 Loss: 12.093307
Train Epoch: 129 	 Loss: 12.218595
Train Epoch: 130 	 Loss: 12.234236
Train Epoch: 131 	 Loss: 12.227608
Train Epoch: 132 	 Loss: 12.005854
Train Epoch: 133 	 Loss: 11.850712
Train Epoch: 134 	 Loss: 12.565125
Train Epoch: 135 	 Loss: 12.081106
Train Epoch: 136 	 Loss: 11.966476
Train Epoch: 137 	 Loss: 12.102818
Train Epoch: 138 	 Loss: 12.182167
Train Epoch: 139 	 Loss: 11.944941
Train Epoch: 140 	 Loss: 12.109787
Train Epoch: 141 	 Loss: 12.001818
Train Epoch: 142 	 Loss: 11.954960
Train Epoch: 143 	 Loss: 11.994402
Train Epoch: 144 	 Loss: 12.105102
Train Epoch: 145 	 Loss: 12.439602
Train Epoch: 146 	 Loss: 12.308357
Train Epoch: 147 	 Loss: 11.835320
Train Epoch: 148 	 Loss: 11.978914
Train Epoch: 149 	 Loss: 11.966505
Train Epoch: 150 	 Loss: 12.071952
Train Epoch: 151 	 Loss: 11.909390
Train Epoch: 152 	 Loss: 11.964940
Train Epoch: 153 	 Loss: 11.936111
Train Epoch: 154 	 Loss: 11.984908
Train Epoch: 155 	 Loss: 12.111786
Train Epoch: 156 	 Loss: 11.914867
Train Epoch: 157 	 Loss: 12.071932
Train Epoch: 158 	 Loss: 11.999070
Train Epoch: 159 	 Loss: 12.055592
Train Epoch: 160 	 Loss: 12.565979
Train Epoch: 161 	 Loss: 11.942746
Train Epoch: 162 	 Loss: 12.387260
Train Epoch: 163 	 Loss: 11.903602
Train Epoch: 164 	 Loss: 12.014425
Train Epoch: 165 	 Loss: 12.227363
Train Epoch: 166 	 Loss: 12.260357
Train Epoch: 167 	 Loss: 11.869459
Train Epoch: 168 	 Loss: 11.955117
Train Epoch: 169 	 Loss: 11.879093
Train Epoch: 170 	 Loss: 11.869543
Train Epoch: 171 	 Loss: 11.958454
Train Epoch: 172 	 Loss: 12.228489
Train Epoch: 173 	 Loss: 11.815141
Train Epoch: 174 	 Loss: 11.907339
Train Epoch: 175 	 Loss: 11.983034
Train Epoch: 176 	 Loss: 12.437769
Train Epoch: 177 	 Loss: 12.263769
Train Epoch: 178 	 Loss: 13.278246
Train Epoch: 179 	 Loss: 12.210711
Train Epoch: 180 	 Loss: 12.154256
Train Epoch: 181 	 Loss: 12.214664
Train Epoch: 182 	 Loss: 12.246648
Train Epoch: 183 	 Loss: 11.878655
Train Epoch: 184 	 Loss: 12.231052
Train Epoch: 185 	 Loss: 12.077493
Train Epoch: 186 	 Loss: 12.009344
Train Epoch: 187 	 Loss: 12.152221
Train Epoch: 188 	 Loss: 12.098022
Train Epoch: 189 	 Loss: 12.012239
Train Epoch: 190 	 Loss: 12.021244
Train Epoch: 191 	 Loss: 12.035732
Train Epoch: 192 	 Loss: 11.880949
Train Epoch: 193 	 Loss: 11.898832
Train Epoch: 194 	 Loss: 12.280181
Train Epoch: 195 	 Loss: 11.915962
Train Epoch: 196 	 Loss: 12.156415
Train Epoch: 197 	 Loss: 12.045891
Train Epoch: 198 	 Loss: 12.145088
Train Epoch: 199 	 Loss: 11.931733
Train Epoch: 200 	 Loss: 11.932353
Train Epoch: 201 	 Loss: 12.188681
Train Epoch: 202 	 Loss: 12.176026
Train Epoch: 203 	 Loss: 11.831690
Train Epoch: 204 	 Loss: 12.134559
Train Epoch: 205 	 Loss: 12.017548
Train Epoch: 206 	 Loss: 12.020479
Train Epoch: 207 	 Loss: 12.005299
Train Epoch: 208 	 Loss: 11.926851
Train Epoch: 209 	 Loss: 11.930325
Train Epoch: 210 	 Loss: 11.980021
Train Epoch: 211 	 Loss: 12.196715
Train Epoch: 212 	 Loss: 11.906363
Train Epoch: 213 	 Loss: 12.252215
Train Epoch: 214 	 Loss: 11.989128
Train Epoch: 215 	 Loss: 11.981698
Train Epoch: 216 	 Loss: 11.947294
Train Epoch: 217 	 Loss: 12.087944
Train Epoch: 218 	 Loss: 11.866665
Train Epoch: 219 	 Loss: 11.891624
Train Epoch: 220 	 Loss: 11.973286
Train Epoch: 221 	 Loss: 12.387224
Train Epoch: 222 	 Loss: 12.034517
Train Epoch: 223 	 Loss: 12.255964
Train Epoch: 224 	 Loss: 11.931297
Train Epoch: 225 	 Loss: 12.327908
Train Epoch: 226 	 Loss: 12.000231
Train Epoch: 227 	 Loss: 12.013484
Train Epoch: 228 	 Loss: 11.956753
Train Epoch: 229 	 Loss: 12.066195
Train Epoch: 230 	 Loss: 12.015995
Train Epoch: 231 	 Loss: 11.906365
Train Epoch: 232 	 Loss: 11.873635
Train Epoch: 233 	 Loss: 11.882850
Train Epoch: 234 	 Loss: 11.803720
Train Epoch: 235 	 Loss: 12.089173
Train Epoch: 236 	 Loss: 11.986919
Train Epoch: 237 	 Loss: 11.936474
Train Epoch: 238 	 Loss: 12.001909
Train Epoch: 239 	 Loss: 11.921495
Train Epoch: 240 	 Loss: 12.300855
Train Epoch: 241 	 Loss: 11.924386
Train Epoch: 242 	 Loss: 11.981673
Train Epoch: 243 	 Loss: 11.950563
Train Epoch: 244 	 Loss: 12.335159
Train Epoch: 245 	 Loss: 11.974125
Train Epoch: 246 	 Loss: 12.230614
Train Epoch: 247 	 Loss: 12.158785
Train Epoch: 248 	 Loss: 11.909010
Train Epoch: 249 	 Loss: 12.119757
Train Epoch: 250 	 Loss: 12.018038
Train Epoch: 251 	 Loss: 11.920950
Train Epoch: 252 	 Loss: 11.842715
Train Epoch: 253 	 Loss: 12.182467
Train Epoch: 254 	 Loss: 12.139889
Train Epoch: 255 	 Loss: 11.973987
Train Epoch: 256 	 Loss: 11.912601
Train Epoch: 257 	 Loss: 11.829446
Train Epoch: 258 	 Loss: 11.943972
Train Epoch: 259 	 Loss: 11.973375
Train Epoch: 260 	 Loss: 12.212420
Train Epoch: 261 	 Loss: 12.080452
Train Epoch: 262 	 Loss: 12.495992
Train Epoch: 263 	 Loss: 11.929276
Train Epoch: 264 	 Loss: 11.921988
Train Epoch: 265 	 Loss: 12.177620
Train Epoch: 266 	 Loss: 11.958366
Train Epoch: 267 	 Loss: 12.175892
Train Epoch: 268 	 Loss: 11.997206
Train Epoch: 269 	 Loss: 11.878309
Train Epoch: 270 	 Loss: 12.035687
Train Epoch: 271 	 Loss: 12.331513
Train Epoch: 272 	 Loss: 12.009575
Train Epoch: 273 	 Loss: 11.899786
Train Epoch: 274 	 Loss: 11.859661
Train Epoch: 275 	 Loss: 11.866374
Train Epoch: 276 	 Loss: 12.102852
Train Epoch: 277 	 Loss: 11.992786
Train Epoch: 278 	 Loss: 12.296919
Train Epoch: 279 	 Loss: 11.938087
Train Epoch: 280 	 Loss: 12.597813
Train Epoch: 281 	 Loss: 12.749454
Train Epoch: 282 	 Loss: 11.966206
Train Epoch: 283 	 Loss: 11.881191
Train Epoch: 284 	 Loss: 12.029186
Train Epoch: 285 	 Loss: 11.923138
Train Epoch: 286 	 Loss: 12.104370
Train Epoch: 287 	 Loss: 11.918139
Train Epoch: 288 	 Loss: 12.063185
Train Epoch: 289 	 Loss: 12.157821
Train Epoch: 290 	 Loss: 12.051411
Train Epoch: 291 	 Loss: 11.868786
Train Epoch: 292 	 Loss: 12.037128
Train Epoch: 293 	 Loss: 11.996343
Train Epoch: 294 	 Loss: 12.160288
Train Epoch: 295 	 Loss: 12.388287
Train Epoch: 296 	 Loss: 12.017458
Train Epoch: 297 	 Loss: 12.033318
Train Epoch: 298 	 Loss: 12.210859
Train Epoch: 299 	 Loss: 11.948744
Train Epoch: 300 	 Loss: 12.088224
Train Epoch: 301 	 Loss: 12.072252
Train Epoch: 302 	 Loss: 12.115269
Train Epoch: 303 	 Loss: 12.000111
Train Epoch: 304 	 Loss: 11.845339
Train Epoch: 305 	 Loss: 12.227281
Train Epoch: 306 	 Loss: 11.999765
Train Epoch: 307 	 Loss: 11.962460
Train Epoch: 308 	 Loss: 13.313139
Train Epoch: 309 	 Loss: 12.125757
Train Epoch: 310 	 Loss: 12.146235
Train Epoch: 311 	 Loss: 11.919882
Train Epoch: 312 	 Loss: 12.095840
Train Epoch: 313 	 Loss: 11.816174
Train Epoch: 314 	 Loss: 11.893714
Train Epoch: 315 	 Loss: 12.638311
Train Epoch: 316 	 Loss: 11.921936
Train Epoch: 317 	 Loss: 12.060766
Train Epoch: 318 	 Loss: 11.980084
Train Epoch: 319 	 Loss: 11.866638
Train Epoch: 320 	 Loss: 12.129564
Train Epoch: 321 	 Loss: 12.218354
Train Epoch: 322 	 Loss: 11.843821
Train Epoch: 323 	 Loss: 11.990773
Train Epoch: 324 	 Loss: 12.028981
Train Epoch: 325 	 Loss: 11.801933
Train Epoch: 326 	 Loss: 12.710353
Train Epoch: 327 	 Loss: 11.829092
Train Epoch: 328 	 Loss: 11.901979
Train Epoch: 329 	 Loss: 11.888993
Train Epoch: 330 	 Loss: 11.863460
Train Epoch: 331 	 Loss: 12.124676
Train Epoch: 332 	 Loss: 11.864540
Train Epoch: 333 	 Loss: 11.956089
Train Epoch: 334 	 Loss: 12.123073
Train Epoch: 335 	 Loss: 11.974527
Train Epoch: 336 	 Loss: 12.221959
Train Epoch: 337 	 Loss: 11.866320
Train Epoch: 338 	 Loss: 11.996772
Train Epoch: 339 	 Loss: 11.853478
Train Epoch: 340 	 Loss: 12.162491
Train Epoch: 341 	 Loss: 12.336945
Train Epoch: 342 	 Loss: 11.840089
Train Epoch: 343 	 Loss: 11.847580
Train Epoch: 344 	 Loss: 12.479763
Train Epoch: 345 	 Loss: 11.878267
Train Epoch: 346 	 Loss: 11.848072
Train Epoch: 347 	 Loss: 12.099264
Train Epoch: 348 	 Loss: 12.295517
Train Epoch: 349 	 Loss: 11.984622
Train Epoch: 350 	 Loss: 11.812124
Train Epoch: 351 	 Loss: 11.867072
Train Epoch: 352 	 Loss: 11.945015
Train Epoch: 353 	 Loss: 12.177401
Train Epoch: 354 	 Loss: 11.821847
Train Epoch: 355 	 Loss: 12.004277
Train Epoch: 356 	 Loss: 11.993781
Train Epoch: 357 	 Loss: 11.964382
Train Epoch: 358 	 Loss: 12.056073
Train Epoch: 359 	 Loss: 11.916101
Train Epoch: 360 	 Loss: 12.081927
Train Epoch: 361 	 Loss: 11.857393
Train Epoch: 362 	 Loss: 11.926601
Train Epoch: 363 	 Loss: 11.930205
Train Epoch: 364 	 Loss: 12.384518
Train Epoch: 365 	 Loss: 11.925048
Train Epoch: 366 	 Loss: 11.897757
Train Epoch: 367 	 Loss: 12.086105
Train Epoch: 368 	 Loss: 12.529243
Train Epoch: 369 	 Loss: 12.128866
Train Epoch: 370 	 Loss: 12.187933
Train Epoch: 371 	 Loss: 11.844967
Train Epoch: 372 	 Loss: 12.009027
Train Epoch: 373 	 Loss: 11.816598
Train Epoch: 374 	 Loss: 12.341547
Train Epoch: 375 	 Loss: 11.929190
Train Epoch: 376 	 Loss: 11.891653
Train Epoch: 377 	 Loss: 11.841837
Train Epoch: 378 	 Loss: 12.030391
Train Epoch: 379 	 Loss: 12.307007
Train Epoch: 380 	 Loss: 11.904833
Train Epoch: 381 	 Loss: 11.828562
Train Epoch: 382 	 Loss: 11.987455
Train Epoch: 383 	 Loss: 11.861517
Train Epoch: 384 	 Loss: 11.913593
Train Epoch: 385 	 Loss: 11.931759
Train Epoch: 386 	 Loss: 12.201810
Train Epoch: 387 	 Loss: 11.988704
Train Epoch: 388 	 Loss: 11.800018
Train Epoch: 389 	 Loss: 12.205034
Train Epoch: 390 	 Loss: 12.176090
Train Epoch: 391 	 Loss: 11.920297
Train Epoch: 392 	 Loss: 11.891518
Train Epoch: 393 	 Loss: 12.022061
Train Epoch: 394 	 Loss: 12.182424
Train Epoch: 395 	 Loss: 12.130560
Train Epoch: 396 	 Loss: 11.925219
Train Epoch: 397 	 Loss: 11.931955
Train Epoch: 398 	 Loss: 11.841788
Train Epoch: 399 	 Loss: 11.852821

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.863

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.504

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.848

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.868

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.896

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.910

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.886

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.870

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.879

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.878

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.849

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.865
------
f1 mean across methods is 0.843


 ---------------------------------------- 


For each of the methods
Average F1:  [0.87498506 0.55933753 0.87812605 0.84841682 0.90893049 0.78393111
 0.91339299 0.8865934  0.90084979 0.89319153 0.84535213 0.89884636]
Std F1:  [0.00990553 0.06958857 0.03653666 0.03656311 0.02777612 0.24375448
 0.02397167 0.01644424 0.01214345 0.01780797 0.03673274 0.01937805]
Average over repetitions across all methods
Average f1 score:  0.8493294379170392
Std F1:  0.020872356987370028

 -------------------------------------------------------------------------------- 



{'batch_rate': 0.01, 'kernel_length': 0.005, 'n_epochs': 400, 'order_hermite': 100, 'subsampled_rate': 0.3} 


Repetition:  0
 • seed                     - 0
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 400
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=0_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33142857e-01 4.43051948e-01 1.84025974e-02 5.18831169e-03
 2.14285714e-04] 

Train Epoch: 0 	 Loss: 19.182297
Train Epoch: 1 	 Loss: 19.925909
Train Epoch: 2 	 Loss: 18.011078
Train Epoch: 3 	 Loss: 17.885292
Train Epoch: 4 	 Loss: 16.166082
Train Epoch: 5 	 Loss: 15.302275
Train Epoch: 6 	 Loss: 15.047758
Train Epoch: 7 	 Loss: 15.101767
Train Epoch: 8 	 Loss: 14.762222
Train Epoch: 9 	 Loss: 14.629263
Train Epoch: 10 	 Loss: 14.465895
Train Epoch: 11 	 Loss: 14.503386
Train Epoch: 12 	 Loss: 14.885658
Train Epoch: 13 	 Loss: 13.796076
Train Epoch: 14 	 Loss: 14.628799
Train Epoch: 15 	 Loss: 14.594286
Train Epoch: 16 	 Loss: 14.229370
Train Epoch: 17 	 Loss: 14.195208
Train Epoch: 18 	 Loss: 14.015147
Train Epoch: 19 	 Loss: 14.524124
Train Epoch: 20 	 Loss: 14.810912
Train Epoch: 21 	 Loss: 14.859655
Train Epoch: 22 	 Loss: 14.654513
Train Epoch: 23 	 Loss: 14.363371
Train Epoch: 24 	 Loss: 14.624424
Train Epoch: 25 	 Loss: 14.236513
Train Epoch: 26 	 Loss: 14.352546
Train Epoch: 27 	 Loss: 14.284415
Train Epoch: 28 	 Loss: 14.328091
Train Epoch: 29 	 Loss: 14.252251
Train Epoch: 30 	 Loss: 14.268522
Train Epoch: 31 	 Loss: 14.213526
Train Epoch: 32 	 Loss: 14.507915
Train Epoch: 33 	 Loss: 14.235332
Train Epoch: 34 	 Loss: 14.989943
Train Epoch: 35 	 Loss: 14.651366
Train Epoch: 36 	 Loss: 14.292158
Train Epoch: 37 	 Loss: 14.407872
Train Epoch: 38 	 Loss: 14.301022
Train Epoch: 39 	 Loss: 14.861219
Train Epoch: 40 	 Loss: 14.518208
Train Epoch: 41 	 Loss: 14.314627
Train Epoch: 42 	 Loss: 14.038593
Train Epoch: 43 	 Loss: 15.004812
Train Epoch: 44 	 Loss: 14.353722
Train Epoch: 45 	 Loss: 14.333857
Train Epoch: 46 	 Loss: 14.223498
Train Epoch: 47 	 Loss: 14.176428
Train Epoch: 48 	 Loss: 14.525106
Train Epoch: 49 	 Loss: 14.293725
Train Epoch: 50 	 Loss: 14.339475
Train Epoch: 51 	 Loss: 14.177332
Train Epoch: 52 	 Loss: 14.008434
Train Epoch: 53 	 Loss: 14.268751
Train Epoch: 54 	 Loss: 14.297001
Train Epoch: 55 	 Loss: 14.543484
Train Epoch: 56 	 Loss: 14.607344
Train Epoch: 57 	 Loss: 14.321329
Train Epoch: 58 	 Loss: 14.471803
Train Epoch: 59 	 Loss: 14.274233
Train Epoch: 60 	 Loss: 14.530762
Train Epoch: 61 	 Loss: 14.175652
Train Epoch: 62 	 Loss: 14.456963
Train Epoch: 63 	 Loss: 15.788910
Train Epoch: 64 	 Loss: 13.956719
Train Epoch: 65 	 Loss: 14.865477
Train Epoch: 66 	 Loss: 14.400001
Train Epoch: 67 	 Loss: 14.740448
Train Epoch: 68 	 Loss: 14.294314
Train Epoch: 69 	 Loss: 14.243690
Train Epoch: 70 	 Loss: 14.228090
Train Epoch: 71 	 Loss: 14.415055
Train Epoch: 72 	 Loss: 14.249830
Train Epoch: 73 	 Loss: 14.244004
Train Epoch: 74 	 Loss: 14.418327
Train Epoch: 75 	 Loss: 14.446265
Train Epoch: 76 	 Loss: 14.161519
Train Epoch: 77 	 Loss: 14.258135
Train Epoch: 78 	 Loss: 14.279862
Train Epoch: 79 	 Loss: 14.439163
Train Epoch: 80 	 Loss: 14.372080
Train Epoch: 81 	 Loss: 14.197554
Train Epoch: 82 	 Loss: 14.497747
Train Epoch: 83 	 Loss: 14.264482
Train Epoch: 84 	 Loss: 14.209953
Train Epoch: 85 	 Loss: 14.366648
Train Epoch: 86 	 Loss: 14.308239
Train Epoch: 87 	 Loss: 14.386948
Train Epoch: 88 	 Loss: 14.411894
Train Epoch: 89 	 Loss: 14.447384
Train Epoch: 90 	 Loss: 14.378511
Train Epoch: 91 	 Loss: 14.248311
Train Epoch: 92 	 Loss: 14.389811
Train Epoch: 93 	 Loss: 14.236542
Train Epoch: 94 	 Loss: 14.162409
Train Epoch: 95 	 Loss: 14.527888
Train Epoch: 96 	 Loss: 14.321011
Train Epoch: 97 	 Loss: 14.342417
Train Epoch: 98 	 Loss: 14.226385
Train Epoch: 99 	 Loss: 14.223735
Train Epoch: 100 	 Loss: 14.382645
Train Epoch: 101 	 Loss: 14.403066
Train Epoch: 102 	 Loss: 14.182137
Train Epoch: 103 	 Loss: 14.179083
Train Epoch: 104 	 Loss: 14.183933
Train Epoch: 105 	 Loss: 14.280566
Train Epoch: 106 	 Loss: 14.459961
Train Epoch: 107 	 Loss: 14.865393
Train Epoch: 108 	 Loss: 14.178077
Train Epoch: 109 	 Loss: 14.319013
Train Epoch: 110 	 Loss: 14.315884
Train Epoch: 111 	 Loss: 14.401964
Train Epoch: 112 	 Loss: 14.414223
Train Epoch: 113 	 Loss: 14.360872
Train Epoch: 114 	 Loss: 14.243712
Train Epoch: 115 	 Loss: 14.160231
Train Epoch: 116 	 Loss: 14.429569
Train Epoch: 117 	 Loss: 14.199871
Train Epoch: 118 	 Loss: 14.271884
Train Epoch: 119 	 Loss: 14.173182
Train Epoch: 120 	 Loss: 14.169655
Train Epoch: 121 	 Loss: 14.365591
Train Epoch: 122 	 Loss: 14.270582
Train Epoch: 123 	 Loss: 14.340450
Train Epoch: 124 	 Loss: 14.289092
Train Epoch: 125 	 Loss: 14.207499
Train Epoch: 126 	 Loss: 14.302207
Train Epoch: 127 	 Loss: 14.641612
Train Epoch: 128 	 Loss: 14.437134
Train Epoch: 129 	 Loss: 14.169180
Train Epoch: 130 	 Loss: 14.229736
Train Epoch: 131 	 Loss: 14.162802
Train Epoch: 132 	 Loss: 14.322251
Train Epoch: 133 	 Loss: 14.538157
Train Epoch: 134 	 Loss: 14.477309
Train Epoch: 135 	 Loss: 14.140936
Train Epoch: 136 	 Loss: 14.347044
Train Epoch: 137 	 Loss: 14.282850
Train Epoch: 138 	 Loss: 14.252785
Train Epoch: 139 	 Loss: 14.169847
Train Epoch: 140 	 Loss: 14.271768
Train Epoch: 141 	 Loss: 14.299595
Train Epoch: 142 	 Loss: 14.612449
Train Epoch: 143 	 Loss: 14.380588
Train Epoch: 144 	 Loss: 14.618049
Train Epoch: 145 	 Loss: 14.163151
Train Epoch: 146 	 Loss: 14.358266
Train Epoch: 147 	 Loss: 14.228952
Train Epoch: 148 	 Loss: 14.091082
Train Epoch: 149 	 Loss: 14.350431
Train Epoch: 150 	 Loss: 13.949331
Train Epoch: 151 	 Loss: 13.945993
Train Epoch: 152 	 Loss: 13.988621
Train Epoch: 153 	 Loss: 14.107835
Train Epoch: 154 	 Loss: 14.212389
Train Epoch: 155 	 Loss: 13.997129
Train Epoch: 156 	 Loss: 14.403844
Train Epoch: 157 	 Loss: 14.133247
Train Epoch: 158 	 Loss: 14.078321
Train Epoch: 159 	 Loss: 14.091612
Train Epoch: 160 	 Loss: 14.093208
Train Epoch: 161 	 Loss: 14.363187
Train Epoch: 162 	 Loss: 13.961784
Train Epoch: 163 	 Loss: 14.026411
Train Epoch: 164 	 Loss: 14.299896
Train Epoch: 165 	 Loss: 14.082249
Train Epoch: 166 	 Loss: 13.969764
Train Epoch: 167 	 Loss: 14.561952
Train Epoch: 168 	 Loss: 13.999769
Train Epoch: 169 	 Loss: 14.025495
Train Epoch: 170 	 Loss: 13.942602
Train Epoch: 171 	 Loss: 14.230244
Train Epoch: 172 	 Loss: 13.924000
Train Epoch: 173 	 Loss: 13.952993
Train Epoch: 174 	 Loss: 13.964829
Train Epoch: 175 	 Loss: 13.918642
Train Epoch: 176 	 Loss: 14.120476
Train Epoch: 177 	 Loss: 13.959866
Train Epoch: 178 	 Loss: 14.021305
Train Epoch: 179 	 Loss: 14.137173
Train Epoch: 180 	 Loss: 13.976803
Train Epoch: 181 	 Loss: 13.930804
Train Epoch: 182 	 Loss: 14.355838
Train Epoch: 183 	 Loss: 14.212290
Train Epoch: 184 	 Loss: 14.036118
Train Epoch: 185 	 Loss: 15.422327
Train Epoch: 186 	 Loss: 13.976143
Train Epoch: 187 	 Loss: 14.069843
Train Epoch: 188 	 Loss: 14.138227
Train Epoch: 189 	 Loss: 14.134622
Train Epoch: 190 	 Loss: 13.989307
Train Epoch: 191 	 Loss: 14.199162
Train Epoch: 192 	 Loss: 14.147043
Train Epoch: 193 	 Loss: 13.939325
Train Epoch: 194 	 Loss: 14.140468
Train Epoch: 195 	 Loss: 13.943310
Train Epoch: 196 	 Loss: 13.867651
Train Epoch: 197 	 Loss: 13.996781
Train Epoch: 198 	 Loss: 14.082214
Train Epoch: 199 	 Loss: 14.132036
Train Epoch: 200 	 Loss: 13.976433
Train Epoch: 201 	 Loss: 14.193444
Train Epoch: 202 	 Loss: 14.069771
Train Epoch: 203 	 Loss: 13.924341
Train Epoch: 204 	 Loss: 14.125782
Train Epoch: 205 	 Loss: 14.176531
Train Epoch: 206 	 Loss: 13.971485
Train Epoch: 207 	 Loss: 13.932028
Train Epoch: 208 	 Loss: 14.103147
Train Epoch: 209 	 Loss: 14.322182
Train Epoch: 210 	 Loss: 14.072257
Train Epoch: 211 	 Loss: 13.964139
Train Epoch: 212 	 Loss: 13.953522
Train Epoch: 213 	 Loss: 14.165117
Train Epoch: 214 	 Loss: 14.073714
Train Epoch: 215 	 Loss: 13.934174
Train Epoch: 216 	 Loss: 14.374027
Train Epoch: 217 	 Loss: 13.953781
Train Epoch: 218 	 Loss: 13.975224
Train Epoch: 219 	 Loss: 14.011926
Train Epoch: 220 	 Loss: 13.697037
Train Epoch: 221 	 Loss: 13.970474
Train Epoch: 222 	 Loss: 14.068897
Train Epoch: 223 	 Loss: 13.969193
Train Epoch: 224 	 Loss: 14.090224
Train Epoch: 225 	 Loss: 13.955120
Train Epoch: 226 	 Loss: 13.940523
Train Epoch: 227 	 Loss: 14.718056
Train Epoch: 228 	 Loss: 14.029821
Train Epoch: 229 	 Loss: 13.942378
Train Epoch: 230 	 Loss: 13.900283
Train Epoch: 231 	 Loss: 13.926358
Train Epoch: 232 	 Loss: 14.208001
Train Epoch: 233 	 Loss: 13.963829
Train Epoch: 234 	 Loss: 14.244710
Train Epoch: 235 	 Loss: 14.118596
Train Epoch: 236 	 Loss: 14.160471
Train Epoch: 237 	 Loss: 13.905281
Train Epoch: 238 	 Loss: 14.127384
Train Epoch: 239 	 Loss: 14.055481
Train Epoch: 240 	 Loss: 14.143133
Train Epoch: 241 	 Loss: 14.158564
Train Epoch: 242 	 Loss: 14.087158
Train Epoch: 243 	 Loss: 14.223871
Train Epoch: 244 	 Loss: 13.868018
Train Epoch: 245 	 Loss: 14.174517
Train Epoch: 246 	 Loss: 13.988637
Train Epoch: 247 	 Loss: 13.987881
Train Epoch: 248 	 Loss: 14.408878
Train Epoch: 249 	 Loss: 13.947039
Train Epoch: 250 	 Loss: 13.908497
Train Epoch: 251 	 Loss: 13.945724
Train Epoch: 252 	 Loss: 14.237872
Train Epoch: 253 	 Loss: 14.145906
Train Epoch: 254 	 Loss: 14.104036
Train Epoch: 255 	 Loss: 14.177088
Train Epoch: 256 	 Loss: 13.968528
Train Epoch: 257 	 Loss: 14.153240
Train Epoch: 258 	 Loss: 13.846945
Train Epoch: 259 	 Loss: 14.137471
Train Epoch: 260 	 Loss: 14.228584
Train Epoch: 261 	 Loss: 13.940304
Train Epoch: 262 	 Loss: 13.924196
Train Epoch: 263 	 Loss: 14.151718
Train Epoch: 264 	 Loss: 14.112830
Train Epoch: 265 	 Loss: 13.980186
Train Epoch: 266 	 Loss: 14.000581
Train Epoch: 267 	 Loss: 14.231890
Train Epoch: 268 	 Loss: 13.946997
Train Epoch: 269 	 Loss: 14.071869
Train Epoch: 270 	 Loss: 14.270256
Train Epoch: 271 	 Loss: 14.303164
Train Epoch: 272 	 Loss: 14.135882
Train Epoch: 273 	 Loss: 14.010702
Train Epoch: 274 	 Loss: 14.187388
Train Epoch: 275 	 Loss: 14.603035
Train Epoch: 276 	 Loss: 13.882847
Train Epoch: 277 	 Loss: 14.138275
Train Epoch: 278 	 Loss: 13.913736
Train Epoch: 279 	 Loss: 14.005266
Train Epoch: 280 	 Loss: 14.144062
Train Epoch: 281 	 Loss: 13.981684
Train Epoch: 282 	 Loss: 14.034872
Train Epoch: 283 	 Loss: 14.378112
Train Epoch: 284 	 Loss: 13.914677
Train Epoch: 285 	 Loss: 14.646858
Train Epoch: 286 	 Loss: 14.074348
Train Epoch: 287 	 Loss: 13.933533
Train Epoch: 288 	 Loss: 13.994255
Train Epoch: 289 	 Loss: 14.001732
Train Epoch: 290 	 Loss: 13.924181
Train Epoch: 291 	 Loss: 14.065557
Train Epoch: 292 	 Loss: 13.991174
Train Epoch: 293 	 Loss: 14.037752
Train Epoch: 294 	 Loss: 14.010767
Train Epoch: 295 	 Loss: 14.159716
Train Epoch: 296 	 Loss: 13.919760
Train Epoch: 297 	 Loss: 13.965190
Train Epoch: 298 	 Loss: 14.334656
Train Epoch: 299 	 Loss: 14.150639
Train Epoch: 300 	 Loss: 13.871479
Train Epoch: 301 	 Loss: 13.959458
Train Epoch: 302 	 Loss: 14.148924
Train Epoch: 303 	 Loss: 13.934286
Train Epoch: 304 	 Loss: 14.394161
Train Epoch: 305 	 Loss: 14.152997
Train Epoch: 306 	 Loss: 14.095037
Train Epoch: 307 	 Loss: 14.008997
Train Epoch: 308 	 Loss: 14.001103
Train Epoch: 309 	 Loss: 14.042723
Train Epoch: 310 	 Loss: 13.943590
Train Epoch: 311 	 Loss: 13.989548
Train Epoch: 312 	 Loss: 13.974027
Train Epoch: 313 	 Loss: 13.875704
Train Epoch: 314 	 Loss: 14.065848
Train Epoch: 315 	 Loss: 14.584826
Train Epoch: 316 	 Loss: 14.009559
Train Epoch: 317 	 Loss: 14.297091
Train Epoch: 318 	 Loss: 14.310411
Train Epoch: 319 	 Loss: 14.105190
Train Epoch: 320 	 Loss: 14.199833
Train Epoch: 321 	 Loss: 13.832314
Train Epoch: 322 	 Loss: 14.549498
Train Epoch: 323 	 Loss: 14.038006
Train Epoch: 324 	 Loss: 13.870937
Train Epoch: 325 	 Loss: 13.891268
Train Epoch: 326 	 Loss: 14.161236
Train Epoch: 327 	 Loss: 13.873015
Train Epoch: 328 	 Loss: 14.029388
Train Epoch: 329 	 Loss: 14.293694
Train Epoch: 330 	 Loss: 13.922901
Train Epoch: 331 	 Loss: 13.890992
Train Epoch: 332 	 Loss: 13.862643
Train Epoch: 333 	 Loss: 13.942600
Train Epoch: 334 	 Loss: 14.320625
Train Epoch: 335 	 Loss: 13.832126
Train Epoch: 336 	 Loss: 13.900298
Train Epoch: 337 	 Loss: 13.867399
Train Epoch: 338 	 Loss: 14.032323
Train Epoch: 339 	 Loss: 13.955231
Train Epoch: 340 	 Loss: 13.855743
Train Epoch: 341 	 Loss: 14.029136
Train Epoch: 342 	 Loss: 14.020648
Train Epoch: 343 	 Loss: 13.856734
Train Epoch: 344 	 Loss: 14.155474
Train Epoch: 345 	 Loss: 14.131323
Train Epoch: 346 	 Loss: 14.622395
Train Epoch: 347 	 Loss: 14.234747
Train Epoch: 348 	 Loss: 13.994819
Train Epoch: 349 	 Loss: 14.019390
Train Epoch: 350 	 Loss: 14.172892
Train Epoch: 351 	 Loss: 14.163177
Train Epoch: 352 	 Loss: 14.457119
Train Epoch: 353 	 Loss: 13.879774
Train Epoch: 354 	 Loss: 14.007708
Train Epoch: 355 	 Loss: 13.980069
Train Epoch: 356 	 Loss: 13.999754
Train Epoch: 357 	 Loss: 14.104992
Train Epoch: 358 	 Loss: 13.786408
Train Epoch: 359 	 Loss: 14.057415
Train Epoch: 360 	 Loss: 13.984985
Train Epoch: 361 	 Loss: 13.848494
Train Epoch: 362 	 Loss: 13.901657
Train Epoch: 363 	 Loss: 14.027773
Train Epoch: 364 	 Loss: 14.439819
Train Epoch: 365 	 Loss: 14.314817
Train Epoch: 366 	 Loss: 14.177426
Train Epoch: 367 	 Loss: 14.147156
Train Epoch: 368 	 Loss: 13.814822
Train Epoch: 369 	 Loss: 13.893515
Train Epoch: 370 	 Loss: 13.859951
Train Epoch: 371 	 Loss: 13.940109
Train Epoch: 372 	 Loss: 14.034787
Train Epoch: 373 	 Loss: 14.013161
Train Epoch: 374 	 Loss: 14.012583
Train Epoch: 375 	 Loss: 13.920762
Train Epoch: 376 	 Loss: 13.984211
Train Epoch: 377 	 Loss: 13.966560
Train Epoch: 378 	 Loss: 15.170319
Train Epoch: 379 	 Loss: 13.942314
Train Epoch: 380 	 Loss: 14.028326
Train Epoch: 381 	 Loss: 14.209296
Train Epoch: 382 	 Loss: 14.171192
Train Epoch: 383 	 Loss: 13.856912
Train Epoch: 384 	 Loss: 14.071459
Train Epoch: 385 	 Loss: 14.033541
Train Epoch: 386 	 Loss: 14.014511
Train Epoch: 387 	 Loss: 13.882939
Train Epoch: 388 	 Loss: 14.148472
Train Epoch: 389 	 Loss: 14.301019
Train Epoch: 390 	 Loss: 13.849986
Train Epoch: 391 	 Loss: 13.883474
Train Epoch: 392 	 Loss: 13.897900
Train Epoch: 393 	 Loss: 13.907045
Train Epoch: 394 	 Loss: 13.933601
Train Epoch: 395 	 Loss: 14.018604
Train Epoch: 396 	 Loss: 13.958328
Train Epoch: 397 	 Loss: 13.810190
Train Epoch: 398 	 Loss: 14.455454
Train Epoch: 399 	 Loss: 13.982465

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.884

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.543

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.851

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.878

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.938

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.828

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.928

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.914

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.925

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.899

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.884

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.913
------
f1 mean across methods is 0.865


Repetition:  1
 • seed                     - 1
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 400
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=1_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.34577922e-01 4.41084416e-01 1.90324675e-02 5.05844156e-03
 2.46753247e-04] 

Train Epoch: 0 	 Loss: 19.306217
Train Epoch: 1 	 Loss: 18.174948
Train Epoch: 2 	 Loss: 21.640404
Train Epoch: 3 	 Loss: 17.076584
Train Epoch: 4 	 Loss: 16.298855
Train Epoch: 5 	 Loss: 14.543985
Train Epoch: 6 	 Loss: 14.552161
Train Epoch: 7 	 Loss: 13.872620
Train Epoch: 8 	 Loss: 13.847976
Train Epoch: 9 	 Loss: 13.557743
Train Epoch: 10 	 Loss: 13.653088
Train Epoch: 11 	 Loss: 13.330858
Train Epoch: 12 	 Loss: 13.237198
Train Epoch: 13 	 Loss: 14.264059
Train Epoch: 14 	 Loss: 13.769685
Train Epoch: 15 	 Loss: 13.263828
Train Epoch: 16 	 Loss: 13.311291
Train Epoch: 17 	 Loss: 13.560349
Train Epoch: 18 	 Loss: 12.740332
Train Epoch: 19 	 Loss: 13.104742
Train Epoch: 20 	 Loss: 13.613041
Train Epoch: 21 	 Loss: 13.389460
Train Epoch: 22 	 Loss: 13.325579
Train Epoch: 23 	 Loss: 12.984622
Train Epoch: 24 	 Loss: 14.028595
Train Epoch: 25 	 Loss: 12.918477
Train Epoch: 26 	 Loss: 12.820884
Train Epoch: 27 	 Loss: 13.017984
Train Epoch: 28 	 Loss: 13.401017
Train Epoch: 29 	 Loss: 12.848064
Train Epoch: 30 	 Loss: 12.513447
Train Epoch: 31 	 Loss: 13.455784
Train Epoch: 32 	 Loss: 12.690544
Train Epoch: 33 	 Loss: 12.965310
Train Epoch: 34 	 Loss: 13.588331
Train Epoch: 35 	 Loss: 13.325473
Train Epoch: 36 	 Loss: 12.368029
Train Epoch: 37 	 Loss: 12.412230
Train Epoch: 38 	 Loss: 12.470016
Train Epoch: 39 	 Loss: 12.797384
Train Epoch: 40 	 Loss: 12.841064
Train Epoch: 41 	 Loss: 14.568162
Train Epoch: 42 	 Loss: 12.490898
Train Epoch: 43 	 Loss: 12.408718
Train Epoch: 44 	 Loss: 12.440842
Train Epoch: 45 	 Loss: 12.843075
Train Epoch: 46 	 Loss: 12.427597
Train Epoch: 47 	 Loss: 12.527229
Train Epoch: 48 	 Loss: 12.375070
Train Epoch: 49 	 Loss: 12.314141
Train Epoch: 50 	 Loss: 13.098089
Train Epoch: 51 	 Loss: 12.672066
Train Epoch: 52 	 Loss: 13.390865
Train Epoch: 53 	 Loss: 12.327396
Train Epoch: 54 	 Loss: 12.410561
Train Epoch: 55 	 Loss: 12.313287
Train Epoch: 56 	 Loss: 12.486897
Train Epoch: 57 	 Loss: 12.623997
Train Epoch: 58 	 Loss: 12.400267
Train Epoch: 59 	 Loss: 12.593331
Train Epoch: 60 	 Loss: 12.284480
Train Epoch: 61 	 Loss: 13.082779
Train Epoch: 62 	 Loss: 12.704091
Train Epoch: 63 	 Loss: 12.840901
Train Epoch: 64 	 Loss: 12.710520
Train Epoch: 65 	 Loss: 12.469575
Train Epoch: 66 	 Loss: 12.241847
Train Epoch: 67 	 Loss: 12.635868
Train Epoch: 68 	 Loss: 12.494406
Train Epoch: 69 	 Loss: 12.322832
Train Epoch: 70 	 Loss: 12.470659
Train Epoch: 71 	 Loss: 12.224918
Train Epoch: 72 	 Loss: 12.434675
Train Epoch: 73 	 Loss: 12.270598
Train Epoch: 74 	 Loss: 12.229809
Train Epoch: 75 	 Loss: 12.512053
Train Epoch: 76 	 Loss: 12.182446
Train Epoch: 77 	 Loss: 12.682966
Train Epoch: 78 	 Loss: 12.817094
Train Epoch: 79 	 Loss: 12.322493
Train Epoch: 80 	 Loss: 12.301057
Train Epoch: 81 	 Loss: 12.313087
Train Epoch: 82 	 Loss: 12.428585
Train Epoch: 83 	 Loss: 12.347225
Train Epoch: 84 	 Loss: 12.306422
Train Epoch: 85 	 Loss: 12.402283
Train Epoch: 86 	 Loss: 12.149331
Train Epoch: 87 	 Loss: 12.352114
Train Epoch: 88 	 Loss: 12.232355
Train Epoch: 89 	 Loss: 12.535441
Train Epoch: 90 	 Loss: 12.350525
Train Epoch: 91 	 Loss: 12.234653
Train Epoch: 92 	 Loss: 12.216654
Train Epoch: 93 	 Loss: 12.289480
Train Epoch: 94 	 Loss: 12.542800
Train Epoch: 95 	 Loss: 12.177558
Train Epoch: 96 	 Loss: 12.150476
Train Epoch: 97 	 Loss: 12.728916
Train Epoch: 98 	 Loss: 12.488680
Train Epoch: 99 	 Loss: 13.081201
Train Epoch: 100 	 Loss: 12.499098
Train Epoch: 101 	 Loss: 12.617644
Train Epoch: 102 	 Loss: 12.486065
Train Epoch: 103 	 Loss: 12.192285
Train Epoch: 104 	 Loss: 12.194994
Train Epoch: 105 	 Loss: 12.358424
Train Epoch: 106 	 Loss: 12.465431
Train Epoch: 107 	 Loss: 12.405928
Train Epoch: 108 	 Loss: 12.288551
Train Epoch: 109 	 Loss: 12.201704
Train Epoch: 110 	 Loss: 12.224089
Train Epoch: 111 	 Loss: 12.306055
Train Epoch: 112 	 Loss: 12.438184
Train Epoch: 113 	 Loss: 12.358868
Train Epoch: 114 	 Loss: 12.406422
Train Epoch: 115 	 Loss: 12.450507
Train Epoch: 116 	 Loss: 12.172826
Train Epoch: 117 	 Loss: 12.463835
Train Epoch: 118 	 Loss: 12.462667
Train Epoch: 119 	 Loss: 12.331115
Train Epoch: 120 	 Loss: 12.149035
Train Epoch: 121 	 Loss: 12.226672
Train Epoch: 122 	 Loss: 12.267323
Train Epoch: 123 	 Loss: 12.290940
Train Epoch: 124 	 Loss: 12.566483
Train Epoch: 125 	 Loss: 12.635465
Train Epoch: 126 	 Loss: 12.212361
Train Epoch: 127 	 Loss: 12.323687
Train Epoch: 128 	 Loss: 12.299788
Train Epoch: 129 	 Loss: 12.222798
Train Epoch: 130 	 Loss: 12.270452
Train Epoch: 131 	 Loss: 12.202237
Train Epoch: 132 	 Loss: 12.136258
Train Epoch: 133 	 Loss: 12.104777
Train Epoch: 134 	 Loss: 12.773340
Train Epoch: 135 	 Loss: 12.397906
Train Epoch: 136 	 Loss: 12.164641
Train Epoch: 137 	 Loss: 12.173145
Train Epoch: 138 	 Loss: 12.390512
Train Epoch: 139 	 Loss: 12.443895
Train Epoch: 140 	 Loss: 12.131312
Train Epoch: 141 	 Loss: 12.264528
Train Epoch: 142 	 Loss: 12.518312
Train Epoch: 143 	 Loss: 12.847324
Train Epoch: 144 	 Loss: 12.041225
Train Epoch: 145 	 Loss: 12.286169
Train Epoch: 146 	 Loss: 12.144111
Train Epoch: 147 	 Loss: 12.099812
Train Epoch: 148 	 Loss: 12.341544
Train Epoch: 149 	 Loss: 12.315189
Train Epoch: 150 	 Loss: 12.809860
Train Epoch: 151 	 Loss: 12.259635
Train Epoch: 152 	 Loss: 12.281748
Train Epoch: 153 	 Loss: 12.167218
Train Epoch: 154 	 Loss: 12.293612
Train Epoch: 155 	 Loss: 12.176922
Train Epoch: 156 	 Loss: 12.093060
Train Epoch: 157 	 Loss: 12.501452
Train Epoch: 158 	 Loss: 12.275541
Train Epoch: 159 	 Loss: 12.232986
Train Epoch: 160 	 Loss: 12.226112
Train Epoch: 161 	 Loss: 12.222036
Train Epoch: 162 	 Loss: 12.076875
Train Epoch: 163 	 Loss: 12.428913
Train Epoch: 164 	 Loss: 12.500382
Train Epoch: 165 	 Loss: 12.664804
Train Epoch: 166 	 Loss: 12.313547
Train Epoch: 167 	 Loss: 12.467869
Train Epoch: 168 	 Loss: 12.357668
Train Epoch: 169 	 Loss: 12.357702
Train Epoch: 170 	 Loss: 12.265742
Train Epoch: 171 	 Loss: 12.197902
Train Epoch: 172 	 Loss: 12.285095
Train Epoch: 173 	 Loss: 13.182541
Train Epoch: 174 	 Loss: 12.250744
Train Epoch: 175 	 Loss: 12.098019
Train Epoch: 176 	 Loss: 12.149610
Train Epoch: 177 	 Loss: 12.156298
Train Epoch: 178 	 Loss: 12.113321
Train Epoch: 179 	 Loss: 12.583060
Train Epoch: 180 	 Loss: 12.079847
Train Epoch: 181 	 Loss: 12.220213
Train Epoch: 182 	 Loss: 12.117392
Train Epoch: 183 	 Loss: 12.091263
Train Epoch: 184 	 Loss: 12.121737
Train Epoch: 185 	 Loss: 12.320505
Train Epoch: 186 	 Loss: 12.354692
Train Epoch: 187 	 Loss: 12.203328
Train Epoch: 188 	 Loss: 12.449858
Train Epoch: 189 	 Loss: 12.335505
Train Epoch: 190 	 Loss: 12.212280
Train Epoch: 191 	 Loss: 12.644281
Train Epoch: 192 	 Loss: 12.156057
Train Epoch: 193 	 Loss: 12.451143
Train Epoch: 194 	 Loss: 12.484325
Train Epoch: 195 	 Loss: 12.228143
Train Epoch: 196 	 Loss: 12.497114
Train Epoch: 197 	 Loss: 12.140865
Train Epoch: 198 	 Loss: 12.309193
Train Epoch: 199 	 Loss: 12.150558
Train Epoch: 200 	 Loss: 12.622574
Train Epoch: 201 	 Loss: 12.282331
Train Epoch: 202 	 Loss: 12.196341
Train Epoch: 203 	 Loss: 12.936597
Train Epoch: 204 	 Loss: 12.735812
Train Epoch: 205 	 Loss: 12.102043
Train Epoch: 206 	 Loss: 12.204479
Train Epoch: 207 	 Loss: 12.392079
Train Epoch: 208 	 Loss: 12.398523
Train Epoch: 209 	 Loss: 12.239140
Train Epoch: 210 	 Loss: 12.477722
Train Epoch: 211 	 Loss: 12.298365
Train Epoch: 212 	 Loss: 12.137554
Train Epoch: 213 	 Loss: 12.298470
Train Epoch: 214 	 Loss: 12.112864
Train Epoch: 215 	 Loss: 12.086643
Train Epoch: 216 	 Loss: 12.572368
Train Epoch: 217 	 Loss: 12.678846
Train Epoch: 218 	 Loss: 12.439882
Train Epoch: 219 	 Loss: 12.197493
Train Epoch: 220 	 Loss: 12.221562
Train Epoch: 221 	 Loss: 12.445152
Train Epoch: 222 	 Loss: 12.135325
Train Epoch: 223 	 Loss: 12.150795
Train Epoch: 224 	 Loss: 12.176768
Train Epoch: 225 	 Loss: 12.373734
Train Epoch: 226 	 Loss: 12.502554
Train Epoch: 227 	 Loss: 12.698077
Train Epoch: 228 	 Loss: 12.158553
Train Epoch: 229 	 Loss: 12.260925
Train Epoch: 230 	 Loss: 12.320648
Train Epoch: 231 	 Loss: 12.552155
Train Epoch: 232 	 Loss: 12.261331
Train Epoch: 233 	 Loss: 12.254971
Train Epoch: 234 	 Loss: 12.270380
Train Epoch: 235 	 Loss: 12.292442
Train Epoch: 236 	 Loss: 12.369769
Train Epoch: 237 	 Loss: 12.137197
Train Epoch: 238 	 Loss: 12.337227
Train Epoch: 239 	 Loss: 12.324915
Train Epoch: 240 	 Loss: 12.351388
Train Epoch: 241 	 Loss: 12.476780
Train Epoch: 242 	 Loss: 12.584803
Train Epoch: 243 	 Loss: 12.608809
Train Epoch: 244 	 Loss: 12.230496
Train Epoch: 245 	 Loss: 12.284001
Train Epoch: 246 	 Loss: 12.193382
Train Epoch: 247 	 Loss: 12.233939
Train Epoch: 248 	 Loss: 12.386350
Train Epoch: 249 	 Loss: 12.298851
Train Epoch: 250 	 Loss: 12.444989
Train Epoch: 251 	 Loss: 12.303572
Train Epoch: 252 	 Loss: 12.210211
Train Epoch: 253 	 Loss: 12.035166
Train Epoch: 254 	 Loss: 12.331932
Train Epoch: 255 	 Loss: 12.346214
Train Epoch: 256 	 Loss: 12.341219
Train Epoch: 257 	 Loss: 12.162186
Train Epoch: 258 	 Loss: 12.118602
Train Epoch: 259 	 Loss: 12.588780
Train Epoch: 260 	 Loss: 12.440108
Train Epoch: 261 	 Loss: 12.080004
Train Epoch: 262 	 Loss: 12.150591
Train Epoch: 263 	 Loss: 12.010798
Train Epoch: 264 	 Loss: 12.483308
Train Epoch: 265 	 Loss: 12.222168
Train Epoch: 266 	 Loss: 12.419271
Train Epoch: 267 	 Loss: 12.471876
Train Epoch: 268 	 Loss: 12.605038
Train Epoch: 269 	 Loss: 12.218754
Train Epoch: 270 	 Loss: 12.145927
Train Epoch: 271 	 Loss: 12.385832
Train Epoch: 272 	 Loss: 12.305223
Train Epoch: 273 	 Loss: 12.317102
Train Epoch: 274 	 Loss: 12.083479
Train Epoch: 275 	 Loss: 12.169818
Train Epoch: 276 	 Loss: 12.391199
Train Epoch: 277 	 Loss: 12.366561
Train Epoch: 278 	 Loss: 12.083643
Train Epoch: 279 	 Loss: 12.630691
Train Epoch: 280 	 Loss: 12.400446
Train Epoch: 281 	 Loss: 12.106859
Train Epoch: 282 	 Loss: 12.141763
Train Epoch: 283 	 Loss: 12.348368
Train Epoch: 284 	 Loss: 12.267941
Train Epoch: 285 	 Loss: 12.223554
Train Epoch: 286 	 Loss: 12.279857
Train Epoch: 287 	 Loss: 12.274299
Train Epoch: 288 	 Loss: 12.056878
Train Epoch: 289 	 Loss: 12.178249
Train Epoch: 290 	 Loss: 12.219830
Train Epoch: 291 	 Loss: 12.289579
Train Epoch: 292 	 Loss: 12.382394
Train Epoch: 293 	 Loss: 12.364906
Train Epoch: 294 	 Loss: 12.285242
Train Epoch: 295 	 Loss: 12.285840
Train Epoch: 296 	 Loss: 12.211808
Train Epoch: 297 	 Loss: 12.262543
Train Epoch: 298 	 Loss: 12.341430
Train Epoch: 299 	 Loss: 12.036499
Train Epoch: 300 	 Loss: 12.197818
Train Epoch: 301 	 Loss: 12.258507
Train Epoch: 302 	 Loss: 12.593129
Train Epoch: 303 	 Loss: 12.253618
Train Epoch: 304 	 Loss: 12.185081
Train Epoch: 305 	 Loss: 12.300883
Train Epoch: 306 	 Loss: 12.108526
Train Epoch: 307 	 Loss: 12.398524
Train Epoch: 308 	 Loss: 12.245570
Train Epoch: 309 	 Loss: 12.325306
Train Epoch: 310 	 Loss: 12.226823
Train Epoch: 311 	 Loss: 12.271582
Train Epoch: 312 	 Loss: 12.185556
Train Epoch: 313 	 Loss: 12.287256
Train Epoch: 314 	 Loss: 12.083263
Train Epoch: 315 	 Loss: 12.231955
Train Epoch: 316 	 Loss: 12.686792
Train Epoch: 317 	 Loss: 12.071137
Train Epoch: 318 	 Loss: 12.143484
Train Epoch: 319 	 Loss: 12.270272
Train Epoch: 320 	 Loss: 12.414967
Train Epoch: 321 	 Loss: 12.389467
Train Epoch: 322 	 Loss: 12.274773
Train Epoch: 323 	 Loss: 12.346982
Train Epoch: 324 	 Loss: 12.245852
Train Epoch: 325 	 Loss: 12.244057
Train Epoch: 326 	 Loss: 12.217302
Train Epoch: 327 	 Loss: 12.050947
Train Epoch: 328 	 Loss: 12.184245
Train Epoch: 329 	 Loss: 12.229702
Train Epoch: 330 	 Loss: 12.360840
Train Epoch: 331 	 Loss: 12.207686
Train Epoch: 332 	 Loss: 12.184038
Train Epoch: 333 	 Loss: 12.241122
Train Epoch: 334 	 Loss: 12.241585
Train Epoch: 335 	 Loss: 12.293092
Train Epoch: 336 	 Loss: 12.238982
Train Epoch: 337 	 Loss: 12.216063
Train Epoch: 338 	 Loss: 12.584928
Train Epoch: 339 	 Loss: 12.138320
Train Epoch: 340 	 Loss: 12.142358
Train Epoch: 341 	 Loss: 12.783889
Train Epoch: 342 	 Loss: 12.795507
Train Epoch: 343 	 Loss: 12.227561
Train Epoch: 344 	 Loss: 12.071391
Train Epoch: 345 	 Loss: 12.164234
Train Epoch: 346 	 Loss: 12.386562
Train Epoch: 347 	 Loss: 12.216193
Train Epoch: 348 	 Loss: 12.430176
Train Epoch: 349 	 Loss: 12.560249
Train Epoch: 350 	 Loss: 12.638956
Train Epoch: 351 	 Loss: 12.177824
Train Epoch: 352 	 Loss: 12.081388
Train Epoch: 353 	 Loss: 12.116563
Train Epoch: 354 	 Loss: 12.250751
Train Epoch: 355 	 Loss: 12.347182
Train Epoch: 356 	 Loss: 12.184105
Train Epoch: 357 	 Loss: 12.192737
Train Epoch: 358 	 Loss: 12.668610
Train Epoch: 359 	 Loss: 12.103666
Train Epoch: 360 	 Loss: 12.371096
Train Epoch: 361 	 Loss: 12.138796
Train Epoch: 362 	 Loss: 12.271075
Train Epoch: 363 	 Loss: 12.084570
Train Epoch: 364 	 Loss: 12.124170
Train Epoch: 365 	 Loss: 12.309860
Train Epoch: 366 	 Loss: 12.078800
Train Epoch: 367 	 Loss: 12.409563
Train Epoch: 368 	 Loss: 12.169195
Train Epoch: 369 	 Loss: 12.371260
Train Epoch: 370 	 Loss: 12.310049
Train Epoch: 371 	 Loss: 12.198132
Train Epoch: 372 	 Loss: 12.268150
Train Epoch: 373 	 Loss: 12.617451
Train Epoch: 374 	 Loss: 12.643555
Train Epoch: 375 	 Loss: 12.144029
Train Epoch: 376 	 Loss: 12.302943
Train Epoch: 377 	 Loss: 12.996599
Train Epoch: 378 	 Loss: 12.801100
Train Epoch: 379 	 Loss: 12.140076
Train Epoch: 380 	 Loss: 12.228221
Train Epoch: 381 	 Loss: 12.151602
Train Epoch: 382 	 Loss: 12.307617
Train Epoch: 383 	 Loss: 12.061234
Train Epoch: 384 	 Loss: 12.433477
Train Epoch: 385 	 Loss: 12.407559
Train Epoch: 386 	 Loss: 12.108768
Train Epoch: 387 	 Loss: 12.538527
Train Epoch: 388 	 Loss: 12.177531
Train Epoch: 389 	 Loss: 12.274790
Train Epoch: 390 	 Loss: 12.040813
Train Epoch: 391 	 Loss: 12.119345
Train Epoch: 392 	 Loss: 12.535117
Train Epoch: 393 	 Loss: 12.180555
Train Epoch: 394 	 Loss: 12.274450
Train Epoch: 395 	 Loss: 12.251081
Train Epoch: 396 	 Loss: 12.168960
Train Epoch: 397 	 Loss: 12.455347
Train Epoch: 398 	 Loss: 12.365539
Train Epoch: 399 	 Loss: 12.296608

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.885

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.567

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.850

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.883

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.955

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.916

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.930

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.920

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.926

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.917

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.860

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.916
------
f1 mean across methods is 0.877


Repetition:  2
 • seed                     - 2
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 400
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=2_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.32896104e-01 4.43051948e-01 1.85844156e-02 5.24675325e-03
 2.20779221e-04] 

Train Epoch: 0 	 Loss: 19.465832
Train Epoch: 1 	 Loss: 18.937328
Train Epoch: 2 	 Loss: 18.999573
Train Epoch: 3 	 Loss: 16.416836
Train Epoch: 4 	 Loss: 15.649345
Train Epoch: 5 	 Loss: 15.484482
Train Epoch: 6 	 Loss: 15.010200
Train Epoch: 7 	 Loss: 14.793452
Train Epoch: 8 	 Loss: 14.747691
Train Epoch: 9 	 Loss: 14.644528
Train Epoch: 10 	 Loss: 15.474092
Train Epoch: 11 	 Loss: 14.748098
Train Epoch: 12 	 Loss: 14.370808
Train Epoch: 13 	 Loss: 14.065821
Train Epoch: 14 	 Loss: 13.638267
Train Epoch: 15 	 Loss: 14.136744
Train Epoch: 16 	 Loss: 14.539444
Train Epoch: 17 	 Loss: 14.445155
Train Epoch: 18 	 Loss: 14.138525
Train Epoch: 19 	 Loss: 14.410661
Train Epoch: 20 	 Loss: 14.106592
Train Epoch: 21 	 Loss: 14.218232
Train Epoch: 22 	 Loss: 14.218355
Train Epoch: 23 	 Loss: 14.418612
Train Epoch: 24 	 Loss: 13.895731
Train Epoch: 25 	 Loss: 14.213491
Train Epoch: 26 	 Loss: 14.363434
Train Epoch: 27 	 Loss: 14.339767
Train Epoch: 28 	 Loss: 13.753141
Train Epoch: 29 	 Loss: 14.043909
Train Epoch: 30 	 Loss: 14.236786
Train Epoch: 31 	 Loss: 14.293547
Train Epoch: 32 	 Loss: 14.530732
Train Epoch: 33 	 Loss: 14.269929
Train Epoch: 34 	 Loss: 14.212746
Train Epoch: 35 	 Loss: 14.281474
Train Epoch: 36 	 Loss: 14.183048
Train Epoch: 37 	 Loss: 14.128204
Train Epoch: 38 	 Loss: 14.241451
Train Epoch: 39 	 Loss: 14.200450
Train Epoch: 40 	 Loss: 14.129585
Train Epoch: 41 	 Loss: 14.342743
Train Epoch: 42 	 Loss: 14.672059
Train Epoch: 43 	 Loss: 14.876755
Train Epoch: 44 	 Loss: 14.197197
Train Epoch: 45 	 Loss: 14.027047
Train Epoch: 46 	 Loss: 14.084225
Train Epoch: 47 	 Loss: 14.255028
Train Epoch: 48 	 Loss: 14.167395
Train Epoch: 49 	 Loss: 14.230075
Train Epoch: 50 	 Loss: 14.164928
Train Epoch: 51 	 Loss: 13.905258
Train Epoch: 52 	 Loss: 14.464890
Train Epoch: 53 	 Loss: 14.124743
Train Epoch: 54 	 Loss: 14.456497
Train Epoch: 55 	 Loss: 14.095571
Train Epoch: 56 	 Loss: 14.215221
Train Epoch: 57 	 Loss: 14.017812
Train Epoch: 58 	 Loss: 14.159529
Train Epoch: 59 	 Loss: 14.365242
Train Epoch: 60 	 Loss: 14.130750
Train Epoch: 61 	 Loss: 14.318264
Train Epoch: 62 	 Loss: 14.008270
Train Epoch: 63 	 Loss: 14.466619
Train Epoch: 64 	 Loss: 14.017059
Train Epoch: 65 	 Loss: 14.197237
Train Epoch: 66 	 Loss: 14.913046
Train Epoch: 67 	 Loss: 14.329062
Train Epoch: 68 	 Loss: 14.320350
Train Epoch: 69 	 Loss: 14.049250
Train Epoch: 70 	 Loss: 14.114513
Train Epoch: 71 	 Loss: 14.091157
Train Epoch: 72 	 Loss: 14.600315
Train Epoch: 73 	 Loss: 14.356318
Train Epoch: 74 	 Loss: 14.143229
Train Epoch: 75 	 Loss: 13.703883
Train Epoch: 76 	 Loss: 14.216101
Train Epoch: 77 	 Loss: 14.301218
Train Epoch: 78 	 Loss: 14.278980
Train Epoch: 79 	 Loss: 14.100190
Train Epoch: 80 	 Loss: 14.049271
Train Epoch: 81 	 Loss: 13.980115
Train Epoch: 82 	 Loss: 13.979999
Train Epoch: 83 	 Loss: 14.254418
Train Epoch: 84 	 Loss: 14.094661
Train Epoch: 85 	 Loss: 14.241417
Train Epoch: 86 	 Loss: 14.048700
Train Epoch: 87 	 Loss: 14.176911
Train Epoch: 88 	 Loss: 13.968428
Train Epoch: 89 	 Loss: 14.038345
Train Epoch: 90 	 Loss: 13.840638
Train Epoch: 91 	 Loss: 14.204405
Train Epoch: 92 	 Loss: 13.967826
Train Epoch: 93 	 Loss: 14.396024
Train Epoch: 94 	 Loss: 14.168955
Train Epoch: 95 	 Loss: 14.095873
Train Epoch: 96 	 Loss: 14.137821
Train Epoch: 97 	 Loss: 14.247534
Train Epoch: 98 	 Loss: 14.059896
Train Epoch: 99 	 Loss: 13.960168
Train Epoch: 100 	 Loss: 13.991693
Train Epoch: 101 	 Loss: 14.483834
Train Epoch: 102 	 Loss: 14.178185
Train Epoch: 103 	 Loss: 14.033367
Train Epoch: 104 	 Loss: 14.709115
Train Epoch: 105 	 Loss: 14.162168
Train Epoch: 106 	 Loss: 14.419747
Train Epoch: 107 	 Loss: 14.325693
Train Epoch: 108 	 Loss: 14.401844
Train Epoch: 109 	 Loss: 14.171757
Train Epoch: 110 	 Loss: 14.009626
Train Epoch: 111 	 Loss: 14.260957
Train Epoch: 112 	 Loss: 14.124292
Train Epoch: 113 	 Loss: 14.006638
Train Epoch: 114 	 Loss: 14.013498
Train Epoch: 115 	 Loss: 14.280582
Train Epoch: 116 	 Loss: 14.153423
Train Epoch: 117 	 Loss: 14.000574
Train Epoch: 118 	 Loss: 14.248373
Train Epoch: 119 	 Loss: 14.047065
Train Epoch: 120 	 Loss: 14.287594
Train Epoch: 121 	 Loss: 13.994389
Train Epoch: 122 	 Loss: 14.102161
Train Epoch: 123 	 Loss: 13.951889
Train Epoch: 124 	 Loss: 14.501797
Train Epoch: 125 	 Loss: 14.427685
Train Epoch: 126 	 Loss: 14.365782
Train Epoch: 127 	 Loss: 14.248941
Train Epoch: 128 	 Loss: 13.960537
Train Epoch: 129 	 Loss: 14.066781
Train Epoch: 130 	 Loss: 14.225138
Train Epoch: 131 	 Loss: 14.375134
Train Epoch: 132 	 Loss: 14.045801
Train Epoch: 133 	 Loss: 14.019859
Train Epoch: 134 	 Loss: 14.002173
Train Epoch: 135 	 Loss: 13.936876
Train Epoch: 136 	 Loss: 14.024634
Train Epoch: 137 	 Loss: 13.987177
Train Epoch: 138 	 Loss: 14.011296
Train Epoch: 139 	 Loss: 13.920159
Train Epoch: 140 	 Loss: 13.939377
Train Epoch: 141 	 Loss: 13.814603
Train Epoch: 142 	 Loss: 13.556196
Train Epoch: 143 	 Loss: 13.981122
Train Epoch: 144 	 Loss: 13.842610
Train Epoch: 145 	 Loss: 13.913080
Train Epoch: 146 	 Loss: 14.035184
Train Epoch: 147 	 Loss: 13.623075
Train Epoch: 148 	 Loss: 14.001865
Train Epoch: 149 	 Loss: 13.559196
Train Epoch: 150 	 Loss: 13.620283
Train Epoch: 151 	 Loss: 13.622755
Train Epoch: 152 	 Loss: 14.763844
Train Epoch: 153 	 Loss: 13.899973
Train Epoch: 154 	 Loss: 14.397287
Train Epoch: 155 	 Loss: 13.539907
Train Epoch: 156 	 Loss: 14.648708
Train Epoch: 157 	 Loss: 13.905411
Train Epoch: 158 	 Loss: 14.426882
Train Epoch: 159 	 Loss: 13.644857
Train Epoch: 160 	 Loss: 13.994076
Train Epoch: 161 	 Loss: 14.065979
Train Epoch: 162 	 Loss: 13.615820
Train Epoch: 163 	 Loss: 14.159382
Train Epoch: 164 	 Loss: 14.499056
Train Epoch: 165 	 Loss: 13.563518
Train Epoch: 166 	 Loss: 13.657889
Train Epoch: 167 	 Loss: 13.616227
Train Epoch: 168 	 Loss: 13.555718
Train Epoch: 169 	 Loss: 13.480196
Train Epoch: 170 	 Loss: 13.374518
Train Epoch: 171 	 Loss: 13.436855
Train Epoch: 172 	 Loss: 13.826014
Train Epoch: 173 	 Loss: 13.610237
Train Epoch: 174 	 Loss: 13.482382
Train Epoch: 175 	 Loss: 13.386084
Train Epoch: 176 	 Loss: 13.243581
Train Epoch: 177 	 Loss: 13.501376
Train Epoch: 178 	 Loss: 13.530874
Train Epoch: 179 	 Loss: 13.180161
Train Epoch: 180 	 Loss: 13.223186
Train Epoch: 181 	 Loss: 13.231829
Train Epoch: 182 	 Loss: 13.268255
Train Epoch: 183 	 Loss: 13.358120
Train Epoch: 184 	 Loss: 13.537603
Train Epoch: 185 	 Loss: 13.533705
Train Epoch: 186 	 Loss: 13.260091
Train Epoch: 187 	 Loss: 13.340325
Train Epoch: 188 	 Loss: 13.698392
Train Epoch: 189 	 Loss: 13.370116
Train Epoch: 190 	 Loss: 13.609388
Train Epoch: 191 	 Loss: 13.419226
Train Epoch: 192 	 Loss: 13.564379
Train Epoch: 193 	 Loss: 13.521969
Train Epoch: 194 	 Loss: 13.778996
Train Epoch: 195 	 Loss: 13.674112
Train Epoch: 196 	 Loss: 13.325792
Train Epoch: 197 	 Loss: 13.243015
Train Epoch: 198 	 Loss: 13.363216
Train Epoch: 199 	 Loss: 13.260962
Train Epoch: 200 	 Loss: 13.550985
Train Epoch: 201 	 Loss: 13.276962
Train Epoch: 202 	 Loss: 13.388916
Train Epoch: 203 	 Loss: 13.288120
Train Epoch: 204 	 Loss: 13.194162
Train Epoch: 205 	 Loss: 14.201113
Train Epoch: 206 	 Loss: 13.313447
Train Epoch: 207 	 Loss: 13.642783
Train Epoch: 208 	 Loss: 13.253138
Train Epoch: 209 	 Loss: 13.494480
Train Epoch: 210 	 Loss: 13.605826
Train Epoch: 211 	 Loss: 13.603724
Train Epoch: 212 	 Loss: 13.180112
Train Epoch: 213 	 Loss: 13.732021
Train Epoch: 214 	 Loss: 13.886832
Train Epoch: 215 	 Loss: 13.110645
Train Epoch: 216 	 Loss: 13.129146
Train Epoch: 217 	 Loss: 13.201470
Train Epoch: 218 	 Loss: 13.398406
Train Epoch: 219 	 Loss: 13.279020
Train Epoch: 220 	 Loss: 13.218267
Train Epoch: 221 	 Loss: 13.582822
Train Epoch: 222 	 Loss: 13.533873
Train Epoch: 223 	 Loss: 13.537541
Train Epoch: 224 	 Loss: 13.261237
Train Epoch: 225 	 Loss: 13.603413
Train Epoch: 226 	 Loss: 13.577518
Train Epoch: 227 	 Loss: 13.452686
Train Epoch: 228 	 Loss: 13.349076
Train Epoch: 229 	 Loss: 13.625568
Train Epoch: 230 	 Loss: 13.966671
Train Epoch: 231 	 Loss: 13.534332
Train Epoch: 232 	 Loss: 13.312239
Train Epoch: 233 	 Loss: 13.241049
Train Epoch: 234 	 Loss: 13.353820
Train Epoch: 235 	 Loss: 13.357239
Train Epoch: 236 	 Loss: 13.736982
Train Epoch: 237 	 Loss: 14.004506
Train Epoch: 238 	 Loss: 13.739727
Train Epoch: 239 	 Loss: 13.875890
Train Epoch: 240 	 Loss: 13.331989
Train Epoch: 241 	 Loss: 13.523303
Train Epoch: 242 	 Loss: 13.201265
Train Epoch: 243 	 Loss: 13.232133
Train Epoch: 244 	 Loss: 13.636809
Train Epoch: 245 	 Loss: 13.190562
Train Epoch: 246 	 Loss: 13.219769
Train Epoch: 247 	 Loss: 13.429955
Train Epoch: 248 	 Loss: 13.279609
Train Epoch: 249 	 Loss: 13.304171
Train Epoch: 250 	 Loss: 13.251149
Train Epoch: 251 	 Loss: 13.296571
Train Epoch: 252 	 Loss: 13.608186
Train Epoch: 253 	 Loss: 13.541811
Train Epoch: 254 	 Loss: 13.489013
Train Epoch: 255 	 Loss: 13.632822
Train Epoch: 256 	 Loss: 14.070648
Train Epoch: 257 	 Loss: 13.782367
Train Epoch: 258 	 Loss: 13.231751
Train Epoch: 259 	 Loss: 13.769550
Train Epoch: 260 	 Loss: 13.349541
Train Epoch: 261 	 Loss: 13.521151
Train Epoch: 262 	 Loss: 13.565802
Train Epoch: 263 	 Loss: 13.331075
Train Epoch: 264 	 Loss: 13.483278
Train Epoch: 265 	 Loss: 13.590529
Train Epoch: 266 	 Loss: 13.226226
Train Epoch: 267 	 Loss: 13.432035
Train Epoch: 268 	 Loss: 14.270500
Train Epoch: 269 	 Loss: 13.795322
Train Epoch: 270 	 Loss: 13.576676
Train Epoch: 271 	 Loss: 14.045954
Train Epoch: 272 	 Loss: 14.103338
Train Epoch: 273 	 Loss: 13.236728
Train Epoch: 274 	 Loss: 13.389959
Train Epoch: 275 	 Loss: 13.367495
Train Epoch: 276 	 Loss: 13.206804
Train Epoch: 277 	 Loss: 13.185678
Train Epoch: 278 	 Loss: 14.088205
Train Epoch: 279 	 Loss: 13.379352
Train Epoch: 280 	 Loss: 13.448538
Train Epoch: 281 	 Loss: 13.552288
Train Epoch: 282 	 Loss: 13.301174
Train Epoch: 283 	 Loss: 13.404862
Train Epoch: 284 	 Loss: 13.379928
Train Epoch: 285 	 Loss: 13.440008
Train Epoch: 286 	 Loss: 13.429561
Train Epoch: 287 	 Loss: 13.323736
Train Epoch: 288 	 Loss: 13.324821
Train Epoch: 289 	 Loss: 13.308813
Train Epoch: 290 	 Loss: 14.132547
Train Epoch: 291 	 Loss: 13.583398
Train Epoch: 292 	 Loss: 13.340464
Train Epoch: 293 	 Loss: 13.147565
Train Epoch: 294 	 Loss: 13.304409
Train Epoch: 295 	 Loss: 14.031307
Train Epoch: 296 	 Loss: 13.552123
Train Epoch: 297 	 Loss: 13.225197
Train Epoch: 298 	 Loss: 13.591221
Train Epoch: 299 	 Loss: 13.606709
Train Epoch: 300 	 Loss: 13.206653
Train Epoch: 301 	 Loss: 13.355388
Train Epoch: 302 	 Loss: 13.504646
Train Epoch: 303 	 Loss: 13.239370
Train Epoch: 304 	 Loss: 13.257561
Train Epoch: 305 	 Loss: 13.357510
Train Epoch: 306 	 Loss: 13.233351
Train Epoch: 307 	 Loss: 13.322992
Train Epoch: 308 	 Loss: 13.409029
Train Epoch: 309 	 Loss: 13.318680
Train Epoch: 310 	 Loss: 13.399872
Train Epoch: 311 	 Loss: 13.558231
Train Epoch: 312 	 Loss: 13.159322
Train Epoch: 313 	 Loss: 13.372108
Train Epoch: 314 	 Loss: 13.188829
Train Epoch: 315 	 Loss: 13.259838
Train Epoch: 316 	 Loss: 13.373478
Train Epoch: 317 	 Loss: 13.413189
Train Epoch: 318 	 Loss: 13.114721
Train Epoch: 319 	 Loss: 13.075186
Train Epoch: 320 	 Loss: 13.224247
Train Epoch: 321 	 Loss: 13.473580
Train Epoch: 322 	 Loss: 13.149506
Train Epoch: 323 	 Loss: 13.544955
Train Epoch: 324 	 Loss: 13.596504
Train Epoch: 325 	 Loss: 15.174368
Train Epoch: 326 	 Loss: 13.455505
Train Epoch: 327 	 Loss: 13.933371
Train Epoch: 328 	 Loss: 13.402102
Train Epoch: 329 	 Loss: 13.652378
Train Epoch: 330 	 Loss: 14.102512
Train Epoch: 331 	 Loss: 13.299910
Train Epoch: 332 	 Loss: 13.488483
Train Epoch: 333 	 Loss: 13.398295
Train Epoch: 334 	 Loss: 13.391433
Train Epoch: 335 	 Loss: 13.355333
Train Epoch: 336 	 Loss: 13.399568
Train Epoch: 337 	 Loss: 14.125710
Train Epoch: 338 	 Loss: 13.187558
Train Epoch: 339 	 Loss: 13.724613
Train Epoch: 340 	 Loss: 13.500154
Train Epoch: 341 	 Loss: 13.914038
Train Epoch: 342 	 Loss: 13.954459
Train Epoch: 343 	 Loss: 13.528850
Train Epoch: 344 	 Loss: 13.476292
Train Epoch: 345 	 Loss: 13.368988
Train Epoch: 346 	 Loss: 13.503514
Train Epoch: 347 	 Loss: 14.939329
Train Epoch: 348 	 Loss: 13.412138
Train Epoch: 349 	 Loss: 14.430730
Train Epoch: 350 	 Loss: 13.814000
Train Epoch: 351 	 Loss: 14.533517
Train Epoch: 352 	 Loss: 14.398426
Train Epoch: 353 	 Loss: 13.772878
Train Epoch: 354 	 Loss: 13.912115
Train Epoch: 355 	 Loss: 15.112526
Train Epoch: 356 	 Loss: 14.111131
Train Epoch: 357 	 Loss: 13.902308
Train Epoch: 358 	 Loss: 13.843847
Train Epoch: 359 	 Loss: 13.617825
Train Epoch: 360 	 Loss: 13.734419
Train Epoch: 361 	 Loss: 13.797365
Train Epoch: 362 	 Loss: 14.242540
Train Epoch: 363 	 Loss: 14.401282
Train Epoch: 364 	 Loss: 13.352341
Train Epoch: 365 	 Loss: 13.311244
Train Epoch: 366 	 Loss: 13.252274
Train Epoch: 367 	 Loss: 13.180851
Train Epoch: 368 	 Loss: 13.080635
Train Epoch: 369 	 Loss: 14.799178
Train Epoch: 370 	 Loss: 13.455945
Train Epoch: 371 	 Loss: 13.191967
Train Epoch: 372 	 Loss: 13.998866
Train Epoch: 373 	 Loss: 13.308729
Train Epoch: 374 	 Loss: 13.296413
Train Epoch: 375 	 Loss: 13.729644
Train Epoch: 376 	 Loss: 13.447693
Train Epoch: 377 	 Loss: 13.561775
Train Epoch: 378 	 Loss: 13.521040
Train Epoch: 379 	 Loss: 13.632538
Train Epoch: 380 	 Loss: 12.945813
Train Epoch: 381 	 Loss: 13.359834
Train Epoch: 382 	 Loss: 13.446874
Train Epoch: 383 	 Loss: 13.382355
Train Epoch: 384 	 Loss: 13.712414
Train Epoch: 385 	 Loss: 13.237586
Train Epoch: 386 	 Loss: 13.919321
Train Epoch: 387 	 Loss: 13.319409
Train Epoch: 388 	 Loss: 12.927135
Train Epoch: 389 	 Loss: 13.539558
Train Epoch: 390 	 Loss: 13.121410
Train Epoch: 391 	 Loss: 12.858377
Train Epoch: 392 	 Loss: 13.151752
Train Epoch: 393 	 Loss: 13.221881
Train Epoch: 394 	 Loss: 14.647018
Train Epoch: 395 	 Loss: 13.410532
Train Epoch: 396 	 Loss: 13.414059
Train Epoch: 397 	 Loss: 13.087397
Train Epoch: 398 	 Loss: 13.266991
Train Epoch: 399 	 Loss: 13.623478

 generated data


 <class 'sklearn.linear_model._logistic.LogisticRegression'>
logistic regression with balanced class weight
logistic regression with saga solver
logistic regression with liblinear solver
logistic regression with liblinear solver
F1-score on test generated data is 0.883

 <class 'sklearn.naive_bayes.GaussianNB'>
training again
F1-score on test generated data is 0.598

 <class 'sklearn.naive_bayes.BernoulliNB'>
training again
training again
training again
F1-score on test generated data is 0.923

 <class 'sklearn.svm._classes.LinearSVC'>
training again
training again
training again
F1-score on test generated data is 0.893

 <class 'sklearn.tree._classes.DecisionTreeClassifier'>
training again
training again
training again
F1-score on test generated data is 0.914

 <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
test LDA with different hyperparameters
F1-score on test generated data is 0.913

 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
F1-score on test generated data is 0.926

 <class 'sklearn.ensemble._bagging.BaggingClassifier'>
test Bagging with different hyperparameters
F1-score on test generated data is 0.921

 <class 'sklearn.ensemble._forest.RandomForestClassifier'>
training again
training again
training again
training again
F1-score on test generated data is 0.923

 <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
F1-score on test generated data is 0.923

 <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
F1-score on test generated data is 0.886

 <class 'xgboost.sklearn.XGBClassifier'>
test XGB with different hyperparameters
F1-score on test generated data is 0.927
------
f1 mean across methods is 0.886


Repetition:  3
 • seed                     - 3
 • data_name                - intrusion
 • batch_rate               - 0.01
 • epochs                   - 400
 • lr                       - 0.01
 • lr_decay                 - 0.9
 • is_private               - False
 • epsilon                  - 1.0
 • delta                    - 1e-05
 • heuristic_sigma          - True
 • kernel_length            - 0.005
 • order_hermite            - 100
 • undersampled_rate        - 0.3
 • separate_kernel_length   - True
 • normalize_data           - False
 • classifiers              - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
 • log_name                 - intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue
 • log_dir                  - logs/gen/intrusion_seed=3_order=100_private=False_epsilon=1.0_delta=1e-05_heuristic_sigma=True_kernel_length=0.005_br=0.01_lr=0.01_n_epoch=400_undersam_rate=0.3_normalize_dataFalse_separate_kernel_lengthTrue/
-------------------------------------------
dataset is intrusion
(494021, 40)
testing non-standardized data
we use the median heuristic for length scale
we use a separate length scale on each coordinate of the data
computing mean embedding of data: (1) compute the weights

 weights with no privatization are [5.33707792e-01 4.42259740e-01 1.85194805e-02 5.28571429e-03
 2.27272727e-04] 

Train Epoch: 0 	 Loss: 18.500662
Train Epoch: 1 	 Loss: 17.596020
Train Epoch: 2 	 Loss: 14.003053
Train Epoch: 3 	 Loss: 13.832839
Train Epoch: 4 	 Loss: 13.439312
Train Epoch: 5 	 Loss: 13.553920
Train Epoch: 6 	 Loss: 13.151501
Train Epoch: 7 	 Loss: 13.255872
Train Epoch: 8 	 Loss: 13.422574
Train Epoch: 9 	 Loss: 12.961208
Train Epoch: 10 	 Loss: 12.940099
Train Epoch: 11 	 Loss: 12.972646
Train Epoch: 12 	 Loss: 12.723876
Train Epoch: 13 	 Loss: 13.040963
Train Epoch: 14 	 Loss: 13.462631
Train Epoch: 15 	 Loss: 13.406385
Train Epoch: 16 	 Loss: 13.294094
Train Epoch: 17 	 Loss: 13.053948
Train Epoch: 18 	 Loss: 13.414820
Train Epoch: 19 	 Loss: 13.783114
Train Epoch: 20 	 Loss: 12.745919
Train Epoch: 21 	 Loss: 13.131495
Train Epoch: 22 	 Loss: 13.282507
Train Epoch: 23 	 Loss: 13.064295
Train Epoch: 24 	 Loss: 13.280041
Train Epoch: 25 	 Loss: 13.166817
Train Epoch: 26 	 Loss: 12.898780
Train Epoch: 27 	 Loss: 12.865885
Train Epoch: 28 	 Loss: 13.045736
Train Epoch: 29 	 Loss: 13.029623
Train Epoch: 30 	 Loss: 13.273951
Train Epoch: 31 	 Loss: 13.332605
Train Epoch: 32 	 Loss: 12.850650
Train Epoch: 33 	 Loss: 13.053493
Train Epoch: 34 	 Loss: 12.695498
Train Epoch: 35 	 Loss: 12.620252
Train Epoch: 36 	 Loss: 12.645092
Train Epoch: 37 	 Loss: 12.702562
Train Epoch: 38 	 Loss: 12.823570
Train Epoch: 39 	 Loss: 12.630331
Train Epoch: 40 	 Loss: 12.595974
Train Epoch: 41 	 Loss: 12.641321
Train Epoch: 42 	 Loss: 12.773423
Train Epoch: 43 	 Loss: 12.862583
Train Epoch: 44 	 Loss: 12.670154
Train Epoch: 45 	 Loss: 12.598612
Train Epoch: 46 	 Loss: 13.159999
Train Epoch: 47 	 Loss: 12.637788
Train Epoch: 48 	 Loss: 12.668276
Train Epoch: 49 	 Loss: 12.989909
Train Epoch: 50 	 Loss: 12.889239
Train Epoch: 51 	 Loss: 12.617414
Train Epoch: 52 	 Loss: 12.773420
Train Epoch: 53 	 Loss: 12.841838
Train Epoch: 54 	 Loss: 12.525530
Train Epoch: 55 	 Loss: 12.686348
Train Epoch: 56 	 Loss: 12.786081
Train Epoch: 57 	 Loss: 12.560423
Train Epoch: 58 	 Loss: 12.409544
Train Epoch: 59 	 Loss: 12.517284
Train Epoch: 60 	 Loss: 12.721746
Train Epoch: 61 	 Loss: 12.551821
Train Epoch: 62 	 Loss: 12.610805
Train Epoch: 63 	 Loss: 12.411120
Train Epoch: 64 	 Loss: 12.485108
Train Epoch: 65 	 Loss: 12.458969
Train Epoch: 66 	 Loss: 12.774303
Train Epoch: 67 	 Loss: 12.929951
Train Epoch: 68 	 Loss: 12.546861
Train Epoch: 69 	 Loss: 12.432832
Train Epoch: 70 	 Loss: 12.905494
Train Epoch: 71 	 Loss: 12.681907
Train Epoch: 72 	 Loss: 12.419681
Train Epoch: 73 	 Loss: 12.605862
Train Epoch: 74 	 Loss: 12.779388
Train Epoch: 75 	 Loss: 12.618650
Train Epoch: 76 	 Loss: 12.724473
Train Epoch: 77 	 Loss: 12.503884
Train Epoch: 78 	 Loss: 12.454329
Train Epoch: 79 	 Loss: 12.395673
Train Epoch: 80 	 Loss: 12.537368
Train Epoch: 81 	 Loss: 12.543968
Train Epoch: 82 	 Loss: 12.736429
Train Epoch: 83 	 Loss: 12.573935
Train Epoch: 84 	 Loss: 12.700619
Train Epoch: 85 	 Loss: 12.405509
Train Epoch: 86 	 Loss: 12.580466
Train Epoch: 87 	 Loss: 12.655282
Train Epoch: 88 	 Loss: 12.493879
Train Epoch: 89 	 Loss: 12.411774
Train Epoch: 90 	 Loss: 12.825647
Train Epoch: 91 	 Loss: 12.275357
Train Epoch: 92 	 Loss: 12.313793
Train Epoch: 93 	 Loss: 12.458697
Train Epoch: 94 	 Loss: 12.233246
Train Epoch: 95 	 Loss: 12.402771
Train Epoch: 96 	 Loss: 12.511827
Train Epoch: 97 	 Loss: 12.228426
Train Epoch: 98 	 Loss: 12.336466
Train Epoch: 99 	 Loss: 12.525650
Train Epoch: 100 	 Loss: 12.526882
Train Epoch: 101 	 Loss: 12.224941
Train Epoch: 102 	 Loss: 12.447707
Train Epoch: 103 	 Loss: 12.576355
Train Epoch: 104 	 Loss: 12.272637
Train Epoch: 105 	 Loss: 12.386723
Train Epoch: 106 	 Loss: 12.628581
Train Epoch: 107 	 Loss: 12.352469
Train Epoch: 108 	 Loss: 12.174614
Train Epoch: 109 	 Loss: 12.482471
Train Epoch: 110 	 Loss: 12.261888
Train Epoch: 111 	 Loss: 11.708353
Train Epoch: 112 	 Loss: 12.324981
Train Epoch: 113 	 Loss: 12.405174
Train Epoch: 114 	 Loss: 11.989990
Train Epoch: 115 	 Loss: 12.226556
Train Epoch: 116 	 Loss: 11.801708
Train Epoch: 117 	 Loss: 11.811034
Train Epoch: 118 	 Loss: 11.884017
Train Epoch: 119 	 Loss: 12.056328
Train Epoch: 120 	 Loss: 11.807537
Train Epoch: 121 	 Loss: 12.046629
Train Epoch: 122 	 Loss: 11.660065
Train Epoch: 123 	 Loss: 11.671651
Train Epoch: 124 	 Loss: 11.572607
Train Epoch: 125 	 Loss: 11.781624
Train Epoch: 126 	 Loss: 11.746171
Train Epoch: 127 	 Loss: 11.665883
Train Epoch: 128 	 Loss: 11.670647
Train Epoch: 129 	 Loss: 12.280917
Train Epoch: 130 	 Loss: 12.062353
Train Epoch: 131 	 Loss: 11.867679
Train Epoch: 132 	 Loss: 11.614586
Train Epoch: 133 	 Loss: 12.014952
Train Epoch: 134 	 Loss: 13.208546
Train Epoch: 135 	 Loss: 11.668164
Train Epoch: 136 	 Loss: 11.683411
Train Epoch: 137 	 Loss: 12.288518
Train Epoch: 138 	 Loss: 11.740581
Train Epoch: 139 	 Loss: 12.106688
Train Epoch: 140 	 Loss: 11.586047
Train Epoch: 141 	 Loss: 12.528722
Train Epoch: 142 	 Loss: 11.940056
Train Epoch: 143 	 Loss: 11.599661
Train Epoch: 144 	 Loss: 11.649664
Train Epoch: 145 	 Loss: 11.708775
Train Epoch: 146 	 Loss: 11.613054
Train Epoch: 147 	 Loss: 11.656497
Train Epoch: 148 	 Loss: 11.661076
Train Epoch: 149 	 Loss: 11.704569
Train Epoch: 150 	 Loss: 11.744057
Train Epoch: 151 	 Loss: 11.971288
Train Epoch: 152 	 Loss: 11.826658
Train Epoch: 153 	 Loss: 11.725636
Train Epoch: 154 	 Loss: 11.839699
Train Epoch: 155 	 Loss: 11.926989
Train Epoch: 156 	 Loss: 11.666904
Train Epoch: 157 	 Loss: 11.714440
Train Epoch: 158 	 Loss: 11.701239
Train Epoch: 159 	 Loss: 11.767366
Train Epoch: 160 	 Loss: 12.288625
Train Epoch: 161 	 Loss: 11.546951
Train Epoch: 162 	 Loss: 11.676739
Train Epoch: 163 	 Loss: 11.597904
Train Epoch: 164 	 Loss: 11.755373
Train Epoch: 165 	 Loss: 11.657158
Train Epoch: 166 	 Loss: 12.584793
Train Epoch: 167 	 Loss: 12.243502
Train Epoch: 168 	 Loss: 11.873598
Train Epoch: 169 	 Loss: 11.757044
Train Epoch: 170 	 Loss: 12.422484
Train Epoch: 171 	 Loss: 11.998264
Train Epoch: 172 	 Loss: 11.671775
Train Epoch: 173 	 Loss: 11.735292
Train Epoch: 174 	 Loss: 11.876051
Train Epoch: 175 	 Loss: 11.676125
Train Epoch: 176 	 Loss: 11.837935
Train Epoch: 177 	 Loss: 11.683031
Train Epoch: 178 	 Loss: 11.664330
Train Epoch: 179 	 Loss: 11.827820
Train Epoch: 180 	 Loss: 11.696074
Train Epoch: 181 	 Loss: 11.756276
Train Epoch: 182 	 Loss: 11.643938
Train Epoch: 183 	 Loss: 11.632963
Train Epoch: 184 	 Loss: 12.483879
Train Epoch: 185 	 Loss: 12.141253
Train Epoch: 186 	 Loss: 12.194860